{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import watson_developer_cloud\n",
    "import os\n",
    "conversation_secrets = json.load(open('conversation-secrets.json'))\n",
    "discovery_secrets = json.load(open('discovery-secrets.json'))\n",
    "\n",
    "assistant = watson_developer_cloud.AssistantV1(\n",
    "    username=conversation_secrets['username'],\n",
    "    password=conversation_secrets['password'],\n",
    "    version=conversation_secrets['version']\n",
    ")\n",
    "discovery = watson_developer_cloud.DiscoveryV1(\n",
    "    username=discovery_secrets['username'],\n",
    "    password=discovery_secrets['password'],\n",
    "    version=discovery_secrets['version']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. environments\n",
      "{\n",
      "  \"environments\": [\n",
      "    {\n",
      "      \"environment_id\": \"system\",\n",
      "      \"name\": \"Watson System Environment\",\n",
      "      \"description\": \"Shared system data sources\",\n",
      "      \"read_only\": true\n",
      "    },\n",
      "    {\n",
      "      \"environment_id\": \"ce0a3e90-8a4c-4fec-a7cc-f655df3fa2b6\",\n",
      "      \"name\": \"TEC_env\",\n",
      "      \"description\": null,\n",
      "      \"created\": \"2018-04-30T04:41:55.563Z\",\n",
      "      \"updated\": \"2018-05-12T06:35:32.490Z\",\n",
      "      \"read_only\": false\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('1. environments')\n",
    "environments = discovery.list_environments()\n",
    "print(json.dumps(environments, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. my environment id\n",
      "\"ce0a3e90-8a4c-4fec-a7cc-f655df3fa2b6\"\n"
     ]
    }
   ],
   "source": [
    "print('2. my environment id')\n",
    "my_environments = [x for x in environments['environments'] if x['name'] == discovery_secrets['environment']]\n",
    "my_environment_id = my_environments[0]['environment_id']\n",
    "print(json.dumps(my_environment_id, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. my collection id\n",
      "\"c440aadc-2db4-472b-a042-bfca0310509d\"\n"
     ]
    }
   ],
   "source": [
    "print('3. my collection id')\n",
    "collections = discovery.list_collections(my_environment_id)\n",
    "my_collections = [x for x in collections['collections'] if x['name'] == discovery_secrets['collection']]\n",
    "my_collection_id = my_collections[0]['collection_id']\n",
    "print(json.dumps(my_collection_id, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('4. configurations ')\n",
    "configs = discovery.list_configurations(my_environment_id)\n",
    "print(json.dumps(configs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('6. collection details')\n",
    "collection = discovery.get_collection(my_environment_id, my_collection_id)\n",
    "print(json.dumps(collection, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. add a document\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xf7 in position 14: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f7270759a4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'7. add a document'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'caption.docx'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfileinfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0madd_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscovery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_environment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_collection_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfileinfo\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mfile_content_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/msword'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# os.path.join(os.getcwd(), '/Users/krishnendu/Study/TEC docs', 'dictionary-items-organization.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/watson_developer_cloud/discovery_v1.py\u001b[0m in \u001b[0;36madd_document\u001b[0;34m(self, environment_id, collection_id, file, metadata, file_content_type, filename, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m             files={'file': file_tuple,\n\u001b[1;32m    962\u001b[0m                    'metadata': metadata_tuple},\n\u001b[0;32m--> 963\u001b[0;31m             accept_json=True)\n\u001b[0m\u001b[1;32m    964\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/watson_developer_cloud/watson_service.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, accept_json, headers, params, json, data, files, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                                     \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                                     **kwargs)\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         )\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mcookies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_cookies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         )\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[0;34m(self, data, files, json)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;31m# Multi-part file uploads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/site-packages/requests/models.py\u001b[0m in \u001b[0;36m_encode_files\u001b[0;34m(files, data)\u001b[0m\n\u001b[1;32m    157\u001b[0m                 \u001b[0mfdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0mfdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequestField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/envs/TEC-Initiative/lib/python3.6/encodings/ascii.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascii_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xf7 in position 14: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "print('7. add a document')\n",
    "with open('caption.docx') as fileinfo:\n",
    "    add_doc = discovery.add_document(my_environment_id, my_collection_id, file=fileinfo , file_content_type='application/msword')\n",
    "print(json.dumps(add_doc, indent=2)) \n",
    "# os.path.join(os.getcwd(), '/Users/krishnendu/Study/TEC docs', 'dictionary-items-organization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('8.See a add document status')\n",
    "doc_info = discovery.get_document_status(my_environment_id, my_collection_id, '69362ab5-a4a7-4378-9093-b5611dafb1e1' )\n",
    "print(json.dumps(doc_info, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. Query \n",
      "{\n",
      "  \"matching_results\": 3,\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"id\": \"5c2dd9f3d1f9cac1b6f0ba93ea83c4f9\",\n",
      "      \"result_metadata\": {\n",
      "        \"score\": 1\n",
      "      },\n",
      "      \"extracted_metadata\": {\n",
      "        \"publicationdate\": \"2009-04-01\",\n",
      "        \"sha1\": \"200c6d9e75b0c14ab840573224db3a51aee0699d\",\n",
      "        \"author\": \"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\",\n",
      "        \"filename\": \"01bool.pdf\",\n",
      "        \"file_type\": \"pdf\",\n",
      "        \"title\": \"Introduction to Information Retrieval\"\n",
      "      },\n",
      "      \"html\": \"<?xml version='1.0' encoding='UTF-8' standalone='yes'?><html>\\n<head>\\n    <meta content=\\\"text/html; charset=UTF-8\\\" http-equiv=\\\"Content-Type\\\"/><meta content=\\\"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\\\" name=\\\"author\\\"/><meta content=\\\"2009-04-01\\\" name=\\\"publicationdate\\\"/><meta content=\\\"18\\\" name=\\\"numPages\\\"/><title>Introduction to Information Retrieval</title></head>\\n<body><h1><p>1 </p></h1><h3><p>Boolean retrieval\\n</p></h3><p>The meaning of the term <i>information retrieval </i>can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study,\\n<i>information retrieval </i>might be defined thus:INFORMATION\\n</p><p>RETRIEVAL\\n</p><p>Information retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).\\n</p><p>As defined in this way, information retrieval used to be an activity that only a few people engaged in: reference librarians, paralegals, and similar professional searchers. Now the world has changed, and hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email.1 Information retrieval is fast becoming the dominant form of information access, overtaking traditional database-style searching (the sort that is going on when a clerk says to you: \\\"I'm sorry, I can only look up your order if you can give me your Order ID\\\").\\n</p><p>IR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term \\\"unstructured data\\\" refers to data which does not have clear, semantically overt, easy-for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly \\\"unstructured\\\". This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web\\n</p><p>1. In modern parlance, the word \\\"search\\\" has tended to replace \\\"(information) retrieval\\\"; the term \\\"search\\\" is quite ambiguous, but in context we use the two synonymously.\\n</p><p>pages). IR is also used to facilitate \\\"semistructured\\\" search such as finding a document where the title contains Java and the body contains threading.\\n</p><p>The field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically.\\n</p><p>Information retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In <i>web search</i>, the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in Chapters 19-21. At the other extreme is <i>personal information retrieval</i>. In the last few years, consumer operating systems have integrated information retrieval (such as Apple's Mac OS X Spotlight or Windows Vista's Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of <i>enterprise, institutional, and domain-specific search</i>, where retrieval might be provided for collections such as a corporation's internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios.\\n</p><p>In this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1) and the central inverted index data structure (Section 1.2). We will then examine the Boolean retrieval model and how Boolean queries are processed (Sections 1.3 and 1.4).\\n</p><p><b>1.1 An example information retrieval problem\\n</b></p><p>A fat book which many people own is Shakespeare's Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar AND NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as <i>grepping </i>through text, after the Unix command grep, whichGREP\\n</p><p>performs this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of regular expressions. With modern computers, for simple querying of modest collections (the size of Shakespeare's Collected Works is a bit under one million words of text in total), you really need nothing more.\\n</p><p>But for many purposes, you do need more:\\n</p><p>1. To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words.\\n</p><p>2. To allow more flexible matching operations. For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as \\\"within 5 words\\\" or \\\"within the same sentence\\\".\\n</p><p>3. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words.\\n</p><p>The way to avoid linearly scanning the texts for each query is to <i>index </i>theINDEX documents in advance. Let us stick with Shakespeare's Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document - here a play of Shakespeare's - whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document <i>incidence</i>INCIDENCE MATRIX\\n</p><p><i>matrix</i>, as in Figure 1.1. <i>Terms </i>are the indexed units (further discussed inTERM Section 2.2); they are usually words, and for the moment you can think of\\nAntony Julius The Hamlet Othello Macbeth . . . and Caesar Tempest Cleopatra\\n</p><p>Antony 1 1 0 0 0 1\\n</p><p>Brutus 1 1 0 1 0 0\\n</p><p>Caesar 1 1 0 1 1 1\\n</p><p>Calpurnia 0 1 0 0 0 0\\n</p><p>Cleopatra 1 0 0 0 0 0\\n</p><p>mercy 1 0 1 1 1 1\\n</p><p>worser 1 0 1 1 1 0 . . .\\n</p><p>\\u25ee <b>Figure 1.1 </b>A term-document incidence matrix. Matrix element (<i>t</i>, <i>d</i>) is 1 if the play in column <i>d </i>contains the word in row <i>t</i>, and is 0 otherwise.\\n</p><p>them as words, but the information retrieval literature normally speaks of terms because some of them, such as perhaps I-9 or Hong Kong are not usually thought of as words. Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it.2\\n</p><p>To answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND:\\n</p><p>110100 AND 110111 AND 101111 = 100100\\n</p><p>The answers for this query are thus <i>Antony and Cleopatra </i>and <i>Hamlet </i>(Figure 1.2).\\n</p><p>The <i>Boolean retrieval model </i>is a model for information retrieval in which weBOOLEAN RETRIEVAL MODEL can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators AND, OR, and NOT.\\n</p><p>The model views each document as just a set of words.\\n</p><p>Let us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation. Suppose we have\\n<i>N </i>= 1 million documents. By <i>documents </i>we mean whatever units we haveDOCUMENT\\n</p><p>decided to build a retrieval system over. They might be individual memos or chapters of a book (see Section 2.1.2 (page 20) for further discussion). We will refer to the group of documents over which we perform retrieval as the (document) <i>collection</i>. It is sometimes also referred to as a <i>corpus </i>(a <i>body </i>ofCOLLECTION\\n</p><p>CORPUS texts). Suppose each document is about 1000 words long (2-3 book pages). If\\n</p><p>2. Formally, we take the transpose of the matrix to be able to get the terms as column vectors.\\n</p><p><i>Antony and Cleopatra, Act III, Scene ii\\n</i></p><p>Agrippa [Aside to Domitius Enobarbus]: Why, Enobarbus, When Antony found Julius Caesar dead,\\n</p><p>He cried almost to roaring; and he wept When at Philippi he found Brutus slain.\\n</p><p><i>Hamlet, Act III, Scene ii\\n</i></p><p>Lord Polonius: I did enact Julius Caesar: I was killed i' the Capitol; Brutus killed me.\\n</p><p>\\u25ee <b>Figure 1.2 </b>Results from Shakespeare for the query Brutus AND Caesar AND NOT Calpurnia.\\n</p><p>we assume an average of 6 bytes per word including spaces and punctuation, then this is a document collection about 6 GB in size. Typically, there might be about <i>M </i>= 500,000 distinct terms in these documents. There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle. We will discuss and model these size assumptions in Section 5.1 (page 86).\\n</p><p>Our goal is to develop a system to address the <i>ad hoc retrieval </i>task. This isAD HOC RETRIEVAL the most standard IR task. In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query. An\\n<i>information need </i>is the topic about which the user desires to know more, andINFORMATION NEED\\n</p><p>is differentiated from a <i>query</i>, which is what the user conveys to the com-QUERY puter in an attempt to communicate the information need. A document is\\n<i>relevant </i>if it is one that the user perceives as containing information of valueRELEVANCE\\n</p><p>with respect to their personal information need. Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like \\\"pipeline leaks\\\" and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture. To assess the <i>effectiveness </i>of an IR system (i.e., the quality ofEFFECTIVENESS\\n</p><p>its search results), a user will usually want to know two key statistics about the system's returned results for a query:\\n</p><p><i>Precision</i>: What fraction of the returned results are relevant to the informa-PRECISION tion need?\\n</p><p><i>Recall</i>: What fraction of the relevant documents in the collection were re-RECALL turned by the system?\\n</p><p>Detailed discussion of relevance and evaluation measures including precision and recall is found in Chapter 8.\\n</p><p>We now cannot build a term-document matrix in a naive way. A 500K\\u00d7 1M matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory. But the crucial observation is that the matrix is extremely sparse, that is, it has few non-zero entries. Because each document is 1000 words long, the matrix has no more than one billion 1's, so a minimum of 99.8% of the cells are zero. A much better representation is to record only the things that do occur, that is, the 1 positions.\\n</p><p>This idea is central to the first major concept in information retrieval, the\\n<i>inverted index</i>. The name is actually redundant: an index always maps backINVERTED INDEX\\n</p><p>from terms to the parts of a document where they occur. Nevertheless, <i>inverted index</i>, or sometimes <i>inverted file</i>, has become the standard term in information retrieval.3 The basic idea of an inverted index is shown in Figure 1.3. We keep a <i>dictionary </i>of terms (sometimes also referred to as a <i>vocabulary </i>orDICTIONARY\\n</p><p>VOCABULARY <i>lexicon</i>; in this book, we use <i>dictionary </i>for the data structure and <i>vocabulary\\n</i>LEXICON for the set of terms). Then for each term, we have a list that records which documents the term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called a <i>posting</i>.4 The list is then called a <i>postings</i>POSTING\\n</p><p>POSTINGS LIST <i>list </i>(or inverted list), and all the postings lists taken together are referred to as the <i>postings</i>. The dictionary in Figure 1.3 has been sorted alphabetically andPOSTINGS\\n</p><p>each postings list is sorted by document ID. We will see why this is useful in Section 1.3, below, but later we will also consider alternatives to doing this (Section 7.1.5).\\n</p><p><b>1.2 A first take at building an inverted index\\n</b></p><p>To gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are:\\n</p><p>1. Collect the documents to be indexed:\\n</p><p>Friends, Romans, countrymen. So let it be with Caesar . . .\\n</p><p>2. Tokenize the text, turning each document into a list of tokens:\\n</p><p>Friends Romans countrymen So . . .\\n</p><p>3. Some information retrieval researchers prefer the term inverted file, but expressions like index construction and index compression are much more common than inverted file construction and inverted file compression. For consistency, we use (inverted) index throughout this book.\\n</p><p>4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.\\n</p><p>Brutus \\u2212\\u2192 1 2 4 11 31 45 173 174\\n</p><p>Caesar \\u2212\\u2192 1 2 4 5 6 16 57 132 . . .\\n</p><p>Calpurnia \\u2212\\u2192 2 31 54 101\\n</p><p>...\\n</p><p>\\ufe38 \\ufe37\\ufe37 \\ufe38 \\ufe38 \\ufe37\\ufe37 \\ufe38\\n</p><p><b>Dictionary Postings\\n</b></p><p>\\u25ee <b>Figure 1.3 </b>The two parts of an inverted index. The dictionary is commonly kept in memory, with pointers to each postings list, which is stored on disk.\\n</p><p>3. Do linguistic preprocessing, producing a list of normalized tokens, which\\n</p><p>are the indexing terms: friend roman countryman so . . .\\n</p><p>4. Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings.\\n</p><p>We will define and discuss the earlier stages of processing, that is, steps 1-3, in Section 2.2 (page 22). Until then you can think of <i>tokens </i>and <i>normalized tokens </i>as also loosely equivalent to <i>words</i>. Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by sort-based indexing.\\n</p><p>Within a document collection, we assume that each document has a unique serial number, known as the document identifier (<i>docID</i>). During index con-DOCID\\n</p><p>struction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4. The core indexing step is <i>sorting </i>this listSORTING\\n</p><p>so that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4. Multiple occurrences of the same term from the same document are then merged.5 Instances of the same term are then grouped, and the result is split into a <i>dictionary </i>and <i>postings</i>, as shown in the right column of Figure 1.4. Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the <i>document frequency</i>, which is hereDOCUMENT\\n</p><p>FREQUENCY also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the 5. Unix users can note that these steps are similar to use of the sort and then uniq commands.\\n</p><p><b>Doc 1 Doc 2\\n</b></p><p>I did enact Julius Caesar: I was killed i' the Capitol; Brutus killed me.\\n</p><p>So let it be with Caesar. The noble Brutus hath told you Caesar was ambitious:\\n</p><p><b>term docID\\n</b></p><p>I 1\\n</p><p>did 1\\n</p><p>enact 1\\n</p><p>julius 1 caesar 1\\n</p><p>I 1\\n</p><p>was 1\\n</p><p>killed 1\\n</p><p>i' 1\\n</p><p>the 1\\n</p><p>capitol 1 brutus 1 killed 1\\n</p><p>me 1\\n</p><p>so 2\\n</p><p>let 2\\n</p><p>it 2\\n</p><p>be 2\\n</p><p>with 2 caesar 2 the 2\\n</p><p>noble 2 brutus 2 hath 2\\n</p><p>told 2\\n</p><p>you 2 caesar 2 was 2 ambitious 2\\n</p><p>=\\u21d2\\n</p><p><b>term docID\\n</b></p><p>ambitious 2 be 2\\n</p><p>brutus 1 brutus 2 capitol 1 caesar 1 caesar 2 caesar 2 did 1\\n</p><p>enact 1\\n</p><p>hath 1\\n</p><p>I 1\\n</p><p>I 1\\n</p><p>i' 1\\n</p><p>it 2\\n</p><p>julius 1\\n</p><p>killed 1\\n</p><p>killed 1\\n</p><p>let 2\\n</p><p>me 1\\n</p><p>noble 2\\n</p><p>so 2\\n</p><p>the 1\\n</p><p>the 2\\n</p><p>told 2\\n</p><p>you 2\\n</p><p>was 1\\n</p><p>was 2\\n</p><p>with 2\\n</p><p>=\\u21d2\\n</p><p><b>term doc. freq. </b>\\u2192 <b>postings lists\\n</b>ambitious 1 \\u2192 2 be 1 \\u2192 2 brutus 2 \\u2192 1 \\u2192 2 capitol 1 \\u2192 1 caesar 2 \\u2192 1 \\u2192 2\\n</p><p>did 1 \\u2192 1\\n</p><p>enact 1 \\u2192 1\\n</p><p>hath 1 \\u2192 2\\n</p><p>I 1 \\u2192 1\\n</p><p>i' 1 \\u2192 1\\n</p><p>it 1 \\u2192 2\\n</p><p>julius 1 \\u2192 1\\n</p><p>killed 1 \\u2192 1\\n</p><p>let 1 \\u2192 2\\n</p><p>me 1 \\u2192 1\\n</p><p>noble 1 \\u2192 2\\n</p><p>so 1 \\u2192 2\\n</p><p>the 2 \\u2192 1 \\u2192 2\\n</p><p>told 1 \\u2192 2\\n</p><p>you 1 \\u2192 2\\n</p><p>was 2 \\u2192 1 \\u2192 2\\n</p><p>with 1 \\u2192 2\\n</p><p>\\u25ee <b>Figure 1.4 </b>Building an index by sorting and grouping. The sequence of terms in each document, tagged by their documentID (left) is sorted alphabetically (middle). Instances of the same term are then grouped by word and then by documentID. The terms and documentIDs are then separated out (right). The dictionary stores the terms, and has a pointer to the postings list for each term. It commonly also stores other summary information such as, here, the document frequency of each term. We use this information for improving query time efficiency and, later, for weighting in ranked retrieval models. Each postings list stores the list of documents in which a term occurs, and may store other information such as the term frequency (the frequency of each term in each document) or the position(s) of the term in each document.\\n</p><p>search engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search.\\n</p><p>In the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory.\\n</p><p><i>? </i><b>Exercise 1.1 </b>[\\u22c6]Draw the inverted index that would be built for the following document collection. (See Figure 1.3 for an example.)\\n</p><p><b>Doc 1 </b>new home sales top forecasts\\n<b>Doc 2 </b>home sales rise in july\\n<b>Doc 3 </b>increase in home sales in july\\n<b>Doc 4 </b>july new home sales rise\\n</p><p><b>Exercise 1.2 </b>[\\u22c6]\\n</p><p>Consider these documents:\\n</p><p><b>Doc 1 </b>breakthrough drug for schizophrenia\\n<b>Doc 2 </b>new schizophrenia drug\\n</p><p><b>Doc 3 </b>new approach for treatment of schizophrenia\\n<b>Doc 4 </b>new hopes for schizophrenia patients\\n</p><p>a. Draw the term-document incidence matrix for this document collection.\\n</p><p>Brutus \\u2212\\u2192 1 \\u2192 2 \\u2192 4 \\u2192 11 \\u2192 31 \\u2192 45 \\u2192 173 \\u2192 174\\n</p><p>Calpurnia \\u2212\\u2192 2 \\u2192 31 \\u2192 54 \\u2192 101\\n</p><p>Intersection =\\u21d2 2 \\u2192 31\\n</p><p>\\u25ee <b>Figure 1.5 </b>Intersecting the postings lists for Brutus and Calpurnia from Figure 1.3.\\n</p><p>b. Draw the inverted index representation for this collection, as in Figure 1.3 (page 7).\\n</p><p><b>Exercise 1.3 </b>[\\u22c6]\\n</p><p>For the document collection shown in Exercise 1.2, what are the returned results for these queries:\\n</p><p>a. schizophrenia AND drug b. for AND NOT(drug OR approach)\\n</p><p><b>1.3 Processing Boolean queries\\n</b></p><p>How do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the <i>simple conjunctive query</i>:SIMPLE CONJUNCTIVE QUERIES\\n</p><p>(1.1) Brutus AND Calpurnia\\n</p><p>over the inverted index partially shown in Figure 1.3 (page 7). We:\\n</p><p>1. Locate Brutus in the Dictionary\\n</p><p>2. Retrieve its postings\\n</p><p>3. Locate Calpurnia in the Dictionary\\n</p><p>4. Retrieve its postings\\n</p><p>5. Intersect the two postings lists, as shown in Figure 1.5.\\n</p><p>The <i>intersection </i>operation is the crucial one: we need to efficiently intersectPOSTINGS LIST INTERSECTION postings lists so as to be able to quickly find documents that contain both terms. (This operation is sometimes referred to as <i>merging </i>postings lists:POSTINGS MERGE\\n</p><p>this slightly counterintuitive name reflects using the term <i>merge algorithm </i>for a general family of algorithms that combine multiple sorted lists by interleaved advancing of pointers through each; here we are merging the lists with a logical AND operation.)\\n</p><p>There is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6): we maintain pointers into both lists\\n</p><p>INTERSECT(<i>p</i>1, <i>p</i>2)\\n</p><p>1 <i>answer </i>\\u2190 \\u3008 \\u3009\\n</p><p>2 <b>while </b><i>p</i>1 6= NIL and <i>p</i>2 6= NIL\\n</p><p>3 <b>do if </b><i>docID</i>(<i>p</i>1) = <i>docID</i>(<i>p</i>2)\\n</p><p>4 <b>then </b>ADD(<i>answer</i>, <i>docID</i>(<i>p</i>1))\\n</p><p>5 <i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>6 <i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2)\\n</p><p>7 <b>else if </b><i>docID</i>(<i>p</i>1) &lt; <i>docID</i>(<i>p</i>2)\\n</p><p>8 <b>then </b><i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>9 <b>else </b><i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2) 10 <b>return </b><i>answer\\n</i></p><p>\\u25ee <b>Figure 1.6 </b>Algorithm for the intersection of two postings lists <i>p</i>1 and <i>p</i>2.\\n</p><p>and walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are <i>x </i>and\\n<i>y</i>, the intersection takes <i>O</i>(<i>x </i>+ <i>y</i>) operations. Formally, the complexity of querying is \\u0398(<i>N</i>), where <i>N </i>is the number of documents in the collection.6 Our indexing methods gain us just a constant, not a difference in \\u0398 time complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries like:\\n</p><p>(1.2) (Brutus OR Caesar) AND NOT Calpurnia\\n</p><p><i>Query optimization </i>is the process of selecting how to organize the work of an-QUERY OPTIMIZATION swering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of <i>t </i>terms, for instance:\\n</p><p>(1.3) Brutus AND Caesar AND Calpurnia\\n</p><p>For each of the <i>t </i>terms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document\\n</p><p>6. The notation \\u0398(\\u00b7) is used to express an asymptotically tight bound on the complexity of an algorithm. Informally, this is often written as <i>O</i>(\\u00b7), but this notation really expresses an asymptotic upper bound, which need not be tight (Cormen et al. 1990).\\n</p><p>INTERSECT(\\u3008<i>t</i>1, . . . , <i>t</i><i>n</i>\\u3009)\\n</p><p>1 <i>terms</i>\\u2190 SORTBYINCREASINGFREQUENCY(\\u3008<i>t</i>1, . . . , <i>t</i><i>n</i>\\u3009)\\n</p><p>2 <i>result </i>\\u2190 <i>postings</i>( <i>f irst</i>(<i>terms</i>))\\n</p><p>3 <i>terms</i>\\u2190 <i>rest</i>(<i>terms</i>)\\n</p><p>4 <b>while </b><i>terms </i>6= NIL and <i>result </i>6= NIL\\n</p><p>5 <b>do </b><i>result </i>\\u2190 INTERSECT(<i>result</i>, <i>postings</i>( <i>f irst</i>(<i>terms</i>)))\\n</p><p>6 <i>terms</i>\\u2190 <i>rest</i>(<i>terms</i>) 7 <b>return </b><i>result\\n</i></p><p>\\u25ee <b>Figure 1.7 </b>Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms.\\n</p><p>frequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 (page 7), we execute the above query as:\\n</p><p>(1.4) (Calpurnia AND Brutus) AND Caesar\\n</p><p>This is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list.\\n</p><p>Consider now the optimization of more general queries, such as: (1.5) (madding OR crowd) AND (ignoble OR strife) AND (killed OR slain)\\n</p><p>As before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term.\\n</p><p>For arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7. The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings\\nintersection can still be done by the algorithm in Figure 1.6, but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5. Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common.\\n</p><p><i>? </i><b>Exercise 1.4 </b>[\\u22c6]For the queries below, can we still run through the intersection in time <i>O</i>(<i>x </i>+ <i>y</i>), where <i>x </i>and <i>y </i>are the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve?\\n</p><p>a. Brutus AND NOT Caesar b. Brutus OR NOT Caesar\\n</p><p><b>Exercise 1.5 </b>[\\u22c6]\\n</p><p>Extend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider:\\n</p><p>c. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\\n</p><p>Can we always merge in linear time? Linear in what? Can we do better than this?\\n</p><p><b>Exercise 1.6 </b>[\\u22c6\\u22c6]\\n</p><p>We can use distributive laws for AND and OR to rewrite queries.\\n</p><p>a. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using the distributive laws.\\n</p><p>b. Would the resulting query be more or less efficiently evaluated than the original form of this query?\\n</p><p>c. Is this result true in general or does it depend on the words and the contents of the document collection?\\n</p><p><b>Exercise 1.7 </b>[\\u22c6]\\n</p><p>Recommend a query processing order for d. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)\\n</p><p>given the following postings list sizes:\\n</p><p><b>Term Postings size\\n</b></p><p>eyes 213312\\n</p><p>kaleidoscope 87009\\n</p><p>marmalade 107913\\n</p><p>skies 271658\\n</p><p>tangerine 46653\\n</p><p>trees 316812\\n</p><p><b>Exercise 1.8 </b>[\\u22c6]\\n</p><p>If the query is: e. friends AND romans AND (NOT countrymen)\\n</p><p>how could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing.\\n</p><p><b>Exercise 1.9 </b>[\\u22c6\\u22c6]\\n</p><p>For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn't.\\n</p><p><b>Exercise 1.10 </b>[\\u22c6\\u22c6]\\n</p><p>Write out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an <i>x </i>OR <i>y\\n</i>query.\\n</p><p><b>Exercise 1.11 </b>[\\u22c6\\u22c6]\\n</p><p>How should the Boolean query <i>x </i>AND NOT <i>y </i>be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently.\\n</p><p><b>1.4 The extended Boolean model versus ranked retrieval\\n</b></p><p>The Boolean retrieval model contrasts with <i>ranked retrieval models </i>such as theRANKED RETRIEVAL MODEL vector space model (Section 6.3), in which users largely use <i>free text queries</i>, FREE TEXT QUERIES that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boo-lean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A <i>proximity operator </i>is a way of specifying that two terms in a queryPROXIMITY OPERATOR\\nmust occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph.\\n</p><p>\\u270e <b>Example 1.1: Commercial Boolean searching: Westlaw. </b>Westlaw (http://www.westlaw.com/)is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called \\\"Terms and Connectors\\\" by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called \\\"Natural Language\\\" by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw:\\n</p><p><i>Information need: </i>Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company. <i>Query: </i>\\\"trade secret\\\" /s disclos! /s prevent /s employe!\\n</p><p><i>Information need: </i>Requirements for disabled people to be able to access a workplace.\\n</p><p><i>Query: </i>disab! /p access! /s work-site work-place (employment /3 place)\\n</p><p><i>Information need: </i>Cases about a host's responsibility for drunk guests.\\n<i>Query: </i>host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\\n</p><p>Note the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator), &amp; is AND and /s, /p, and /<i>k </i>ask for matches in the same sentence, same paragraph or within <i>k </i>words respectively. Double quotes give a <i>phrase search\\n</i>(consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trailing wildcard query (see Section 3.2, page 51); thus liab! matches all words starting with liab. Additionally work-site matches any of <i>worksite</i>, <i>work-site </i>or <i>work site</i>; see Section 2.2.1 (page 22). Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user.\\n</p><p>Many users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw's own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground.\\n</p><p>In this chapter, we have looked at the structure and construction of a basic\\ninverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In Chapters 2-7 we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do:\\n</p><p>1. We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words.\\n</p><p>2. It is often useful to search for compounds or phrases that denote a concept such as \\\"operating system\\\". As the Westlaw examples show, we might also wish to do proximity queries such as Gates NEAR Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents.\\n</p><p>3. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need <i>term frequency </i>information (the number of timesTERM FREQUENCY\\n</p><p>a term occurs in a document) in postings lists.\\n</p><p>4. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or \\\"rank\\\") the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query.\\n</p><p>With these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying, most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance.\\n</p><p><i>? </i><b>Exercise 1.12 </b>[\\u22c6]Write a query using Westlaw syntax which would find any of the words professor, teacher, or lecturer in the same sentence as a form of the verb explain.\\n</p><p><b>Exercise 1.13 </b>[\\u22c6]\\n</p><p>Try using the Boolean search features on a couple of major web search engines. For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and top hits. Do they make sense in terms of Boolean logic? Often they haven't for major search engines. Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer. What bound should the number of results from the first two queries place on the third query? Is this bound observed?\\n</p><p><b>1.5 References and further reading\\n</b></p><p>The practical pursuit of computerized information retrieval began in the late 1940s (Cleverdon 1991, Liddy 2005). A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval. However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later.\\n</p><p>The article of Bush (1945) provided lasting inspiration for the new field:\\n</p><p>\\\"Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, 'memex' will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.\\\"\\n</p><p>The term <i>Information Retrieval </i>was coined by Calvin Mooers in 1948/1950 (Mooers 1950).\\n</p><p>In 1958, much newspaper attention was paid to demonstrations at a conference (see Taube and Wooster 1958) of IBM \\\"auto-indexing\\\" machines, based primarily on the work of H. P. Luhn. Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems. For example Moo-ers (1961) dissented:\\n</p><p>\\\"It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design. This view is as widely and uncritically accepted as it is wrong.\\\"\\n</p><p>The observation of AND vs. OR giving you opposite extremes in a precision/ recall tradeoff, but not the middle ground comes from (Lee and Fox 1988).\\n</p><p>The book (Witten et al. 1999) is the standard reference for an in-depth comparison of the space and time efficiency of the inverted index versus other possible data structures; a more succinct and up-to-date presentation appears in Zobel and Moffat (2006). We further discuss several approaches in Chapter 5.\\n</p><p>Friedl (2006) covers the practical usage of <i>regular expressions </i>for searching.REGULAR EXPRESSIONS The underlying computer science appears in (Hopcroft et al. 2000).\\n</p></body></html>\",\n",
      "      \"text\": \"Introduction to Information Retrieval\\n\\n1\\n\\nBoolean retrieval\\n\\nThe meaning of the term information retrieval can be very broad. Just getting a credit card out of your wallet so that you can type in the card number is a form of information retrieval. However, as an academic field of study, information retrieval might be defined thus:INFORMATION\\n\\nRETRIEVAL\\n\\nInformation retrieval (IR) is finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need from within large collections (usually stored on computers).\\n\\nAs defined in this way, information retrieval used to be an activity that only a few people engaged in: reference librarians, paralegals, and similar professional searchers. Now the world has changed, and hundreds of millions of people engage in information retrieval every day when they use a web search engine or search their email.1 Information retrieval is fast becoming the dominant form of information access, overtaking traditional database-style searching (the sort that is going on when a clerk says to you: \\\"I'm sorry, I can only look up your order if you can give me your Order ID\\\").\\n\\nIR can also cover other kinds of data and information problems beyond that specified in the core definition above. The term \\\"unstructured data\\\" refers to data which does not have clear, semantically overt, easy-for-a-computer structure. It is the opposite of structured data, the canonical example of which is a relational database, of the sort companies usually use to maintain product inventories and personnel records. In reality, almost no data are truly \\\"unstructured\\\". This is definitely true of all text data if you count the latent linguistic structure of human languages. But even accepting that the intended notion of structure is overt structure, most text has structure, such as headings and paragraphs and footnotes, which is commonly represented in documents by explicit markup (such as the coding underlying web\\n\\n1. In modern parlance, the word \\\"search\\\" has tended to replace \\\"(information) retrieval\\\"; the term \\\"search\\\" is quite ambiguous, but in context we use the two synonymously.\\n\\npages). IR is also used to facilitate \\\"semistructured\\\" search such as finding a document where the title contains Java and the body contains threading.\\n\\nThe field of information retrieval also covers supporting users in browsing or filtering document collections or further processing a set of retrieved documents. Given a set of documents, clustering is the task of coming up with a good grouping of the documents based on their contents. It is similar to arranging books on a bookshelf according to their topic. Given a set of topics, standing information needs, or other categories (such as suitability of texts for different age groups), classification is the task of deciding which class(es), if any, each of a set of documents belongs to. It is often approached by first manually classifying some documents and then hoping to be able to classify new documents automatically.\\n\\nInformation retrieval systems can also be distinguished by the scale at which they operate, and it is useful to distinguish three prominent scales. In web search, the system has to provide search over billions of documents stored on millions of computers. Distinctive issues are needing to gather documents for indexing, being able to build systems that work efficiently at this enormous scale, and handling particular aspects of the web, such as the exploitation of hypertext and not being fooled by site providers manipulating page content in an attempt to boost their search engine rankings, given the commercial importance of the web. We focus on all these issues in Chapters 19-21. At the other extreme is personal information retrieval. In the last few years, consumer operating systems have integrated information retrieval (such as Apple's Mac OS X Spotlight or Windows Vista's Instant Search). Email programs usually not only provide search but also text classification: they at least provide a spam (junk mail) filter, and commonly also provide either manual or automatic means for classifying mail so that it can be placed directly into particular folders. Distinctive issues here include handling the broad range of document types on a typical personal computer, and making the search system maintenance free and sufficiently lightweight in terms of startup, processing, and disk space usage that it can run on one machine without annoying its owner. In between is the space of enterprise, institutional, and domain-specific search, where retrieval might be provided for collections such as a corporation's internal documents, a database of patents, or research articles on biochemistry. In this case, the documents will typically be stored on centralized file systems and one or a handful of dedicated machines will provide search over the collection. This book contains techniques of value over this whole spectrum, but our coverage of some aspects of parallel and distributed search in web-scale search systems is comparatively light owing to the relatively small published literature on the details of such systems. However, outside of a handful of web search companies, a software developer is most likely to encounter the personal search and enterprise scenarios.\\n\\nIn this chapter we begin with a very simple example of an information retrieval problem, and introduce the idea of a term-document matrix (Section 1.1) and the central inverted index data structure (Section 1.2). We will then examine the Boolean retrieval model and how Boolean queries are processed (Sections 1.3 and 1.4).\\n\\n1.1 An example information retrieval problem\\n\\nA fat book which many people own is Shakespeare's Collected Works. Suppose you wanted to determine which plays of Shakespeare contain the words Brutus AND Caesar AND NOT Calpurnia. One way to do that is to start at the beginning and to read through all the text, noting for each play whether it contains Brutus and Caesar and excluding it from consideration if it contains Calpurnia. The simplest form of document retrieval is for a computer to do this sort of linear scan through documents. This process is commonly referred to as grepping through text, after the Unix command grep, whichGREP\\n\\nperforms this process. Grepping through text can be a very effective process, especially given the speed of modern computers, and often allows useful possibilities for wildcard pattern matching through the use of regular expressions. With modern computers, for simple querying of modest collections (the size of Shakespeare's Collected Works is a bit under one million words of text in total), you really need nothing more.\\n\\nBut for many purposes, you do need more:\\n\\n1. To process large document collections quickly. The amount of online data has grown at least as quickly as the speed of computers, and we would now like to be able to search collections that total in the order of billions to trillions of words.\\n\\n2. To allow more flexible matching operations. For example, it is impractical to perform the query Romans NEAR countrymen with grep, where NEAR might be defined as \\\"within 5 words\\\" or \\\"within the same sentence\\\".\\n\\n3. To allow ranked retrieval: in many cases you want the best answer to an information need among many documents that contain certain words.\\n\\nThe way to avoid linearly scanning the texts for each query is to index theINDEX documents in advance. Let us stick with Shakespeare's Collected Works, and use it to introduce the basics of the Boolean retrieval model. Suppose we record for each document - here a play of Shakespeare's - whether it contains each word out of all the words Shakespeare used (Shakespeare used about 32,000 different words). The result is a binary term-document incidenceINCIDENCE MATRIX\\n\\nmatrix, as in Figure 1.1. Terms are the indexed units (further discussed inTERM Section 2.2); they are usually words, and for the moment you can think of Antony Julius The Hamlet Othello Macbeth . . . and Caesar Tempest Cleopatra\\n\\nAntony 1 1 0 0 0 1\\n\\nBrutus 1 1 0 1 0 0\\n\\nCaesar 1 1 0 1 1 1\\n\\nCalpurnia 0 1 0 0 0 0\\n\\nCleopatra 1 0 0 0 0 0\\n\\nmercy 1 0 1 1 1 1\\n\\nworser 1 0 1 1 1 0 . . .\\n\\n\\u25ee Figure 1.1 A term-document incidence matrix. Matrix element (t, d) is 1 if the play in column d contains the word in row t, and is 0 otherwise.\\n\\nthem as words, but the information retrieval literature normally speaks of terms because some of them, such as perhaps I-9 or Hong Kong are not usually thought of as words. Now, depending on whether we look at the matrix rows or columns, we can have a vector for each term, which shows the documents it appears in, or a vector for each document, showing the terms that occur in it.2\\n\\nTo answer the query Brutus AND Caesar AND NOT Calpurnia, we take the vectors for Brutus, Caesar and Calpurnia, complement the last, and then do a bitwise AND:\\n\\n110100 AND 110111 AND 101111 = 100100\\n\\nThe answers for this query are thus Antony and Cleopatra and Hamlet (Figure 1.2).\\n\\nThe Boolean retrieval model is a model for information retrieval in which weBOOLEAN RETRIEVAL MODEL can pose any query which is in the form of a Boolean expression of terms, that is, in which terms are combined with the operators AND, OR, and NOT.\\n\\nThe model views each document as just a set of words.\\n\\nLet us now consider a more realistic scenario, simultaneously using the opportunity to introduce some terminology and notation. Suppose we have N = 1 million documents. By documents we mean whatever units we haveDOCUMENT\\n\\ndecided to build a retrieval system over. They might be individual memos or chapters of a book (see Section 2.1.2 (page 20) for further discussion). We will refer to the group of documents over which we perform retrieval as the (document) collection. It is sometimes also referred to as a corpus (a body ofCOLLECTION\\n\\nCORPUS texts). Suppose each document is about 1000 words long (2-3 book pages). If\\n\\n2. Formally, we take the transpose of the matrix to be able to get the terms as column vectors.\\n\\nAntony and Cleopatra, Act III, Scene ii\\n\\nAgrippa [Aside to Domitius Enobarbus]: Why, Enobarbus, When Antony found Julius Caesar dead,\\n\\nHe cried almost to roaring; and he wept When at Philippi he found Brutus slain.\\n\\nHamlet, Act III, Scene ii\\n\\nLord Polonius: I did enact Julius Caesar: I was killed i' the Capitol; Brutus killed me.\\n\\n\\u25ee Figure 1.2 Results from Shakespeare for the query Brutus AND Caesar AND NOT Calpurnia.\\n\\nwe assume an average of 6 bytes per word including spaces and punctuation, then this is a document collection about 6 GB in size. Typically, there might be about M = 500,000 distinct terms in these documents. There is nothing special about the numbers we have chosen, and they might vary by an order of magnitude or more, but they give us some idea of the dimensions of the kinds of problems we need to handle. We will discuss and model these size assumptions in Section 5.1 (page 86).\\n\\nOur goal is to develop a system to address the ad hoc retrieval task. This isAD HOC RETRIEVAL the most standard IR task. In it, a system aims to provide documents from within the collection that are relevant to an arbitrary user information need, communicated to the system by means of a one-off, user-initiated query. An information need is the topic about which the user desires to know more, andINFORMATION NEED\\n\\nis differentiated from a query, which is what the user conveys to the com-QUERY puter in an attempt to communicate the information need. A document is relevant if it is one that the user perceives as containing information of valueRELEVANCE\\n\\nwith respect to their personal information need. Our example above was rather artificial in that the information need was defined in terms of particular words, whereas usually a user is interested in a topic like \\\"pipeline leaks\\\" and would like to find relevant documents regardless of whether they precisely use those words or express the concept with other words such as pipeline rupture. To assess the effectiveness of an IR system (i.e., the quality ofEFFECTIVENESS\\n\\nits search results), a user will usually want to know two key statistics about the system's returned results for a query:\\n\\nPrecision: What fraction of the returned results are relevant to the informa-PRECISION tion need?\\n\\nRecall: What fraction of the relevant documents in the collection were re-RECALL turned by the system?\\n\\nDetailed discussion of relevance and evaluation measures including precision and recall is found in Chapter 8.\\n\\nWe now cannot build a term-document matrix in a naive way. A 500K\\u00d7 1M matrix has half-a-trillion 0's and 1's - too many to fit in a computer's memory. But the crucial observation is that the matrix is extremely sparse, that is, it has few non-zero entries. Because each document is 1000 words long, the matrix has no more than one billion 1's, so a minimum of 99.8% of the cells are zero. A much better representation is to record only the things that do occur, that is, the 1 positions.\\n\\nThis idea is central to the first major concept in information retrieval, the inverted index. The name is actually redundant: an index always maps backINVERTED INDEX\\n\\nfrom terms to the parts of a document where they occur. Nevertheless, inverted index, or sometimes inverted file, has become the standard term in information retrieval.3 The basic idea of an inverted index is shown in Figure 1.3. We keep a dictionary of terms (sometimes also referred to as a vocabulary orDICTIONARY\\n\\nVOCABULARY lexicon; in this book, we use dictionary for the data structure and vocabulary LEXICON for the set of terms). Then for each term, we have a list that records which documents the term occurs in. Each item in the list - which records that a term appeared in a document (and, later, often, the positions in the document) - is conventionally called a posting.4 The list is then called a postingsPOSTING\\n\\nPOSTINGS LIST list (or inverted list), and all the postings lists taken together are referred to as the postings. The dictionary in Figure 1.3 has been sorted alphabetically andPOSTINGS\\n\\neach postings list is sorted by document ID. We will see why this is useful in Section 1.3, below, but later we will also consider alternatives to doing this (Section 7.1.5).\\n\\n1.2 A first take at building an inverted index\\n\\nTo gain the speed benefits of indexing at retrieval time, we have to build the index in advance. The major steps in this are:\\n\\n1. Collect the documents to be indexed:\\n\\nFriends, Romans, countrymen. So let it be with Caesar . . .\\n\\n2. Tokenize the text, turning each document into a list of tokens:\\n\\nFriends Romans countrymen So . . .\\n\\n3. Some information retrieval researchers prefer the term inverted file, but expressions like index construction and index compression are much more common than inverted file construction and inverted file compression. For consistency, we use (inverted) index throughout this book.\\n\\n4. In a (non-positional) inverted index, a posting is just a document ID, but it is inherently associated with a term, via the postings list it is placed on; sometimes we will also talk of a (term, docID) pair as a posting.\\n\\nBrutus \\u2212\\u2192 1 2 4 11 31 45 173 174\\n\\nCaesar \\u2212\\u2192 1 2 4 5 6 16 57 132 . . .\\n\\nCalpurnia \\u2212\\u2192 2 31 54 101\\n\\n...\\n\\n\\ufe38 \\ufe37\\ufe37 \\ufe38 \\ufe38 \\ufe37\\ufe37 \\ufe38\\n\\nDictionary Postings\\n\\n\\u25ee Figure 1.3 The two parts of an inverted index. The dictionary is commonly kept in memory, with pointers to each postings list, which is stored on disk.\\n\\n3. Do linguistic preprocessing, producing a list of normalized tokens, which\\n\\nare the indexing terms: friend roman countryman so . . .\\n\\n4. Index the documents that each term occurs in by creating an inverted index, consisting of a dictionary and postings.\\n\\nWe will define and discuss the earlier stages of processing, that is, steps 1-3, in Section 2.2 (page 22). Until then you can think of tokens and normalized tokens as also loosely equivalent to words. Here, we assume that the first 3 steps have already been done, and we examine building a basic inverted index by sort-based indexing.\\n\\nWithin a document collection, we assume that each document has a unique serial number, known as the document identifier (docID). During index con-DOCID\\n\\nstruction, we can simply assign successive integers to each new document when it is first encountered. The input to indexing is a list of normalized tokens for each document, which we can equally think of as a list of pairs of term and docID, as in Figure 1.4. The core indexing step is sorting this listSORTING\\n\\nso that the terms are alphabetical, giving us the representation in the middle column of Figure 1.4. Multiple occurrences of the same term from the same document are then merged.5 Instances of the same term are then grouped, and the result is split into a dictionary and postings, as shown in the right column of Figure 1.4. Since a term generally occurs in a number of documents, this data organization already reduces the storage requirements of the index. The dictionary also records some statistics, such as the number of documents which contain each term (the document frequency, which is hereDOCUMENT\\n\\nFREQUENCY also the length of each postings list). This information is not vital for a basic Boolean search engine, but it allows us to improve the efficiency of the 5. Unix users can note that these steps are similar to use of the sort and then uniq commands.\\n\\nDoc 1 Doc 2\\n\\nI did enact Julius Caesar: I was killed i' the Capitol; Brutus killed me.\\n\\nSo let it be with Caesar. The noble Brutus hath told you Caesar was ambitious:\\n\\nterm docID\\n\\nI 1\\n\\ndid 1\\n\\nenact 1\\n\\njulius 1 caesar 1\\n\\nI 1\\n\\nwas 1\\n\\nkilled 1\\n\\ni' 1\\n\\nthe 1\\n\\ncapitol 1 brutus 1 killed 1\\n\\nme 1\\n\\nso 2\\n\\nlet 2\\n\\nit 2\\n\\nbe 2\\n\\nwith 2 caesar 2 the 2\\n\\nnoble 2 brutus 2 hath 2\\n\\ntold 2\\n\\nyou 2 caesar 2 was 2 ambitious 2\\n\\n=\\u21d2\\n\\nterm docID\\n\\nambitious 2 be 2\\n\\nbrutus 1 brutus 2 capitol 1 caesar 1 caesar 2 caesar 2 did 1\\n\\nenact 1\\n\\nhath 1\\n\\nI 1\\n\\nI 1\\n\\ni' 1\\n\\nit 2\\n\\njulius 1\\n\\nkilled 1\\n\\nkilled 1\\n\\nlet 2\\n\\nme 1\\n\\nnoble 2\\n\\nso 2\\n\\nthe 1\\n\\nthe 2\\n\\ntold 2\\n\\nyou 2\\n\\nwas 1\\n\\nwas 2\\n\\nwith 2\\n\\n=\\u21d2\\n\\nterm doc. freq. \\u2192 postings lists ambitious 1 \\u2192 2 be 1 \\u2192 2 brutus 2 \\u2192 1 \\u2192 2 capitol 1 \\u2192 1 caesar 2 \\u2192 1 \\u2192 2\\n\\ndid 1 \\u2192 1\\n\\nenact 1 \\u2192 1\\n\\nhath 1 \\u2192 2\\n\\nI 1 \\u2192 1\\n\\ni' 1 \\u2192 1\\n\\nit 1 \\u2192 2\\n\\njulius 1 \\u2192 1\\n\\nkilled 1 \\u2192 1\\n\\nlet 1 \\u2192 2\\n\\nme 1 \\u2192 1\\n\\nnoble 1 \\u2192 2\\n\\nso 1 \\u2192 2\\n\\nthe 2 \\u2192 1 \\u2192 2\\n\\ntold 1 \\u2192 2\\n\\nyou 1 \\u2192 2\\n\\nwas 2 \\u2192 1 \\u2192 2\\n\\nwith 1 \\u2192 2\\n\\n\\u25ee Figure 1.4 Building an index by sorting and grouping. The sequence of terms in each document, tagged by their documentID (left) is sorted alphabetically (middle). Instances of the same term are then grouped by word and then by documentID. The terms and documentIDs are then separated out (right). The dictionary stores the terms, and has a pointer to the postings list for each term. It commonly also stores other summary information such as, here, the document frequency of each term. We use this information for improving query time efficiency and, later, for weighting in ranked retrieval models. Each postings list stores the list of documents in which a term occurs, and may store other information such as the term frequency (the frequency of each term in each document) or the position(s) of the term in each document.\\n\\nsearch engine at query time, and it is a statistic later used in many ranked retrieval models. The postings are secondarily sorted by docID. This provides the basis for efficient query processing. This inverted index structure is essentially without rivals as the most efficient structure for supporting ad hoc text search.\\n\\nIn the resulting index, we pay for storage of both the dictionary and the postings lists. The latter are much larger, but the dictionary is commonly kept in memory, while postings lists are normally kept on disk, so the size of each is important, and in Chapter 5 we will examine how each can be optimized for storage and access efficiency. What data structure should be used for a postings list? A fixed length array would be wasteful as some words occur in many documents, and others in very few. For an in-memory postings list, two good alternatives are singly linked lists or variable length arrays. Singly linked lists allow cheap insertion of documents into postings lists (following updates, such as when recrawling the web for updated documents), and naturally extend to more advanced indexing strategies such as skip lists (Section 2.3), which require additional pointers. Variable length arrays win in space requirements by avoiding the overhead for pointers and in time requirements because their use of contiguous memory increases speed on modern processors with memory caches. Extra pointers can in practice be encoded into the lists as offsets. If updates are relatively infrequent, variable length arrays will be more compact and faster to traverse. We can also use a hybrid scheme with a linked list of fixed length arrays for each term. When postings lists are stored on disk, they are stored (perhaps compressed) as a contiguous run of postings without explicit pointers (as in Figure 1.3), so as to minimize the size of the postings list and the number of disk seeks to read a postings list into memory.\\n\\n? Exercise 1.1 [\\u22c6]Draw the inverted index that would be built for the following document collection. (See Figure 1.3 for an example.)\\n\\nDoc 1 new home sales top forecasts Doc 2 home sales rise in july Doc 3 increase in home sales in july Doc 4 july new home sales rise\\n\\nExercise 1.2 [\\u22c6]\\n\\nConsider these documents:\\n\\nDoc 1 breakthrough drug for schizophrenia Doc 2 new schizophrenia drug\\n\\nDoc 3 new approach for treatment of schizophrenia Doc 4 new hopes for schizophrenia patients\\n\\na. Draw the term-document incidence matrix for this document collection.\\n\\nBrutus \\u2212\\u2192 1 \\u2192 2 \\u2192 4 \\u2192 11 \\u2192 31 \\u2192 45 \\u2192 173 \\u2192 174\\n\\nCalpurnia \\u2212\\u2192 2 \\u2192 31 \\u2192 54 \\u2192 101\\n\\nIntersection =\\u21d2 2 \\u2192 31\\n\\n\\u25ee Figure 1.5 Intersecting the postings lists for Brutus and Calpurnia from Figure 1.3.\\n\\nb. Draw the inverted index representation for this collection, as in Figure 1.3 (page 7).\\n\\nExercise 1.3 [\\u22c6]\\n\\nFor the document collection shown in Exercise 1.2, what are the returned results for these queries:\\n\\na. schizophrenia AND drug b. for AND NOT(drug OR approach)\\n\\n1.3 Processing Boolean queries\\n\\nHow do we process a query using an inverted index and the basic Boolean retrieval model? Consider processing the simple conjunctive query:SIMPLE CONJUNCTIVE QUERIES\\n\\n(1.1) Brutus AND Calpurnia\\n\\nover the inverted index partially shown in Figure 1.3 (page 7). We:\\n\\n1. Locate Brutus in the Dictionary\\n\\n2. Retrieve its postings\\n\\n3. Locate Calpurnia in the Dictionary\\n\\n4. Retrieve its postings\\n\\n5. Intersect the two postings lists, as shown in Figure 1.5.\\n\\nThe intersection operation is the crucial one: we need to efficiently intersectPOSTINGS LIST INTERSECTION postings lists so as to be able to quickly find documents that contain both terms. (This operation is sometimes referred to as merging postings lists:POSTINGS MERGE\\n\\nthis slightly counterintuitive name reflects using the term merge algorithm for a general family of algorithms that combine multiple sorted lists by interleaved advancing of pointers through each; here we are merging the lists with a logical AND operation.)\\n\\nThere is a simple and effective method of intersecting postings lists using the merge algorithm (see Figure 1.6): we maintain pointers into both lists\\n\\nINTERSECT(p1, p2)\\n\\n1 answer \\u2190 \\u3008 \\u3009\\n\\n2 while p1 6= NIL and p2 6= NIL\\n\\n3 do if docID(p1) = docID(p2)\\n\\n4 then ADD(answer, docID(p1))\\n\\n5 p1 \\u2190 next(p1)\\n\\n6 p2 \\u2190 next(p2)\\n\\n7 else if docID(p1) < docID(p2)\\n\\n8 then p1 \\u2190 next(p1)\\n\\n9 else p2 \\u2190 next(p2) 10 return answer\\n\\n\\u25ee Figure 1.6 Algorithm for the intersection of two postings lists p1 and p2.\\n\\nand walk through the two postings lists simultaneously, in time linear in the total number of postings entries. At each step, we compare the docID pointed to by both pointers. If they are the same, we put that docID in the results list, and advance both pointers. Otherwise we advance the pointer pointing to the smaller docID. If the lengths of the postings lists are x and y, the intersection takes O(x + y) operations. Formally, the complexity of querying is \\u0398(N), where N is the number of documents in the collection.6 Our indexing methods gain us just a constant, not a difference in \\u0398 time complexity compared to a linear scan, but in practice the constant is huge. To use this algorithm, it is crucial that postings be sorted by a single global ordering. Using a numeric sort by docID is one simple way to achieve this. We can extend the intersection operation to process more complicated queries like:\\n\\n(1.2) (Brutus OR Caesar) AND NOT Calpurnia\\n\\nQuery optimization is the process of selecting how to organize the work of an-QUERY OPTIMIZATION swering a query so that the least total amount of work needs to be done by the system. A major element of this for Boolean queries is the order in which postings lists are accessed. What is the best order for query processing? Consider a query that is an AND of t terms, for instance:\\n\\n(1.3) Brutus AND Caesar AND Calpurnia\\n\\nFor each of the t terms, we need to get its postings, then AND them together. The standard heuristic is to process terms in order of increasing document\\n\\n6. The notation \\u0398(\\u00b7) is used to express an asymptotically tight bound on the complexity of an algorithm. Informally, this is often written as O(\\u00b7), but this notation really expresses an asymptotic upper bound, which need not be tight (Cormen et al. 1990).\\n\\nINTERSECT(\\u3008t1, . . . , tn\\u3009)\\n\\n1 terms\\u2190 SORTBYINCREASINGFREQUENCY(\\u3008t1, . . . , tn\\u3009)\\n\\n2 result \\u2190 postings( f irst(terms))\\n\\n3 terms\\u2190 rest(terms)\\n\\n4 while terms 6= NIL and result 6= NIL\\n\\n5 do result \\u2190 INTERSECT(result, postings( f irst(terms)))\\n\\n6 terms\\u2190 rest(terms) 7 return result\\n\\n\\u25ee Figure 1.7 Algorithm for conjunctive queries that returns the set of documents containing each term in the input list of terms.\\n\\nfrequency: if we start by intersecting the two smallest postings lists, then all intermediate results must be no bigger than the smallest postings list, and we are therefore likely to do the least amount of total work. So, for the postings lists in Figure 1.3 (page 7), we execute the above query as:\\n\\n(1.4) (Calpurnia AND Brutus) AND Caesar\\n\\nThis is a first justification for keeping the frequency of terms in the dictionary: it allows us to make this ordering decision based on in-memory data before accessing any postings list.\\n\\nConsider now the optimization of more general queries, such as: (1.5) (madding OR crowd) AND (ignoble OR strife) AND (killed OR slain)\\n\\nAs before, we will get the frequencies for all terms, and we can then (conservatively) estimate the size of each OR by the sum of the frequencies of its disjuncts. We can then process the query in increasing order of the size of each disjunctive term.\\n\\nFor arbitrary Boolean queries, we have to evaluate and temporarily store the answers for intermediate expressions in a complex expression. However, in many circumstances, either because of the nature of the query language, or just because this is the most common type of query that users submit, a query is purely conjunctive. In this case, rather than viewing merging postings lists as a function with two inputs and a distinct output, it is more efficient to intersect each retrieved postings list with the current intermediate result in memory, where we initialize the intermediate result by loading the postings list of the least frequent term. This algorithm is shown in Figure 1.7. The intersection operation is then asymmetric: the intermediate results list is in memory while the list it is being intersected with is being read from disk. Moreover the intermediate results list is always at least as short as the other list, and in many cases it is orders of magnitude shorter. The postings intersection can still be done by the algorithm in Figure 1.6, but when the difference between the list lengths is very large, opportunities to use alternative techniques open up. The intersection can be calculated in place by destructively modifying or marking invalid items in the intermediate results list. Or the intersection can be done as a sequence of binary searches in the long postings lists for each posting in the intermediate results list. Another possibility is to store the long postings list as a hashtable, so that membership of an intermediate result item can be calculated in constant rather than linear or log time. However, such alternative techniques are difficult to combine with postings list compression of the sort discussed in Chapter 5. Moreover, standard postings list intersection operations remain necessary when both terms of a query are very common.\\n\\n? Exercise 1.4 [\\u22c6]For the queries below, can we still run through the intersection in time O(x + y), where x and y are the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve?\\n\\na. Brutus AND NOT Caesar b. Brutus OR NOT Caesar\\n\\nExercise 1.5 [\\u22c6]\\n\\nExtend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider:\\n\\nc. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\\n\\nCan we always merge in linear time? Linear in what? Can we do better than this?\\n\\nExercise 1.6 [\\u22c6\\u22c6]\\n\\nWe can use distributive laws for AND and OR to rewrite queries.\\n\\na. Show how to rewrite the query in Exercise 1.5 into disjunctive normal form using the distributive laws.\\n\\nb. Would the resulting query be more or less efficiently evaluated than the original form of this query?\\n\\nc. Is this result true in general or does it depend on the words and the contents of the document collection?\\n\\nExercise 1.7 [\\u22c6]\\n\\nRecommend a query processing order for d. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes)\\n\\ngiven the following postings list sizes:\\n\\nTerm Postings size\\n\\neyes 213312\\n\\nkaleidoscope 87009\\n\\nmarmalade 107913\\n\\nskies 271658\\n\\ntangerine 46653\\n\\ntrees 316812\\n\\nExercise 1.8 [\\u22c6]\\n\\nIf the query is: e. friends AND romans AND (NOT countrymen)\\n\\nhow could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing.\\n\\nExercise 1.9 [\\u22c6\\u22c6]\\n\\nFor a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn't.\\n\\nExercise 1.10 [\\u22c6\\u22c6]\\n\\nWrite out a postings merge algorithm, in the style of Figure 1.6 (page 11), for an x OR y query.\\n\\nExercise 1.11 [\\u22c6\\u22c6]\\n\\nHow should the Boolean query x AND NOT y be handled? Why is naive evaluation of this query normally very expensive? Write out a postings merge algorithm that evaluates this query efficiently.\\n\\n1.4 The extended Boolean model versus ranked retrieval\\n\\nThe Boolean retrieval model contrasts with ranked retrieval models such as theRANKED RETRIEVAL MODEL vector space model (Section 6.3), in which users largely use free text queries, FREE TEXT QUERIES that is, just typing one or more words rather than using a precise language with operators for building up query expressions, and the system decides which documents best satisfy the query. Despite decades of academic research on the advantages of ranked retrieval, systems implementing the Boo-lean retrieval model were the main or only search option provided by large commercial information providers for three decades until the early 1990s (approximately the date of arrival of the World Wide Web). However, these systems did not have just the basic Boolean operations (AND, OR, and NOT) which we have presented so far. A strict Boolean expression over terms with an unordered results set is too limited for many of the information needs that people have, and these systems implemented extended Boolean retrieval models by incorporating additional operators such as term proximity operators. A proximity operator is a way of specifying that two terms in a queryPROXIMITY OPERATOR must occur close to each other in a document, where closeness may be measured by limiting the allowed number of intervening words or by reference to a structural unit such as a sentence or paragraph.\\n\\n\\u270e Example 1.1: Commercial Boolean searching: Westlaw. Westlaw (http://www.westlaw.com/)is the largest commercial legal search service (in terms of the number of paying subscribers), with over half a million subscribers performing millions of searches a day over tens of terabytes of text data. The service was started in 1975. In 2005, Boolean search (called \\\"Terms and Connectors\\\" by Westlaw) was still the default, and used by a large percentage of users, although ranked free text querying (called \\\"Natural Language\\\" by Westlaw) was added in 1992. Here are some example Boolean queries on Westlaw:\\n\\nInformation need: Information on the legal theories involved in preventing the disclosure of trade secrets by employees formerly employed by a competing company. Query: \\\"trade secret\\\" /s disclos! /s prevent /s employe!\\n\\nInformation need: Requirements for disabled people to be able to access a workplace.\\n\\nQuery: disab! /p access! /s work-site work-place (employment /3 place)\\n\\nInformation need: Cases about a host's responsibility for drunk guests. Query: host! /p (responsib! liab!) /p (intoxicat! drunk!) /p guest\\n\\nNote the long, precise queries and the use of proximity operators, both uncommon in web search. Submitted queries average about ten words in length. Unlike web search conventions, a space between words represents disjunction (the tightest binding operator), & is AND and /s, /p, and /k ask for matches in the same sentence, same paragraph or within k words respectively. Double quotes give a phrase search (consecutive words); see Section 2.4 (page 39). The exclamation mark (!) gives a trailing wildcard query (see Section 3.2, page 51); thus liab! matches all words starting with liab. Additionally work-site matches any of worksite, work-site or work site; see Section 2.2.1 (page 22). Typical expert queries are usually carefully defined and incrementally developed until they obtain what look to be good results to the user.\\n\\nMany users, particularly professionals, prefer Boolean query models. Boolean queries are precise: a document either matches the query or it does not. This offers the user greater control and transparency over what is retrieved. And some domains, such as legal materials, allow an effective means of document ranking within a Boolean model: Westlaw returns documents in reverse chronological order, which is in practice quite effective. In 2007, the majority of law librarians still seem to recommend terms and connectors for high recall searches, and the majority of legal users think they are getting greater control by using them. However, this does not mean that Boolean queries are more effective for professional searchers. Indeed, experimenting on a Westlaw subcollection, Turtle (1994) found that free text queries produced better results than Boolean queries prepared by Westlaw's own reference librarians for the majority of the information needs in his experiments. A general problem with Boolean search is that using AND operators tends to produce high precision but low recall searches, while using OR operators gives low precision but high recall searches, and it is difficult or impossible to find a satisfactory middle ground.\\n\\nIn this chapter, we have looked at the structure and construction of a basic inverted index, comprising a dictionary and postings lists. We introduced the Boolean retrieval model, and examined how to do efficient retrieval via linear time merges and simple query optimization. In Chapters 2-7 we will consider in detail richer query models and the sort of augmented index structures that are needed to handle them efficiently. Here we just mention a few of the main additional things we would like to be able to do:\\n\\n1. We would like to better determine the set of terms in the dictionary and to provide retrieval that is tolerant to spelling mistakes and inconsistent choice of words.\\n\\n2. It is often useful to search for compounds or phrases that denote a concept such as \\\"operating system\\\". As the Westlaw examples show, we might also wish to do proximity queries such as Gates NEAR Microsoft. To answer such queries, the index has to be augmented to capture the proximities of terms in documents.\\n\\n3. A Boolean model only records term presence or absence, but often we would like to accumulate evidence, giving more weight to documents that have a term several times as opposed to ones that contain it only once. To be able to do this we need term frequency information (the number of timesTERM FREQUENCY\\n\\na term occurs in a document) in postings lists.\\n\\n4. Boolean queries just retrieve a set of matching documents, but commonly we wish to have an effective method to order (or \\\"rank\\\") the returned results. This requires having a mechanism for determining a document score which encapsulates how good a match a document is for a query.\\n\\nWith these additional ideas, we will have seen most of the basic technology that supports ad hoc searching over unstructured information. Ad hoc searching over documents has recently conquered the world, powering not only web search engines but the kind of unstructured search that lies behind the large eCommerce websites. Although the main web search engines differ by emphasizing free text querying, most of the basic issues and technologies of indexing and querying remain the same, as we will see in later chapters. Moreover, over time, web search engines have added at least partial implementations of some of the most popular operators from extended Boolean models: phrase search is especially popular and most have a very partial implementation of Boolean operators. Nevertheless, while these options are liked by expert searchers, they are little used by most people and are not the main focus in work on trying to improve web search engine performance.\\n\\n? Exercise 1.12 [\\u22c6]Write a query using Westlaw syntax which would find any of the words professor, teacher, or lecturer in the same sentence as a form of the verb explain.\\n\\nExercise 1.13 [\\u22c6]\\n\\nTry using the Boolean search features on a couple of major web search engines. For instance, choose a word, such as burglar, and submit the queries (i) burglar, (ii) burglar AND burglar, and (iii) burglar OR burglar. Look at the estimated number of results and top hits. Do they make sense in terms of Boolean logic? Often they haven't for major search engines. Can you make sense of what is going on? What about if you try different words? For example, query for (i) knight, (ii) conquer, and then (iii) knight OR conquer. What bound should the number of results from the first two queries place on the third query? Is this bound observed?\\n\\n1.5 References and further reading\\n\\nThe practical pursuit of computerized information retrieval began in the late 1940s (Cleverdon 1991, Liddy 2005). A great increase in the production of scientific literature, much in the form of less formal technical reports rather than traditional journal articles, coupled with the availability of computers, led to interest in automatic document retrieval. However, in those days, document retrieval was always based on author, title, and keywords; full-text search came much later.\\n\\nThe article of Bush (1945) provided lasting inspiration for the new field:\\n\\n\\\"Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, 'memex' will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.\\\"\\n\\nThe term Information Retrieval was coined by Calvin Mooers in 1948/1950 (Mooers 1950).\\n\\nIn 1958, much newspaper attention was paid to demonstrations at a conference (see Taube and Wooster 1958) of IBM \\\"auto-indexing\\\" machines, based primarily on the work of H. P. Luhn. Commercial interest quickly gravitated towards Boolean retrieval systems, but the early years saw a heady debate over various disparate technologies for retrieval systems. For example Moo-ers (1961) dissented:\\n\\n\\\"It is a common fallacy, underwritten at this date by the investment of several million dollars in a variety of retrieval hardware, that the algebra of George Boole (1847) is the appropriate formalism for retrieval system design. This view is as widely and uncritically accepted as it is wrong.\\\"\\n\\nThe observation of AND vs. OR giving you opposite extremes in a precision/ recall tradeoff, but not the middle ground comes from (Lee and Fox 1988).\\n\\nThe book (Witten et al. 1999) is the standard reference for an in-depth comparison of the space and time efficiency of the inverted index versus other possible data structures; a more succinct and up-to-date presentation appears in Zobel and Moffat (2006). We further discuss several approaches in Chapter 5.\\n\\nFriedl (2006) covers the practical usage of regular expressions for searching.REGULAR EXPRESSIONS The underlying computer science appears in (Hopcroft et al. 2000).\",\n",
      "      \"enriched_text\": {\n",
      "        \"sentiment\": {\n",
      "          \"document\": {\n",
      "            \"score\": 0.121166,\n",
      "            \"label\": \"positive\"\n",
      "          }\n",
      "        },\n",
      "        \"entities\": [\n",
      "          {\n",
      "            \"count\": 18,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"a. Brutus\",\n",
      "            \"relevance\": 0.906635,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 7,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.485522,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Caesar Tempest Cleopatra\",\n",
      "            \"relevance\": 0.574319,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Dedicatee\",\n",
      "                \"MilitaryCommander\",\n",
      "                \"Monarch\",\n",
      "                \"NoblePerson\",\n",
      "                \"OperaCharacter\"\n",
      "              ],\n",
      "              \"name\": \"Augustus\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Augustus\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 4,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Calpurnia\",\n",
      "            \"relevance\": 0.491171,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 7,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Julius Caesar\",\n",
      "            \"relevance\": 0.481292,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [],\n",
      "              \"name\": \"Julius Caesar\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Julius_Caesar\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 8,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.163955,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"docID\",\n",
      "            \"relevance\": 0.462345,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 6,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Antony Julius\",\n",
      "            \"relevance\": 0.390499,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 7,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.274891,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Westlaw\",\n",
      "            \"relevance\": 0.339299,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 3,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Cleopatra\",\n",
      "            \"relevance\": 0.338184,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [],\n",
      "              \"name\": \"Cleopatra\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Cleopatra\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.508063,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Apple\",\n",
      "            \"relevance\": 0.33462,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Brand\",\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProcessorManufacturer\",\n",
      "                \"ProgrammingLanguageDesigner\",\n",
      "                \"ProgrammingLanguageDeveloper\",\n",
      "                \"ProtocolProvider\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"VentureFundedCompany\",\n",
      "                \"VideoGameDeveloper\",\n",
      "                \"VideoGamePublisher\"\n",
      "              ],\n",
      "              \"name\": \"Apple Inc.\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Apple_Inc.\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"AND\",\n",
      "            \"relevance\": 0.327656,\n",
      "            \"type\": \"Organization\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 5,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.216404,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Shakespeare\",\n",
      "            \"relevance\": 0.31669,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.132897,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Philippi\",\n",
      "            \"relevance\": 0.30896,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ],\n",
      "              \"name\": \"Philippi\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Philippi\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.28568,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Calvin Mooers\",\n",
      "            \"relevance\": 0.303923,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"ComputerScientist\"\n",
      "              ],\n",
      "              \"name\": \"Calvin Mooers\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Calvin_Mooers\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.405721,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Microsoft\",\n",
      "            \"relevance\": 0.303409,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProcessorManufacturer\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"VentureFundedCompany\",\n",
      "                \"VideoGameDeveloper\",\n",
      "                \"VideoGamePublisher\",\n",
      "                \"ProgrammingLanguageDesigner\"\n",
      "              ],\n",
      "              \"name\": \"Microsoft\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Microsoft\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.525628,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"software developer\",\n",
      "            \"relevance\": 0.301389,\n",
      "            \"type\": \"JobTitle\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"George Boole\",\n",
      "            \"relevance\": 0.300703,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Philosopher\"\n",
      "              ],\n",
      "              \"name\": \"George Boole\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/George_Boole\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.375071,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"OR\",\n",
      "            \"relevance\": 0.29116,\n",
      "            \"type\": \"Organization\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Hong Kong\",\n",
      "            \"relevance\": 0.289903,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"schizophrenia\",\n",
      "            \"relevance\": 0.289815,\n",
      "            \"type\": \"HealthCondition\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"struction\",\n",
      "            \"relevance\": 0.282125,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.392696,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Lord Polonius\",\n",
      "            \"relevance\": 0.279394,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"IBM\",\n",
      "            \"relevance\": 0.276998,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"SoftwareLicense\",\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProcessorManufacturer\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"CompanyFounder\",\n",
      "                \"ProgrammingLanguageDesigner\",\n",
      "                \"ProgrammingLanguageDeveloper\"\n",
      "              ],\n",
      "              \"name\": \"IBM\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/IBM\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Bush\",\n",
      "            \"relevance\": 0.275362,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.348891,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Witten\",\n",
      "            \"relevance\": 0.275109,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Agrippa\",\n",
      "            \"relevance\": 0.273633,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Cleverdon\",\n",
      "            \"relevance\": 0.272242,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"professor\",\n",
      "            \"relevance\": 0.271824,\n",
      "            \"type\": \"JobTitle\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.578885,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Cormen\",\n",
      "            \"relevance\": 0.267694,\n",
      "            \"type\": \"Person\"\n",
      "          }\n",
      "        ],\n",
      "        \"concepts\": [\n",
      "          {\n",
      "            \"text\": \"Information retrieval\",\n",
      "            \"relevance\": 0.941712,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Information_retrieval\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Web search engine\",\n",
      "            \"relevance\": 0.673612,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Web_search_engine\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Boolean logic\",\n",
      "            \"relevance\": 0.613997,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Boolean_logic\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Document retrieval\",\n",
      "            \"relevance\": 0.430055,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Document_retrieval\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Inverted index\",\n",
      "            \"relevance\": 0.42435,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Inverted_index\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Index\",\n",
      "            \"relevance\": 0.418085,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Index_(search_engine)\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Vector space model\",\n",
      "            \"relevance\": 0.396694,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Vector_space_model\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Web crawler\",\n",
      "            \"relevance\": 0.365526,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Web_crawler\"\n",
      "          }\n",
      "        ],\n",
      "        \"categories\": [\n",
      "          {\n",
      "            \"score\": 0.614732,\n",
      "            \"label\": \"/technology and computing/software/databases\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.330628,\n",
      "            \"label\": \"/health and fitness/exercise\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.287674,\n",
      "            \"label\": \"/technology and computing/internet technology/web search\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"4b22ffb5730ea19dad6c5f41e4d0487b\",\n",
      "      \"result_metadata\": {\n",
      "        \"score\": 1\n",
      "      },\n",
      "      \"extracted_metadata\": {\n",
      "        \"publicationdate\": \"2009-04-01\",\n",
      "        \"sha1\": \"8d2b87685f36390b0167b3f0066c5b717f41028f\",\n",
      "        \"author\": \"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\",\n",
      "        \"filename\": \"02voc.pdf\",\n",
      "        \"file_type\": \"pdf\",\n",
      "        \"title\": \"Introduction to Information Retrieval\"\n",
      "      },\n",
      "      \"html\": \"<?xml version='1.0' encoding='UTF-8' standalone='yes'?><html>\\n<head>\\n    <meta content=\\\"text/html; charset=UTF-8\\\" http-equiv=\\\"Content-Type\\\"/><meta content=\\\"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\\\" name=\\\"author\\\"/><meta content=\\\"2009-04-01\\\" name=\\\"publicationdate\\\"/><meta content=\\\"29\\\" name=\\\"numPages\\\"/><title>Introduction to Information Retrieval</title></head>\\n<body><h1><p>2 </p></h1><h3><p>The term vocabulary and postingslists\\n</p></h3><p>Recall the major steps in inverted index construction:\\n</p><p>1. Collect the documents to be indexed.\\n</p><p>2. Tokenize the text.\\n</p><p>3. Do linguistic preprocessing of tokens.\\n</p><p>4. Index the documents that each term occurs in.\\n</p><p>In this chapter we first briefly mention how the basic unit of a document can be defined and how the character sequence that it comprises is determined (Section 2.1). We then examine in detail some of the substantive linguistic issues of tokenization and linguistic preprocessing, which determine the vocabulary of terms which a system uses (Section 2.2). Tokenization is the process of chopping character streams into tokens, while linguistic preprocessing then deals with building equivalence classes of tokens which are the set of terms that are indexed. Indexing itself is covered in Chapters 1 and 4. Then we return to the implementation of postings lists. In Section 2.3, we examine an extended postings list data structure that supports faster querying, while Section 2.4 covers building postings data structures suitable for handling phrase and proximity queries, of the sort that commonly appear in both extended Boolean models and on the web.\\n</p><p><b>2.1 Document delineation and character sequence decoding\\n</b></p><p><b>2.1.1 Obtaining the character sequence in a document\\n</b></p><p>Digital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more\\ncomplex. The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards. We need to determine the correct encoding. This can be regarded as a machine learning classification problem, as discussed in Chapter 13,1 but is often handled by heuristic methods, user selection, or by using provided document metadata. Once the encoding is determined, we decode the byte sequence to a character sequence. We might save the choice of encoding because it gives some evidence about what language the document is written in.\\n</p><p>The characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files. Again, we must determine the document format, and then an appropriate decoder has to be used. Even for plain text documents, additional decoding may need to be done. In XML documents (Section 10.1, page 197), character entities, such as &amp;amp; , need to be decoded to give the correct character, namely &amp; for &amp;amp; . Finally, the textual part of the document may need to be extracted out of other material that will not be processed. This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files. We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters. Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is. Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk. This problem is usually solved by licensing a software library that handles decoding document formats and character encodings.\\n</p><p>The idea that text is a linear sequence of characters is also called into question by some writing systems, such as Arabic, where text takes on some two dimensional and mixed order characteristics, as shown in Figures 2.1 and 2.2. But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1.\\n</p><p><b>2.1.2 Choosing a document unit\\n</b></p><p>The next phase is to determine what the <i>document unit </i>for indexing is. ThusDOCUMENT UNIT far we have assumed that documents are fixed units for the purposes of indexing. For example, we take each file in a folder as a document. But there\\n</p><p>1. A classifier is a function that takes objects of some sort and assigns them to one of a number of distinct classes (see Chapter 13). Usually classification is done by machine learning methods such as probabilistic models, but it can also be done by hand-written rules.\\n</p><h4><p dir=\\\"rtl\\\">\\ufe7a\\ufe81\\ufe76  \\ufe72\\ufe8f</p></h4><p> </p><h4><p dir=\\\"rtl\\\">\\ufe8f \\ufe72 \\u21d0</p></h4><p> </p><h4><p dir=\\\"rtl\\\">\\ufe8d</p></h4><p> </p><h4><p dir=\\\"rtl\\\">\\ufed9 \\ufe7a \\ufe95</p></h4><p> </p><h4><p> un b \\u0101 t i k /kit\\u0101bun/ 'a book'</p></h4><p> </p><p>\\u25ee <b>Figure 2.1 </b>An example of a vocalized Modern Standard Arabic word. The writing is from right to left and letters undergo complex mutations as they are combined. The representation of short vowels (here, /i/ and /u/) and the final /n/ (nunation) departs from strict linearity by being represented as diacritics above and below letters. Nevertheless, the represented text is still clearly a linear ordering of characters representing sounds. Full vocalization, as here, normally appears only in the Koran and children's books. Day-to-day text is unvocalized (short vowels are not represented but the letter for a\\u0304 would still appear) or partially vocalized, with short vowels inserted in places where the writer perceives ambiguities. These choices add further complexities to indexing.\\n</p><p dir=\\\"rtl\\\">\\ufe8d   \\ufe8d . #\\\"!\\\" ! \\ufe8d    \\ufedd \\ufe8d 132          1962\\ufe8d</p><p> \\u2190 \\u2192 \\u2190 \\u2192 \\u2190 START </p><p>'Algeria achieved its independence in 1962 after 132 years of French occupation.' </p><p>\\u25ee <b>Figure 2.2 </b>The conceptual linear order of characters is not necessarily the order that you see on the page. In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts. With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings.\\n</p><p>are many cases in which you might want to do something different. A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document. Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents. If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document. Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a LATEX document) and split them into separate HTML pages for each slide or subsection, stored as separate files. In these cases, you might want to combine multiple files into a single document.\\n</p><p>More generally, for very long documents, the issue of indexing <i>granularity</i>INDEXING GRANULARITY arises. For a collection of books, it would usually be a bad idea to index an\\nentire book as a document. A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query. Instead, we may well wish to index each chapter or paragraph as a mini-document. Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document. But why stop there? We could treat individual sentences as mini-documents. It becomes clear that there is a precision/recall tradeoff here. If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find.\\n</p><p>The problems with large document units can be alleviated by use of explicit or implicit proximity search (Sections 2.4.2 and 7.2.2), and the tradeoffs in resulting system performance that we are hinting at are discussed in Chapter 8. The issue of index granularity, and in particular a need to simultaneously index documents at multiple levels of granularity, appears prominently in XML retrieval, and is taken up again in Chapter 10. An IR system should be designed to offer choices of granularity. For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns. For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed.\\n</p><p><b>2.2 Determining the vocabulary of terms\\n</b></p><p><b>2.2.1 Tokenization\\n</b></p><p>Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called <i>tokens</i>, perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization:\\n</p><p>Input: Friends, Romans, Countrymen, lend me your ears;\\n</p><p>Output: Friends Romans Countrymen lend me your ears\\n</p><p>These tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A <i>token </i>is an instanceTOKEN\\n</p><p>of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A <i>type </i>is the class of allTYPE\\n</p><p>tokens containing the same character sequence. A <i>term </i>is a (perhaps nor-TERM malized) type that is included in the IR system's dictionary. The set of index terms could be entirely distinct from the tokens, for instance, they could be\\nsemantic identifiers in a taxonomy, but in practice in modern IR systems they are strongly related to the tokens in the document. However, rather than being exactly the tokens that appear in the document, they are usually derived from them by various normalization processes which are discussed in Section 2.2.3.2 For example, if the document to be indexed is <i>to sleep perchance to dream</i>, then there are 5 tokens, but only 4 types (since there are 2 instances of\\n<i>to</i>). However, if <i>to </i>is omitted from the index (as a stop word, see Section 2.2.2 (page 27)), then there will be only 3 terms: <i>sleep</i>, <i>perchance</i>, and <i>dream</i>.\\n</p><p>The major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions?\\n</p><p>Mr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing.\\n</p><p>For <i>O'Neill</i>, which of the following is the desired tokenization?\\n</p><p>neill\\n</p><p>oneill\\n</p><p>o'neill\\n</p><p>o' neill\\n</p><p>o neill ?\\n</p><p>And for <i>aren't</i>, is it:\\n</p><p>aren't arent\\n</p><p>are n't\\n</p><p>aren t ?\\n</p><p>A simple strategy is to just split on all non-alphanumeric characters, but\\n</p><p>while o neill looks okay, aren t looks intuitively bad. For all of them, the choices determine which Boolean queries will match. A query of neill AND capital will match in three cases but not the other two. In how many cases would a query of o'neill AND capital match? If no preprocessing of a query is done, then it would match in only one of the five cases. For either\\n</p><p>2. That is, as defined here, tokens that are not indexed (stop words) are not terms, and if multiple tokens are collapsed together via normalization, they are indexed as one term, under the normalized form. However, we later relax this definition when discussing classification and clustering in Chapters 13-18, where there is no index. In these chapters, we drop the requirement of inclusion in the dictionary. A <i>term </i>means a normalized word.\\n</p><p>Boolean or free text queries, you always want to do the exact same tokeniza-tion of document and query words, generally by processing queries with the same tokenizer. This guarantees that a sequence of characters in a text will always match the same sequence typed in a query.3\\n</p><p>These issues of tokenization are language-specific. It thus requires the language of the document to be known. <i>Language identification </i>based on clas-LANGUAGE\\n</p><p>IDENTIFICATION sifiers that use short character subsequences as features is highly effective; most languages have distinctive signature patterns (see page 46 for references).\\n</p><p>For most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms, such as the programming languages C++ and C#, aircraft names like B-52, or a T.V. show name such as M*A*S*H - which is sufficiently integrated into popular culture that you find usages such as <i>M*A*S*H-style hospitals</i>. Computer technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token, including email addresses (jblack@mail.yahoo.com), web URLs (http://stuff.big.com/new/specials.html),numeric IP addresses (142.32.48.231), package tracking numbers (1Z9999W99845399981), and more. One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary. However, this comes at a large cost in restricting what people can search for. For instance, people might want to search in a bug database for the line number where an error occurs. Items such as the date of an email, which have a clear semantic type, are often indexed separately as document metadata (see Section 6.1, page 110).\\n</p><p>In English, <i>hyphenation </i>is used for various purposes ranging from split-HYPHENS ting up vowels in words (<i>co-education</i>) to joining nouns as names (<i>Hewlett-Packard</i>) to a copyediting device to show word grouping (<i>the hold-him-back-and-drag-him-away maneuver</i>). It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just <i>coeducation</i>), the last should be separated into words, and that the middle case is unclear. Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms.\\n</p><p>Conceptually, splitting on white space can also split what should be regarded as a single token. This occurs most commonly with names (<i>San Fran-cisco, Los Angeles</i>) but also with borrowed foreign phrases (<i>au fait</i>) and com-\\n</p><p>3. For the free text case, this is straightforward. The Boolean case is more complex: this tok-enization may produce multiple terms from one query word. This can be handled by combining the terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system to handle the opposite case where the user entered as two terms something that was tokenized together in the document processing.\\n</p><p>pounds that are sometimes written as a single word and sometimes space separated (such as <i>white space </i>vs. <i>whitespace</i>). Other cases with internal spaces that we might wish to regard as a single token include phone numbers ((800) 234-\\n</p><p>2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing <i>New York University</i>. The problems of hyphens and non-separating whitespace can even interact. Advertisements for air fares frequently contain items like <i>San Francisco-Los Angeles</i>, where simply doing whitespace splitting would give unfortunate results. In such cases, issues of tokenization interact with handling phrase queries (which we discuss in Section 2.4 (page 39)), particularly if we would like queries for all of <i>lowercase</i>,\\n<i>lower-case </i>and <i>lower case </i>to return the same results. The last two can be handled by splitting on hyphens and using a phrase index. Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way. One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (Example 1.1), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR \\\"over eager\\\" OR overeager. However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization.\\n</p><p>Each new language presents some new issues. For instance, French has a variant use of the apostrophe for a reduced definite article 'the' before a word beginning with a vowel (e.g., <i>l'ensemble</i>) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g., <i>donne-moi </i>'give me'). Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both <i>l'ensemble </i>and <i>un ensemble </i>to be indexed under <i>ensemble</i>. Other languages make the problem harder in new ways. German writes\\n<i>compound nouns </i>without spaces (e.g., <i>Computerlinguistik </i>'computational lin-COMPOUNDS\\n</p><p>guistics'; <i>Lebensversicherungsgesellschaftsangestellter </i>'life insurance company employee'). Retrieval systems for German greatly benefit from the use of a\\n<i>compound-splitter </i>module, which is usually implemented by seeing if a wordCOMPOUND-SPLITTER\\n</p><p>can be subdivided into multiple words that appear in a vocabulary. This phenomenon reaches its limit case with major East Asian Languages (e.g., Chi-nese, Japanese, Korean, and Thai), where text is written without any spaces between words. An example is shown in Figure 2.3. One approach here is to perform <i>word segmentation </i>as prior linguistic processing. Methods of wordWORD SEGMENTATION\\n</p><p>segmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine learning sequence models, such as hidden Markov models or conditional random fields, trained over hand-segmented words (see the references\\n</p><p><i>26 </i><i>2 The term vocabulary and postings lists</i>\\ufffd                                             \\ufffd                    ! \\\" # $ % &amp; '    '   ( ) *   \\ufffd       + , # -   . /  \\n</p><p>\\u25ee <b>Figure 2.3 </b>The standard unsegmented form of Chinese text using the simplified characters of mainland China. There is no whitespace between words, not even between sentences - the apparent space after the Chinese period (\\u25e6) is just a typographical illusion caused by placing the character on the left side of its square box. The first sentence is just words in Chinese characters with no spaces between them. The second and third sentences include Arabic numerals and punctuation breaking up the Chinese characters.\\n</p><p>\\u25ee <b>Figure 2.4 </b>Ambiguities in Chinese word segmentation. The two characters can be treated as one word meaning 'monk' or as a sequence of two words meaning 'and' and 'still'.\\n</p><p>a an and are as at be by for from has he in is it its of on that the to was were will with \\u25ee <b>Figure 2.5 </b>A stop list of 25 semantically non-selective words which are common in Reuters-RCV1.\\n</p><p>in Section 2.5). Since there are multiple possible segmentations of character sequences (see Figure 2.4), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization. The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character <i>k</i>-grams), regardless of whether particular sequences cross word boundaries or not. Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway. Even in English, some cases of where to put word boundaries are just orthographic conventions - think of <i>notwithstanding </i>vs. <i>not to mention </i>or <i>into </i>vs. <i>on to </i>- but people are educated to write the words with consistent use of spaces.\\n</p><p><b>2.2.2 Dropping common terms: stop words\\n</b></p><p>Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called <i>stop words</i>. The generalSTOP WORDS\\n</p><p>strategy for determining a stop list is to sort the terms by <i>collection frequency</i>COLLECTION FREQUENCY (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a <i>stop list</i>, the members of which are then discarded during indexing. AnSTOP LIST\\n</p><p>example of a stop list is shown in Figure 2.5. Using a stop list significantly reduces the number of postings that a system has to store; we will present some statistics on this in Chapter 5 (see Table 5.1, page 87). And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don't seem very useful. However, this is not true for phrase searches. The phrase query \\\"President of the United States\\\", which contains two stop words, is more precise than President AND \\\"United States\\\". The meaning of flights to London is likely to be lost if the word to is stopped out. A search for Vannevar Bush's article <i>As we may think </i>will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think. Some special query types are disproportionately affected. Some song titles and well known pieces of verse consist entirely of words that are commonly on stop lists (<i>To be or not to be</i>, <i>Let It Be</i>, <i>I don't want to be</i>, . . . ).\\n</p><p>The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists. Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways. We will show in Section 5.3 (page 95) how good compression techniques greatly reduce the cost of storing the postings for common words. Section 6.2.1 (page 117) then discusses how standard term weighting leads to very common words having little impact on document rankings. Finally, Section 7.1.5 (page 140) shows how an IR system with impact-sorted indexes can terminate scanning a postings list early when weights get small, and hence common words do not cause a large additional processing cost for the average query, even though postings lists for stop words are very long. So for most modern IR systems, the additional cost of including stop words is not that big - neither in terms of index size nor in terms of query processing time.\\n</p><p><b>Query term Terms in documents that should be matched\\n</b></p><p>Windows Windows windows Windows, windows, window window window, windows\\n</p><p>\\u25ee <b>Figure 2.6 </b>An example of how asymmetric expansion of query terms can usefully model users' expectations.\\n</p><p><b>2.2.3 Normalization (equivalence classing of terms)\\n</b></p><p>Having broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document. However, there are many cases when two character sequences are not quite the same but you would like a match to occur. For instance, if you search for <i>USA</i>, you might hope to also match documents containing <i>U.S.A</i>.\\n<i>Token normalization </i>is the process of canonicalizing tokens so that matchesTOKEN\\n</p><p>NORMALIZATION occur despite superficial differences in the character sequences of the tokens.4 The most standard way to normalize is to implicitly create <i>equivalence</i>EQUIVALENCE CLASSES\\n</p><p><i>classes</i>, which are normally named after one member of the set. For instance, if the tokens <i>anti-discriminatory </i>and <i>antidiscriminatory </i>are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either.\\n</p><p>The advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes. It is only easy to write rules of this sort that remove characters. Since the equivalence classes are implicit, it is not obvious when you might want to add characters. For instance, it would be hard to know to turn <i>antidiscriminatory </i>into <i>anti-discriminatory</i>.\\n</p><p>An alternative to creating equivalence classes is to maintain relations between unnormalized tokens. This method can be extended to hand-constructed lists of synonyms such as <i>car </i>and <i>automobile</i>, a topic we discuss further in Chapter 9. These term relationships can be achieved in two ways. The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term. A query term is then effectively a disjunction of several postings lists. The alternative is to perform the expansion during index construction. When the document contains automobile, we index it under car as well (and, usually, also vice-versa). Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge. The first\\n</p><p>4. It is also often referred to as <i>term normalization</i>, but we prefer to reserve the name <i>term </i>for the output of the normalization process.\\n</p><p>method adds a query expansion dictionary and requires more processing at query time, while the second method requires more space for storing postings. Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing.\\n</p><p>These approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical. This means there can be an asymmetry in expansion. An example of how such an asymmetry can be exploited is shown in Figure 2.6: if the user enters windows, we wish to allow matches with the capitalized <i>Windows </i>operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase <i>windows</i>.\\n</p><p>The best amount of equivalence classing or query expansion to do is a fairly open question. Doing some definitely seems a good idea. But doing a lot can easily have unexpected consequences of broadening queries in unintended ways. For instance, equivalence-classing <i>U.S.A. </i>and <i>USA </i>to the latter by deleting periods from tokens might at first seem very reasonable, given the prevalent pattern of optional use of periods in acronyms. However, if I put in as my query term <i>C.A.T.</i>, I might be rather upset if it matches every appearance of the word <i>cat </i>in documents.5\\n</p><p>Below we present some of the forms of normalization that are commonly employed and how they are implemented. In many cases they seem helpful, but they can also do harm. In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance.\\n</p><p><b>Accents and diacritics. </b>Diacritics on characters in English have a fairly marginal status, and we might well want <i>clich\\u00e9 </i>and <i>cliche </i>to match, or <i>naive\\n</i>and <i>na\\u00efve</i>. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in Spanish, <i>pe\\u00f1a </i>is 'a cliff', while <i>pena </i>is 'sorrow'. Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics.\\n</p><p>5. At the time we wrote this chapter (Aug. 2005), this was actually the case on Google: the top result for the query <i>C.A.T. </i>was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/.\\n</p><p><b>Capitalization/case-folding. </b>A common strategy is to do <i>case-folding </i>by re-CASE-FOLDING ducing all letters to lower case. Often this is a good idea: it will allow instances of <i>Automobile </i>at the beginning of a sentence to match with a query of\\n<i>automobile</i>. It will also help on a web search engine when most of your users type in <i>ferrari </i>when they are interested in a <i>Ferrari </i>car. On the other hand, such case folding can equate words that might better be kept apart. Many proper nouns are derived from common nouns and so are distinguished only by case, including companies (<i>General Motors</i>, <i>The Associated Press</i>), government organizations (<i>the Fed </i>vs. <i>fed</i>) and person names (<i>Bush</i>, <i>Black</i>). We already mentioned an example of unintended query expansion with acronyms, which involved not only acronym normalization (<i>C.A.T. </i>\\u2192 <i>CAT</i>) but also case-folding (<i>CAT</i>\\u2192 <i>cat</i>).\\n</p><p>For English, an alternative to making every token lowercase is to just make some tokens lowercase. The simplest heuristic is to convert to lowercase words at the beginning of a sentence and all words occurring in a title that is all uppercase or in which most or all words are capitalized. These words are usually ordinary words that have been capitalized. Mid-sentence capitalized words are left as capitalized (which is usually correct). This will mostly avoid case-folding in cases where distinctions should be kept apart. The same task can be done more accurately by a machine learning sequence model which uses more features to make the decision of when to case-fold. This is known as <i>truecasing</i>. However, trying to get capitalization right in this way probablyTRUECASING\\n</p><p>doesn't help if your users usually use lowercase regardless of the correct case of words. Thus, lowercasing everything often remains the most practical solution.\\n</p><p><b>Other issues in English. </b>Other possible normalizations are quite idiosyncratic and particular to English. For instance, you might wish to equate\\n<i>ne'er </i>and <i>never </i>or the British spelling <i>colour </i>and the American spelling <i>color</i>. Dates, times and similar items come in multiple formats, presenting additional challenges. You might wish to collapse together <i>3/12/91 </i>and <i>Mar. 12, 1991</i>. However, correct processing here is complicated by the fact that in the U.S., <i>3/12/91 </i>is <i>Mar. 12, 1991</i>, whereas in Europe it is <i>3 Dec 1991</i>.\\n</p><p><b>Other languages. </b>English has maintained a dominant position on the WWW; approximately 60% of web pages are in English (Gerrand 2007). But that still leaves 40% of the web, and the non-English portion might be expected to grow over time, since less than one third of Internet users and less than 10% of the world's population primarily speak English. And there are signs of change: Sifry (2007) reports that only about one third of blog posts are in English.\\n</p><p>Other languages again present distinctive issues in equivalence classing.\\n</p><p><i>2.2 Determining the vocabulary of terms </i><i>31</i>\\ufffd                                                           ! \\\" ! # $  %    &amp; ' ( ) * + , - . / 0 )   1  2 3 4 5 6 7 &amp; + 8 9 : ; : &lt;   = > ? @ A B C   - D E6 8 9 : ; : &lt; ) F G * H I * :  J ) K + L M N ? O P  Q RS   T   U V V W X Y &amp; Z [ N ? )   + \\\\ ] ; ^ _ +   ` 4 a + b; c   d e * f V g h V - ? i N j k l m n   : A o       p N 5 +q V r s t u &amp; v w x ) Q y z { h | &amp; }     ~ \\ufffd M ? @ A\\n</p><p>\\u25ee <b>Figure 2.7 </b>Japanese makes use of multiple intermingled writing systems and, like Chinese, does not segment words. The text is mainly Chinese characters with the hiragana syllabary for inflectional endings and function words. The part in latin letters is actually a Japanese expression, but has been taken up as the name of an environmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His name is written using the katakana syllabary in the middle of the first line. The first four characters of the final line express a monetary amount that we would want to match with \\u00a5500,000 (500,000 Japanese yen).\\n</p><p>The French word for <i>the </i>has distinctive forms based not only on the gender (masculine or feminine) and number of the following noun, but also depending on whether the following word begins with a vowel: <i>le</i>, <i>la</i>, <i>l'</i>, <i>les</i>. We may well wish to equivalence class these various forms of <i>the</i>. German has a convention whereby vowels with an umlaut can be rendered instead as a two vowel digraph. We would want to treat <i>Sch\\u00fctze </i>and <i>Schuetze </i>as equivalent.\\n</p><p>Japanese is a well-known difficult writing system, as illustrated in Figure 2.7. Modern Japanese is standardly an intermingling of multiple alphabets, principally Chinese characters, two syllabaries (hiragana and katakana) and western characters (Latin letters, Arabic numerals, and various symbols). While there are strong conventions and standardization through the education system over the choice of writing system, in many cases the same word can be written with multiple writing systems. For example, a word may be written in katakana for emphasis (somewhat like italics). Or a word may sometimes be written in hiragana and sometimes in Chinese characters. Successful retrieval thus requires complex equivalence classing across the writing systems. In particular, an end user might commonly present a query entirely in hiragana, because it is easier to type, just as Western end users commonly use all lowercase.\\n</p><p>Document collections being indexed can include documents from many different languages. Or a single document can easily contain text from multiple languages. For instance, a French email might quote clauses from a contract document written in English. Most commonly, the language is detected and language-particular tokenization and normalization rules are applied at a predetermined granularity, such as whole documents or individual paragraphs, but this still will not correctly deal with cases where language changes occur for brief quotations. When document collections contain mul-\\ntiple languages, a single index may have to contain terms of several languages. One option is to run a language identification classifier on documents and then to tag terms in the vocabulary for their language. Or this tagging can simply be omitted, since it is relatively rare for the exact same character sequence to be a word in different languages.\\n</p><p>When dealing with foreign or complex words, particularly foreign names, the spelling may be unclear or there may be variant transliteration standards giving different spellings (for example, <i>Chebyshev </i>and <i>Tchebycheff </i>or <i>Beijing\\n</i>and <i>Peking</i>). One way of dealing with this is to use heuristics to equivalence class or expand terms with phonetic equivalents. The traditional and best known such algorithm is the Soundex algorithm, which we cover in Section 3.4 (page 63).\\n</p><p><b>2.2.4 Stemming and lemmatization\\n</b></p><p>For grammatical reasons, documents are going to use different forms of a word, such as <i>organize</i>, <i>organizes</i>, and <i>organizing</i>. Additionally, there are families of derivationally related words with similar meanings, such as <i>democracy</i>,\\n<i>democratic</i>, and <i>democratization</i>. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\\n</p><p>The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\\n</p><p>am, are, is\\u21d2 be car, cars, car's, cars'\\u21d2 car\\n</p><p>The result of this mapping of text will be something like:\\n</p><p>the boy's cars are different colors\\u21d2 the boy car be differ color\\n</p><p>However, the two words differ in their flavor. <i>Stemming </i>usually refers toSTEMMING a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. <i>Lemmatization </i>usually refers to doing thingsLEMMATIZATION\\n</p><p>properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the <i>lemma</i>. If confrontedLEMMA\\n</p><p>with the token <i>saw</i>, stemming might return just <i>s</i>, whereas lemmatization would attempt to return either <i>see </i>or <i>saw </i>depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatiza-tion commonly only collapses the different inflectional forms of a lemma.\\n</p><p>Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\\n</p><p>The most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is <i>Porter's algorithm</i>PORTER STEMMER\\n</p><p>(Porter 1980). The entire algorithm is too long and intricate to present here, but we will indicate its general nature. Porter's algorithm consists of 5 phases of word reductions, applied sequentially. Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix. In the first phase, this convention is used with the following rule group:\\n</p><p>(2.1) <b>Rule Example\\n</b></p><p>SSES \\u2192 SS caresses \\u2192 caress\\n</p><p>IES \\u2192 I ponies \\u2192 poni\\n</p><p>SS \\u2192 SS caress \\u2192 caress\\n</p><p>S \\u2192 cats \\u2192 cat\\n</p><p>Many of the later rules use a concept of the <i>measure </i>of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word. For example, the rule:\\n</p><p>(<i>m </i>&gt; 1) EMENT \\u2192\\n</p><p>would map <i>replacement </i>to <i>replac</i>, but not <i>cement </i>to <i>c</i>. The official site for the Porter Stemmer is:\\n</p><p>http://www.tartarus.org/\\u02dc martin/PorterStemmer/\\n</p><p>Other stemmers exist, including the older, one-pass Lovins stemmer (Lovins 1968), and newer entrants like the Paice/Husk stemmer (Paice 1990); see: http://www.cs.waikato.ac.nz/\\u02dc eibe/stemmers/ http://www.comp.lancs.ac.uk/computing/research/stemming/\\n</p><p>Figure 2.8 presents an informal comparison of the different behaviors of these stemmers. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words. Particular domains may also require special stemming rules. However, the exact stemmed form does not matter, only the equivalence classes it forms.\\n</p><p>Rather than using a stemmer, you can use a <i>lemmatizer</i>, a tool from Nat-LEMMATIZER ural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval. It is hard to say more,\\n</p><p><b><i>Sample text: </i></b>Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation\\n</p><p><b><i>Lovins stemmer: </i></b>such an analys can reve featur that ar not eas vis from th vari in th individu gen and can lead to a pictur of expres that is mor biolog transpar and acces to interpres\\n</p><p><b><i>Porter stemmer: </i></b>such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret\\n</p><p><b><i>Paice stemmer: </i></b>such an analys can rev feat that are not easy vis from the vary in the individ gen and can lead to a pict of express that is mor biolog transp and access to interpret\\n</p><p>\\u25ee <b>Figure 2.8 </b>A comparison of three stemming algorithms on a sample text.\\n</p><p>because either form of normalization tends not to improve English information retrieval performance in aggregate - at least not by very much. While it helps a lot for some queries, it equally hurts performance a lot for others. Stemming increases recall while harming precision. As an example of what can go wrong, note that the Porter stemmer stems all of the following words:\\n</p><p><i>operate operating operates operation operative operatives operational\\n</i></p><p>to oper. However, since <i>operate </i>in its various forms is a common verb, we would expect to lose considerable precision on queries such as the following with Porter stemming:\\n</p><p>operational AND research operating AND system operative AND dentistry\\n</p><p>For a case like this, moving to using a lemmatizer would not completely fix the problem because particular inflectional forms are used in particular collocations: a sentence with the words <i>operate </i>and <i>system </i>is not a good match for the query operating AND system. Getting better value from term normalization depends more on pragmatic issues of word use than on formal issues of linguistic morphology.\\n</p><p>The situation is different for languages with much more morphology (such as Spanish, German, and Finnish). Results in the European CLEF evaluations have repeatedly shown quite large gains from the use of stemmers (and compound splitting for languages like German); see the references in Section 2.5.\\n</p><p><i>? </i><b>Exercise 2.1 </b>[\\u22c6]Are the following statements true or false?\\n</p><p>a. In a Boolean retrieval system, stemming never lowers precision. b. In a Boolean retrieval system, stemming never lowers recall. c. Stemming increases the size of the vocabulary.\\n</p><p>d. Stemming should be invoked at indexing time but not while processing a query.\\n</p><p><b>Exercise 2.2 </b>[\\u22c6] Suggest what normalized form should be used for these words (including the word itself as a possibility):\\n</p><p>a. 'Cos b. Shi'ite c. cont'd d. Hawai'i e. O'Rourke\\n</p><p><b>Exercise 2.3 </b>[\\u22c6] The following pairs of words are stemmed to the same form by the Porter stemmer. Which pairs would you argue shouldn't be conflated. Give your reasoning.\\n</p><p>a. abandon/abandonment b. absorbency/absorbent c. marketing/markets d. university/universe e. volume/volumes\\n</p><p><b>Exercise 2.4 </b>[\\u22c6] For the Porter stemmer rule group shown in (2.1):\\n</p><p>a. What is the purpose of including an identity rule such as SS\\u2192 SS?\\n</p><p>b. Applying just this rule group, what will the following words be stemmed to?\\n<i>circus canaries boss\\n</i></p><p>c. What rule should be added to correctly stem <i>pony</i>?\\n</p><p>d. The stemming for <i>ponies </i>and <i>pony </i>might seem strange. Does it have a deleterious effect on retrieval? Why or why not?\\n</p><p>\\u25ee <b>Figure 2.9 </b>Postings lists with skip pointers. The postings intersection can use a skip pointer when the end point is still less than the item on the other list.\\n</p><p><b>2.3 Faster postings list intersection via skip pointers\\n</b></p><p>In the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3 (page 10): we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are <i>m </i>and <i>n</i>, the intersection takes <i>O</i>(<i>m </i>+ <i>n</i>) operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn't changing too fast.\\n</p><p>One way to do this is to use a <i>skip list </i>by augmenting postings lists withSKIP LIST skip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers.\\n</p><p>Consider first efficient merging, with Figure 2.9 as an example. Suppose we've stepped through the lists in the figure until we have matched 8 on each list and moved it to the results list. We advance both pointers, giving us 16 on the upper list and 41 on the lower list. The smallest item is then the element 16 on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to 28 . We thus avoid stepping to 19 and 23 on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown\\n</p><p>INTERSECTWITHSKIPS(<i>p</i>1, <i>p</i>2)\\n</p><p>1 <i>answer </i>\\u2190 \\u3008 \\u3009\\n</p><p>2 <b>while </b><i>p</i>1 6= NIL and <i>p</i>2 6= NIL\\n</p><p>3 <b>do if </b><i>docID</i>(<i>p</i>1) = <i>docID</i>(<i>p</i>2)\\n</p><p>4 <b>then </b>ADD(<i>answer</i>, <i>docID</i>(<i>p</i>1))\\n</p><p>5 <i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>6 <i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2)\\n</p><p>7 <b>else if </b><i>docID</i>(<i>p</i>1) &lt; <i>docID</i>(<i>p</i>2)\\n</p><p>8 <b>then if </b><i>hasSkip</i>(<i>p</i>1) and (<i>docID</i>(<i>skip</i>(<i>p</i>1)) \\u2264 <i>docID</i>(<i>p</i>2))\\n</p><p>9 <b>then while </b><i>hasSkip</i>(<i>p</i>1) and (<i>docID</i>(<i>skip</i>(<i>p</i>1)) \\u2264 <i>docID</i>(<i>p</i>2))\\n</p><p>10 <b>do </b><i>p</i>1 \\u2190 <i>skip</i>(<i>p</i>1)\\n</p><p>11 <b>else </b><i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>12 <b>else if </b><i>hasSkip</i>(<i>p</i>2) and (<i>docID</i>(<i>skip</i>(<i>p</i>2)) \\u2264 <i>docID</i>(<i>p</i>1))\\n</p><p>13 <b>then while </b><i>hasSkip</i>(<i>p</i>2) and (<i>docID</i>(<i>skip</i>(<i>p</i>2)) \\u2264 <i>docID</i>(<i>p</i>1))\\n</p><p>14 <b>do </b><i>p</i>2 \\u2190 <i>skip</i>(<i>p</i>2)\\n</p><p>15 <b>else </b><i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2) 16 <b>return </b><i>answer\\n</i></p><p>\\u25ee <b>Figure 2.10 </b>Postings lists intersection with skip pointers.\\n</p><p>in Figure 2.10. Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call <i>hasSkip</i>(<i>p</i>) will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries.\\n</p><p>Where do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length <i>P</i>, use\\n</p><p>\\u221a\\n</p><p><i>P </i>evenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms.\\n</p><p>Building effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective.\\n</p><p>Choosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you're running a search engine with everything in mem-\\nory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 (page 68) and the impact of index size on system speed in Chapter 5.\\n</p><p><i>? </i><b>Exercise 2.5 </b>[\\u22c6]Why are skip pointers not useful for queries of the form <i>x </i>OR <i>y</i>?\\n</p><p><b>Exercise 2.6 </b>[\\u22c6]\\n</p><p>We have a two-word query. For one term the postings list consists of the following 16 entries:\\n</p><p>[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\\n</p><p>and for the other it is the one entry postings list:\\n</p><p>[47].\\n</p><p>Work out how many comparisons would be done to intersect the two postings lists with the following two strategies. Briefly justify your answers:\\n</p><p>a. Using standard postings lists\\n</p><p>b. Using postings lists stored with skip pointers, with a skip length of\\n</p><p>\\u221a\\n</p><p><i>P</i>, as sug-\\n</p><p>gested in Section 2.3.\\n</p><p><b>Exercise 2.7 </b>[\\u22c6]\\n</p><p>Consider a postings intersection between this postings list, with skip pointers:\\n</p><p>3 5 9 15 24 39 60 68 75 81 84 89 92 96 97 100 115\\n</p><p>and the following intermediate result postings list (which hence has no skip pointers):\\n</p><p>3 5 89 95 97 99 100 101\\n</p><p>Trace through the postings intersection algorithm in Figure 2.10 (page 37).\\n</p><p>a. How often is a skip pointer followed (i.e., <i>p</i>1 is advanced to <i>skip</i>(<i>p</i>1))?\\n</p><p>b. How many postings comparisons will be made by this algorithm while intersecting the two lists?\\n</p><p>c. How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers?\\n</p><p><b>2.4 Positional postings and phrase queries\\n</b></p><p>Many complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like <i>The inventor Stanford Ovshinsky never went to university. </i>is not a match. Most recent search engines support a double quotes syntax (\\\"stanford university\\\") for <i>phrase queries</i>, which has proven to be veryPHRASE QUERIES\\n</p><p>easily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 (page 144) in the context of ranked retrieval.\\n</p><p><b>2.4.1 Biword indexes\\n</b></p><p>One approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase. For example, the text <i>Friends, Romans, Countrymen </i>would generate the <i>biwords</i>:BIWORD INDEX\\n</p><p>friends romans romans countrymen\\n</p><p>In this model, we treat each of these biwords as a vocabulary term. Being able to process two-word phrase queries is immediate. Longer phrases can be processed by breaking them down. The query stanford university palo alto can be broken into the Boolean query on biwords:\\n</p><p>\\\"stanford university\\\" AND \\\"university palo\\\" AND \\\"palo alto\\\"\\n</p><p>This query could be expected to work fairly well in practice, but there can and will be occasional false positives. Without examining the documents, we cannot verify that the documents matching the above Boolean query do actually contain the original 4 word phrase.\\n</p><p>Among possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for. But related nouns can often be divided from each other by various function words, in phrases such as <i>the abolition of slavery </i>or <i>renegotiation of the constitution</i>. These needs can be incorporated into the biword indexing model in the following\\nway. First, we tokenize the text and perform part-of-speech-tagging.6 We can then group terms into nouns, including proper nouns, (N) and function words, including articles and prepositions, (X), among other classes. Now deem any string of terms of the form NX*N to be an extended biword. Each such extended biword is made a term in the vocabulary. For example:\\n</p><p>renegotiation of the constitution\\n</p><p>N X X N\\n</p><p>To process a query using such an extended biword index, we need to also parse it into N's and X's, and then segment the query into extended biwords, which can be looked up in the index.\\n</p><p>This algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries. Using the above algorithm, the query\\n</p><p>cost overruns on a power plant\\n</p><p>is parsed into\\n</p><p>\\\"cost overruns\\\" AND \\\"overruns power\\\" AND \\\"power plant\\\"\\n</p><p>whereas it might seem a better query to omit the middle biword. Better results can be obtained by using more precise part-of-speech patterns that define which extended biwords should be indexed.\\n</p><p>The concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a <i>phrase index</i>. Indeed, searches for a single term arePHRASE INDEX\\n</p><p>not naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms. While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed. But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size. Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary. However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme.\\n</p><p>6. Part of speech taggers classify words as nouns, verbs, etc. - or, in practice, often as finer-grained classes like \\\"plural proper noun\\\". Many fairly accurate (c. 96% per-tag accuracy) part-of-speech taggers now exist, usually trained by machine learning methods on hand-tagged text. See, for instance, Manning and Sch\\u00fctze (1999, ch. 10).\\n</p><p>to, 993427:\\n</p><p>\\u3008 1, 6: \\u30087, 18, 33, 72, 86, 231\\u3009; 2, 5: \\u30081, 17, 74, 222, 255\\u3009; 4, 5: \\u30088, 16, 190, 429, 433\\u3009; 5, 2: \\u3008363, 367\\u3009;\\n</p><p>7, 3: \\u300813, 23, 191\\u3009; . . . \\u3009\\n</p><p>be, 178239:\\n</p><p>\\u3008 1, 2: \\u300817, 25\\u3009;\\n</p><p>4, 5: \\u300817, 191, 291, 430, 434\\u3009; 5, 3: \\u300814, 19, 101\\u3009; . . . \\u3009\\n</p><p>\\u25ee <b>Figure 2.11 </b>Positional index example. The word to has a document frequency 993,477, and occurs 6 times in document 1 at positions 7, 18, 33, etc.\\n</p><p><b>2.4.2 Positional indexes\\n</b></p><p>For the reasons given, a biword index is not the standard solution. Rather, a <i>positional index </i>is most commonly employed. Here, for each term in thePOSITIONAL INDEX\\n</p><p>vocabulary, we store postings of the form docID: \\u3008position1, position2, . . . \\u3009, as shown in Figure 2.11, where each position is a token index in the document. Each posting will also usually record the term frequency, for reasons discussed in Chapter 6.\\n</p><p>To process a phrase query, you still need to access the inverted index entries for each distinct term. As before, you would start with the least frequent term and then work to further restrict the list of possible candidates. In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated. This requires working out offsets between the words.\\n</p><p>\\u270e <b>Example 2.1: Satisfying phrase queries. </b>Suppose the postings lists for to andbe are as in Figure 2.11, and the query is \\\"to be or not to be\\\". The postings lists to access are: to, be, or, not. We will examine intersecting the postings lists for to and be. We first look for documents that contain both terms. Then, we look for places in the lists where there is an occurrence of <i>be </i>with a token index one higher than a position of <i>to</i>, and then we look for another occurrence of each word with token index 4 higher than the first occurrence. In the above lists, the pattern of occurrences that is a possible match is:\\n</p><p>to: \\u3008. . . ; 4:\\u3008. . . ,429,433\\u3009; . . . \\u3009 be: \\u3008. . . ; 4:\\u3008. . . ,430,434\\u3009; . . . \\u3009\\n</p><p>POSITIONALINTERSECT(<i>p</i>1, <i>p</i>2, <i>k</i>)\\n</p><p>1 <i>answer </i>\\u2190 \\u3008 \\u3009\\n</p><p>2 <b>while </b><i>p</i>1 6= NIL and <i>p</i>2 6= NIL\\n</p><p>3 <b>do if </b><i>docID</i>(<i>p</i>1) = <i>docID</i>(<i>p</i>2)\\n</p><p>4 <b>then </b><i>l </i>\\u2190 \\u3008 \\u3009\\n</p><p>5 <i>pp</i>1 \\u2190 <i>positions</i>(<i>p</i>1)\\n</p><p>6 <i>pp</i>2 \\u2190 <i>positions</i>(<i>p</i>2)\\n</p><p>7 <b>while </b><i>pp</i>1 6= NIL\\n</p><p>8 <b>do while </b><i>pp</i>2 6= NIL\\n</p><p>9 <b>do if </b>|<i>pos</i>(<i>pp</i>1)\\u2212 <i>pos</i>(<i>pp</i>2)| \\u2264 <i>k\\n</i></p><p>10 <b>then </b>ADD(<i>l</i>, <i>pos</i>(<i>pp</i>2))\\n</p><p>11 <b>else if </b><i>pos</i>(<i>pp</i>2) > <i>pos</i>(<i>pp</i>1)\\n</p><p>12 <b>then break\\n</b></p><p>13 <i>pp</i>2 \\u2190 <i>next</i>(<i>pp</i>2)\\n</p><p>14 <b>while </b><i>l </i>6= \\u3008 \\u3009 and |<i>l</i>[0]\\u2212 <i>pos</i>(<i>pp</i>1)| > <i>k\\n</i></p><p>15 <b>do </b>DELETE(<i>l</i>[0])\\n</p><p>16 <b>for each </b><i>ps </i>\\u2208 <i>l\\n</i></p><p>17 <b>do </b>ADD(<i>answer</i>, \\u3008<i>docID</i>(<i>p</i>1), <i>pos</i>(<i>pp</i>1), <i>ps</i>\\u3009)\\n</p><p>18 <i>pp</i>1 \\u2190 <i>next</i>(<i>pp</i>1)\\n</p><p>19 <i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>20 <i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2)\\n</p><p>21 <b>else if </b><i>docID</i>(<i>p</i>1) &lt; <i>docID</i>(<i>p</i>2)\\n</p><p>22 <b>then </b><i>p</i>1 \\u2190 <i>next</i>(<i>p</i>1)\\n</p><p>23 <b>else </b><i>p</i>2 \\u2190 <i>next</i>(<i>p</i>2) 24 <b>return </b><i>answer\\n</i></p><p>\\u25ee <b>Figure 2.12 </b>An algorithm for proximity intersection of postings lists <i>p</i>1 and <i>p</i>2. The algorithm finds places where the two terms appear within <i>k </i>words of each other\\n</p><p>and returns a list of triples giving docID and the term position in <i>p</i>1 and <i>p</i>2.\\n</p><p>The same general method is applied for within <i>k </i>word proximity searches,\\n</p><p>of the sort we saw in Example 1.1 (page 15):\\n</p><p>employment /3 place\\n</p><p>Here, /<i>k </i>means \\\"within <i>k </i>words of (on either side)\\\". Clearly, positional in-\\n</p><p>dexes can be used for such queries; biword indexes cannot. We show in Figure 2.12 an algorithm for satisfying within <i>k </i>word proximity searches; it\\n</p><p>is further discussed in Exercise 2.12.\\n</p><p><b>Positional index size. </b>Adopting a positional index expands required post-\\n</p><p>ings storage significantly, even if we compress position values/offsets as we\\nwill discuss in Section 5.3 (page 95). Indeed, moving to a positional index also changes the asymptotic complexity of a postings intersection operation, because the number of items to check is now bounded not by the number of documents but by the total number of tokens in the document collection <i>T</i>. That is, the complexity of a Boolean query is \\u0398(<i>T</i>) rather than \\u0398(<i>N</i>). However, most applications have little choice but to accept this, since most users now expect to have the functionality of phrase and proximity searches.\\n</p><p>Let's examine the space implications of having a positional index. A posting now needs an entry for each occurrence of a term. The index size thus depends on the average document size. The average web page has less than 1000 terms, but documents like SEC stock filings, books, and even some epic poems easily reach 100,000 terms. Consider a term with frequency 1 in 1000 terms on average. The result is that large documents cause an increase of two orders of magnitude in the space required to store the postings list:\\n</p><p>Expected Expected entries Document size postings in positional posting\\n</p><p>1000 1 1\\n</p><p>100,000 1 100\\n</p><p>While the exact numbers depend on the type of documents and the language being indexed, some rough rules of thumb are to expect a positional index to be 2 to 4 times as large as a non-positional index, and to expect a compressed positional index to be about one third to one half the size of the raw text (after removal of markup, etc.) of the original uncompressed documents. Specific numbers for an example collection are given in Table 5.1 (page 87) and Table 5.6 (page 103).\\n</p><p><b>2.4.3 Combination schemes\\n</b></p><p>The strategies of biword indexes and positional indexes can be fruitfully combined. If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists. A combination strategy uses a phrase index, or just a biword index, for certain queries and uses a positional index for other phrase queries. Good queries to include in the phrase index are ones known to be common based on recent querying behavior. But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare. Adding <i>Britney Spears </i>as a phrase index entry may only give a speedup factor to that query of about 3, since most documents that mention either word are valid results, whereas adding <i>The Who </i>as a phrase index entry may speed up that query by a factor of 1000. Hence, having the latter is more desirable, even if it is a relatively less common query.\\n</p><p>Williams et al. (2004) evaluate an even more sophisticated scheme which employs indexes of both these sorts and additionally a partial next word index as a halfway house between the first two strategies. For each term, a\\n<i>next word index </i>records terms that follow it in a document. They concludeNEXT WORD INDEX\\n</p><p>that such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone.\\n</p><p><i>? </i><b>Exercise 2.8 </b>[\\u22c6]Assume a biword index. Give an example of a document which will be returned for a query of New York University but is actually a false positive which should not be returned.\\n</p><p><b>Exercise 2.9 </b>[\\u22c6]\\n</p><p>Shown below is a portion of a positional index in the format: term: doc1: \\u3008position1, position2, . . . \\u3009; doc2: \\u3008position1, position2, . . . \\u3009; etc.\\n</p><p>angels: 2: \\u300836,174,252,651\\u3009; 4: \\u300812,22,102,432\\u3009; 7: \\u300817\\u3009; fools: 2: \\u30081,17,74,222\\u3009; 4: \\u30088,78,108,458\\u3009; 7: \\u30083,13,23,193\\u3009; fear: 2: \\u300887,704,722,901\\u3009; 4: \\u300813,43,113,433\\u3009; 7: \\u300818,328,528\\u3009; in: 2: \\u30083,37,76,444,851\\u3009; 4: \\u300810,20,110,470,500\\u3009; 7: \\u30085,15,25,195\\u3009; rush: 2: \\u30082,66,194,321,702\\u3009; 4: \\u30089,69,149,429,569\\u3009; 7: \\u30084,14,404\\u3009; to: 2: \\u300847,86,234,999\\u3009; 4: \\u300814,24,774,944\\u3009; 7: \\u3008199,319,599,709\\u3009; tread: 2: \\u300857,94,333\\u3009; 4: \\u300815,35,155\\u3009; 7: \\u300820,320\\u3009; where: 2: \\u300867,124,393,1001\\u3009; 4: \\u300811,41,101,421,431\\u3009; 7: \\u300816,36,736\\u3009;\\n</p><p>Which document(s) if any match each of the following queries, where each expression within quotes is a phrase query?\\n</p><p>a. \\\"fools rush in\\\" b. \\\"fools rush in\\\" AND \\\"angels fear to tread\\\"\\n</p><p><b>Exercise 2.10 </b>[\\u22c6]\\n</p><p>Consider the following fragment of a positional index with the format:\\n</p><p>word: document: \\u3008position, position, . . .\\u3009; document: \\u3008position, . . .\\u3009 . . .\\n</p><p>Gates: 1: \\u30083\\u3009; 2: \\u30086\\u3009; 3: \\u30082,17\\u3009; 4: \\u30081\\u3009; IBM: 4: \\u30083\\u3009; 7: \\u300814\\u3009;\\n</p><p>Microsoft: 1: \\u30081\\u3009; 2: \\u30081,21\\u3009; 3: \\u30083\\u3009; 5: \\u300816,22,51\\u3009;\\n</p><p>The /<i>k </i>operator, word1 /<i>k </i>word2 finds occurrences of word1 within <i>k </i>words of word2 (on either side), where <i>k </i>is a positive integer argument. Thus <i>k </i>= 1 demands that word1 be adjacent to word2.\\n</p><p>a. Describe the set of documents that satisfy the query Gates /2 Microsoft.\\n</p><p>b. Describe each set of values for <i>k </i>for which the query Gates /<i>k </i>Microsoft returns a different set of documents as the answer.\\n</p><p><b>Exercise 2.11 </b>[\\u22c6\\u22c6]\\n</p><p>Consider the general procedure for merging two positional postings lists for a given document, to determine the document positions where a document satisfies a /<i>k\\n</i>clause (in general there can be multiple positions at which each term occurs in a single document). We begin with a pointer to the position of occurrence of each term and move each pointer along the list of occurrences in the document, checking as we do so whether we have a hit for /<i>k</i>. Each move of either pointer counts as a step. Let\\n<i>L </i>denote the total number of occurrences of the two terms in the document. What is the big-O complexity of the merge procedure, if we wish to have postings including positions in the result?\\n</p><p><b>Exercise 2.12 </b>[\\u22c6\\u22c6]\\n</p><p>Consider the adaptation of the basic algorithm for intersection of two postings lists (Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\\n</p><p>queries. A naive algorithm for this operation could be <i>O</i>(<i>PL</i>max2), where <i>P </i>is the sum of the lengths of the postings lists (i.e., the sum of document frequencies) and\\n<i>L</i>max is the maximum length of a document (in tokens).\\n</p><p>a. Go through this algorithm carefully and explain how it works.\\n</p><p>b. What is the complexity of this algorithm? Justify your answer carefully.\\n</p><p>c. For certain queries and data distributions, would another algorithm be more efficient? What complexity does it have?\\n</p><p><b>Exercise 2.13 </b>[\\u22c6\\u22c6]\\n</p><p>Suppose we wish to use a postings intersection procedure to determine simply the list of documents that satisfy a /<i>k </i>clause, rather than returning the list of positions, as in Figure 2.12 (page 42). For simplicity, assume <i>k </i>\\u2265 2. Let <i>L </i>denote the total number of occurrences of the two terms in the document collection (i.e., the sum of their collection frequencies). Which of the following is true? Justify your answer.\\n</p><p>a. The merge can be accomplished in a number of steps linear in <i>L </i>and independent of <i>k</i>, and we can ensure that each pointer moves only to the right.\\n</p><p>b. The merge can be accomplished in a number of steps linear in <i>L </i>and independent of <i>k</i>, but a pointer may be forced to move non-monotonically (i.e., to sometimes back up)\\n</p><p>c. The merge can require <i>kL </i>steps in some cases.\\n</p><p><b>Exercise 2.14 </b>[\\u22c6\\u22c6]\\n</p><p>How could an IR system combine use of a positional index and use of stop words? What is the potential problem, and how could it be handled?\\n</p><p><b>2.5 References and further reading\\n</b></p><p>Exhaustive discussion of the character-level processing of East Asian lan-EAST ASIAN LANGUAGES guages can be found in Lunde (1998). Character bigram indexes are perhaps the most standard approach to indexing Chinese, although some systems use word segmentation. Due to differences in the language and writing system, word segmentation is most usual for Japanese (Luk and Kwok 2002, Kishida\\net al. 2005). The structure of a character <i>k</i>-gram index over unsegmented text differs from that in Section 3.2.2 (page 54): there the <i>k</i>-gram dictionary points to postings lists of entries in the regular dictionary, whereas here it points directly to document postings lists. For further discussion of Chinese word segmentation, see Sproat et al. (1996), Sproat and Emerson (2003), Tseng et al. (2005), and Gao et al. (2005).\\n</p><p>Lita et al. (2003) present a method for truecasing. Natural language processing work on computational morphology is presented in (Sproat 1992, Beesley and Karttunen 2003).\\n</p><p>Language identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level <i>k</i>-gram language identification algorithm. While other methods such as looking for particular distinctive function words and letter combinations have been used, with the advent of widespread digital text, many people have explored the character <i>n</i>-gram technique, and found it to be highly successful (Beesley 1998, Dunning 1994, Cavnar and Trenkle 1994). Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al. (2006) for a recent survey.\\n</p><p>Experiments on and discussion of the positive and negative impact of stemming in English can be found in the following works: Salton (1989), Har-man (1991), Krovetz (1995), Hull (1996). Hollink et al. (2004) provide detailed results for the effectiveness of language-specific methods on 8 European languages. In terms of percent change in mean average precision (see page 159) over a baseline system, diacritic removal gains up to 23% (being especially helpful for Finnish, French, and Swedish). Stemming helped markedly for Finnish (30% improvement) and Spanish (10% improvement), but for most languages, including English, the gain from stemming was in the range 0- 5%, and results from a lemmatizer were poorer still. Compound splitting gained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather than language-particular methods, indexing character <i>k</i>-grams (as we suggested for Chinese) could often give as good or better results: using within-word character 4-grams rather than words gave gains of 37% in Finnish, 27% in Swedish, and 20% in German, while even being slightly positive for other languages, such as Dutch, Spanish, and English. Tomlinson (2003) presents broadly similar results. Bar-Ilan and Gutman (2005) suggest that, at the time of their study (2003), the major commercial web search engines suffered from lacking decent language-particular processing; for example, a query on www.google.fr for l'\\u00e9lectricit\\u00e9 did not separate off the article <i>l' </i>but only matched pages with precisely this string of article+noun.\\n</p><p>The classic presentation of skip pointers for IR can be found in Moffat andSKIP LIST Zobel (1996). Extended techniques are discussed in Boldi and Vigna (2005). The main paper in the algorithms literature is Pugh (1990), which uses multilevel skip pointers to give expected <i>O</i>(log <i>P</i>) list access (the same expected\\nefficiency as using a tree data structure) with less implementational complexity. In practice, the effectiveness of using skip pointers depends on various system parameters. Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al. (2002, p. 217) report that, with modern CPUs, using skip lists instead slows down search because it expands the size of the postings list (i.e., disk I/O dominates performance). In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs.\\n</p><p>Johnson et al. (2006) report that 11.7% of all queries in two 2002 web query logs contained phrase queries, though Kammenhuber et al. (2006) report only 3% phrase queries for a different data set. Silverstein et al. (1999) note that many queries without explicit phrase operators are actually implicit phrase searches.\\n</p></body></html>\",\n",
      "      \"text\": \"Introduction to Information Retrieval\\n\\n2\\n\\nThe term vocabulary and postingslists\\n\\nRecall the major steps in inverted index construction:\\n\\n1. Collect the documents to be indexed.\\n\\n2. Tokenize the text.\\n\\n3. Do linguistic preprocessing of tokens.\\n\\n4. Index the documents that each term occurs in.\\n\\nIn this chapter we first briefly mention how the basic unit of a document can be defined and how the character sequence that it comprises is determined (Section 2.1). We then examine in detail some of the substantive linguistic issues of tokenization and linguistic preprocessing, which determine the vocabulary of terms which a system uses (Section 2.2). Tokenization is the process of chopping character streams into tokens, while linguistic preprocessing then deals with building equivalence classes of tokens which are the set of terms that are indexed. Indexing itself is covered in Chapters 1 and 4. Then we return to the implementation of postings lists. In Section 2.3, we examine an extended postings list data structure that supports faster querying, while Section 2.4 covers building postings data structures suitable for handling phrase and proximity queries, of the sort that commonly appear in both extended Boolean models and on the web.\\n\\n2.1 Document delineation and character sequence decoding\\n\\n2.1.1 Obtaining the character sequence in a document\\n\\nDigital documents that are the input to an indexing process are typically bytes in a file or on a web server. The first step of processing is to convert this byte sequence into a linear sequence of characters. For the case of plain English text in ASCII encoding, this is trivial. But often things get much more complex. The sequence of characters may be encoded by one of various single byte or multibyte encoding schemes, such as Unicode UTF-8, or various national or vendor-specific standards. We need to determine the correct encoding. This can be regarded as a machine learning classification problem, as discussed in Chapter 13,1 but is often handled by heuristic methods, user selection, or by using provided document metadata. Once the encoding is determined, we decode the byte sequence to a character sequence. We might save the choice of encoding because it gives some evidence about what language the document is written in.\\n\\nThe characters may have to be decoded out of some binary representation like Microsoft Word DOC files and/or a compressed format such as zip files. Again, we must determine the document format, and then an appropriate decoder has to be used. Even for plain text documents, additional decoding may need to be done. In XML documents (Section 10.1, page 197), character entities, such as &amp; , need to be decoded to give the correct character, namely & for &amp; . Finally, the textual part of the document may need to be extracted out of other material that will not be processed. This might be the desired handling for XML files, if the markup is going to be ignored; we would almost certainly want to do this with postscript or PDF files. We will not deal further with these issues in this book, and will assume henceforth that our documents are a list of characters. Commercial products usually need to support a broad range of document types and encodings, since users want things to just work with their data as is. Often, they just think of documents as text inside applications and are not even aware of how it is encoded on disk. This problem is usually solved by licensing a software library that handles decoding document formats and character encodings.\\n\\nThe idea that text is a linear sequence of characters is also called into question by some writing systems, such as Arabic, where text takes on some two dimensional and mixed order characteristics, as shown in Figures 2.1 and 2.2. But, despite some complicated writing system conventions, there is an underlying sequence of sounds being represented and hence an essentially linear structure remains, and this is what is represented in the digital representation of Arabic, as shown in Figure 2.1.\\n\\n2.1.2 Choosing a document unit\\n\\nThe next phase is to determine what the document unit for indexing is. ThusDOCUMENT UNIT far we have assumed that documents are fixed units for the purposes of indexing. For example, we take each file in a folder as a document. But there\\n\\n1. A classifier is a function that takes objects of some sort and assigns them to one of a number of distinct classes (see Chapter 13). Usually classification is done by machine learning methods such as probabilistic models, but it can also be done by hand-written rules.\\n\\n\\ufe7a\\ufe81\\ufe76 \\ufe72\\ufe8f\\n\\n\\ufe8f \\ufe72 \\u21d0\\n\\n\\ufe8d\\n\\n\\ufed9 \\ufe7a \\ufe95\\n\\nun b \\u0101 t i k /kit\\u0101bun/ 'a book'\\n\\n\\u25ee Figure 2.1 An example of a vocalized Modern Standard Arabic word. The writing is from right to left and letters undergo complex mutations as they are combined. The representation of short vowels (here, /i/ and /u/) and the final /n/ (nunation) departs from strict linearity by being represented as diacritics above and below letters. Nevertheless, the represented text is still clearly a linear ordering of characters representing sounds. Full vocalization, as here, normally appears only in the Koran and children's books. Day-to-day text is unvocalized (short vowels are not represented but the letter for a\\u0304 would still appear) or partially vocalized, with short vowels inserted in places where the writer perceives ambiguities. These choices add further complexities to indexing.\\n\\n\\ufe8d \\ufe8d . #\\\"!\\\" ! \\ufe8d \\ufedd \\ufe8d 132 1962\\ufe8d\\n\\n\\u2190 \\u2192 \\u2190 \\u2192 \\u2190 START\\n\\n'Algeria achieved its independence in 1962 after 132 years of French occupation.'\\n\\n\\u25ee Figure 2.2 The conceptual linear order of characters is not necessarily the order that you see on the page. In languages that are written right-to-left, such as Hebrew and Arabic, it is quite common to also have left-to-right text interspersed, such as numbers and dollar amounts. With modern Unicode representation concepts, the order of characters in files matches the conceptual order, and the reversal of displayed characters is handled by the rendering system, but this may not be true for documents in older encodings.\\n\\nare many cases in which you might want to do something different. A traditional Unix (mbox-format) email file stores a sequence of email messages (an email folder) in one file, but you might wish to regard each email message as a separate document. Many email messages now contain attached documents, and you might then want to regard the email message and each contained attachment as separate documents. If an email message has an attached zip file, you might want to decode the zip file and regard each file it contains as a separate document. Going in the opposite direction, various pieces of web software (such as latex2html) take things that you might regard as a single document (e.g., a Powerpoint file or a LATEX document) and split them into separate HTML pages for each slide or subsection, stored as separate files. In these cases, you might want to combine multiple files into a single document.\\n\\nMore generally, for very long documents, the issue of indexing granularityINDEXING GRANULARITY arises. For a collection of books, it would usually be a bad idea to index an entire book as a document. A search for Chinese toys might bring up a book that mentions China in the first chapter and toys in the last chapter, but this does not make it relevant to the query. Instead, we may well wish to index each chapter or paragraph as a mini-document. Matches are then more likely to be relevant, and since the documents are smaller it will be much easier for the user to find the relevant passages in the document. But why stop there? We could treat individual sentences as mini-documents. It becomes clear that there is a precision/recall tradeoff here. If the units get too small, we are likely to miss important passages because terms were distributed over several mini-documents, while if units are too large we tend to get spurious matches and the relevant information is hard for the user to find.\\n\\nThe problems with large document units can be alleviated by use of explicit or implicit proximity search (Sections 2.4.2 and 7.2.2), and the tradeoffs in resulting system performance that we are hinting at are discussed in Chapter 8. The issue of index granularity, and in particular a need to simultaneously index documents at multiple levels of granularity, appears prominently in XML retrieval, and is taken up again in Chapter 10. An IR system should be designed to offer choices of granularity. For this choice to be made well, the person who is deploying the system must have a good understanding of the document collection, the users, and their likely information needs and usage patterns. For now, we will henceforth assume that a suitable size document unit has been chosen, together with an appropriate way of dividing or aggregating files, if needed.\\n\\n2.2 Determining the vocabulary of terms\\n\\n2.2.1 Tokenization\\n\\nGiven a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, perhaps at the same time throwing away certain characters, such as punctuation. Here is an example of tokenization:\\n\\nInput: Friends, Romans, Countrymen, lend me your ears;\\n\\nOutput: Friends Romans Countrymen lend me your ears\\n\\nThese tokens are often loosely referred to as terms or words, but it is sometimes important to make a type/token distinction. A token is an instanceTOKEN\\n\\nof a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of allTYPE\\n\\ntokens containing the same character sequence. A term is a (perhaps nor-TERM malized) type that is included in the IR system's dictionary. The set of index terms could be entirely distinct from the tokens, for instance, they could be semantic identifiers in a taxonomy, but in practice in modern IR systems they are strongly related to the tokens in the document. However, rather than being exactly the tokens that appear in the document, they are usually derived from them by various normalization processes which are discussed in Section 2.2.3.2 For example, if the document to be indexed is to sleep perchance to dream, then there are 5 tokens, but only 4 types (since there are 2 instances of to). However, if to is omitted from the index (as a stop word, see Section 2.2.2 (page 27)), then there will be only 3 terms: sleep, perchance, and dream.\\n\\nThe major question of the tokenization phase is what are the correct tokens to use? In this example, it looks fairly trivial: you chop on whitespace and throw away punctuation characters. This is a starting point, but even for English there are a number of tricky cases. For example, what do you do about the various uses of the apostrophe for possession and contractions?\\n\\nMr. O'Neill thinks that the boys' stories about Chile's capital aren't amusing.\\n\\nFor O'Neill, which of the following is the desired tokenization?\\n\\nneill\\n\\noneill\\n\\no'neill\\n\\no' neill\\n\\no neill ?\\n\\nAnd for aren't, is it:\\n\\naren't arent\\n\\nare n't\\n\\naren t ?\\n\\nA simple strategy is to just split on all non-alphanumeric characters, but\\n\\nwhile o neill looks okay, aren t looks intuitively bad. For all of them, the choices determine which Boolean queries will match. A query of neill AND capital will match in three cases but not the other two. In how many cases would a query of o'neill AND capital match? If no preprocessing of a query is done, then it would match in only one of the five cases. For either\\n\\n2. That is, as defined here, tokens that are not indexed (stop words) are not terms, and if multiple tokens are collapsed together via normalization, they are indexed as one term, under the normalized form. However, we later relax this definition when discussing classification and clustering in Chapters 13-18, where there is no index. In these chapters, we drop the requirement of inclusion in the dictionary. A term means a normalized word.\\n\\nBoolean or free text queries, you always want to do the exact same tokeniza-tion of document and query words, generally by processing queries with the same tokenizer. This guarantees that a sequence of characters in a text will always match the same sequence typed in a query.3\\n\\nThese issues of tokenization are language-specific. It thus requires the language of the document to be known. Language identification based on clas-LANGUAGE\\n\\nIDENTIFICATION sifiers that use short character subsequences as features is highly effective; most languages have distinctive signature patterns (see page 46 for references).\\n\\nFor most languages and particular domains within them there are unusual specific tokens that we wish to recognize as terms, such as the programming languages C++ and C#, aircraft names like B-52, or a T.V. show name such as M*A*S*H - which is sufficiently integrated into popular culture that you find usages such as M*A*S*H-style hospitals. Computer technology has introduced new types of character sequences that a tokenizer should probably tokenize as a single token, including email addresses (jblack@mail.yahoo.com), web URLs (http://stuff.big.com/new/specials.html),numeric IP addresses (142.32.48.231), package tracking numbers (1Z9999W99845399981), and more. One possible solution is to omit from indexing tokens such as monetary amounts, numbers, and URLs, since their presence greatly expands the size of the vocabulary. However, this comes at a large cost in restricting what people can search for. For instance, people might want to search in a bug database for the line number where an error occurs. Items such as the date of an email, which have a clear semantic type, are often indexed separately as document metadata (see Section 6.1, page 110).\\n\\nIn English, hyphenation is used for various purposes ranging from split-HYPHENS ting up vowels in words (co-education) to joining nouns as names (Hewlett-Packard) to a copyediting device to show word grouping (the hold-him-back-and-drag-him-away maneuver). It is easy to feel that the first example should be regarded as one token (and is indeed more commonly written as just coeducation), the last should be separated into words, and that the middle case is unclear. Handling hyphens automatically can thus be complex: it can either be done as a classification problem, or more commonly by some heuristic rules, such as allowing short hyphenated prefixes on words, but not longer hyphenated forms.\\n\\nConceptually, splitting on white space can also split what should be regarded as a single token. This occurs most commonly with names (San Fran-cisco, Los Angeles) but also with borrowed foreign phrases (au fait) and com-\\n\\n3. For the free text case, this is straightforward. The Boolean case is more complex: this tok-enization may produce multiple terms from one query word. This can be handled by combining the terms with an AND or as a phrase query (see Section 2.4, page 39). It is harder for a system to handle the opposite case where the user entered as two terms something that was tokenized together in the document processing.\\n\\npounds that are sometimes written as a single word and sometimes space separated (such as white space vs. whitespace). Other cases with internal spaces that we might wish to regard as a single token include phone numbers ((800) 234-\\n\\n2333) and dates (Mar 11, 1983). Splitting tokens on spaces can cause bad retrieval results, for example, if a search for York University mainly returns documents containing New York University. The problems of hyphens and non-separating whitespace can even interact. Advertisements for air fares frequently contain items like San Francisco-Los Angeles, where simply doing whitespace splitting would give unfortunate results. In such cases, issues of tokenization interact with handling phrase queries (which we discuss in Section 2.4 (page 39)), particularly if we would like queries for all of lowercase, lower-case and lower case to return the same results. The last two can be handled by splitting on hyphens and using a phrase index. Getting the first case right would depend on knowing that it is sometimes written as two words and also indexing it in this way. One effective strategy in practice, which is used by some Boolean retrieval systems such as Westlaw and Lexis-Nexis (Example 1.1), is to encourage users to enter hyphens wherever they may be possible, and whenever there is a hyphenated form, the system will generalize the query to cover all three of the one word, hyphenated, and two word forms, so that a query for over-eager will search for over-eager OR \\\"over eager\\\" OR overeager. However, this strategy depends on user training, since if you query using either of the other two forms, you get no generalization.\\n\\nEach new language presents some new issues. For instance, French has a variant use of the apostrophe for a reduced definite article 'the' before a word beginning with a vowel (e.g., l'ensemble) and has some uses of the hyphen with postposed clitic pronouns in imperatives and questions (e.g., donne-moi 'give me'). Getting the first case correct will affect the correct indexing of a fair percentage of nouns and adjectives: you would want documents mentioning both l'ensemble and un ensemble to be indexed under ensemble. Other languages make the problem harder in new ways. German writes compound nouns without spaces (e.g., Computerlinguistik 'computational lin-COMPOUNDS\\n\\nguistics'; Lebensversicherungsgesellschaftsangestellter 'life insurance company employee'). Retrieval systems for German greatly benefit from the use of a compound-splitter module, which is usually implemented by seeing if a wordCOMPOUND-SPLITTER\\n\\ncan be subdivided into multiple words that appear in a vocabulary. This phenomenon reaches its limit case with major East Asian Languages (e.g., Chi-nese, Japanese, Korean, and Thai), where text is written without any spaces between words. An example is shown in Figure 2.3. One approach here is to perform word segmentation as prior linguistic processing. Methods of wordWORD SEGMENTATION\\n\\nsegmentation vary from having a large vocabulary and taking the longest vocabulary match with some heuristics for unknown words to the use of machine learning sequence models, such as hidden Markov models or conditional random fields, trained over hand-segmented words (see the references\\n\\n26 2 The term vocabulary and postings lists\\ufffd \\ufffd ! \\\" # $ % & ' ' ( ) * \\ufffd + , # - . /\\n\\n\\u25ee Figure 2.3 The standard unsegmented form of Chinese text using the simplified characters of mainland China. There is no whitespace between words, not even between sentences - the apparent space after the Chinese period (\\u25e6) is just a typographical illusion caused by placing the character on the left side of its square box. The first sentence is just words in Chinese characters with no spaces between them. The second and third sentences include Arabic numerals and punctuation breaking up the Chinese characters.\\n\\n\\u25ee Figure 2.4 Ambiguities in Chinese word segmentation. The two characters can be treated as one word meaning 'monk' or as a sequence of two words meaning 'and' and 'still'.\\n\\na an and are as at be by for from has he in is it its of on that the to was were will with \\u25ee Figure 2.5 A stop list of 25 semantically non-selective words which are common in Reuters-RCV1.\\n\\nin Section 2.5). Since there are multiple possible segmentations of character sequences (see Figure 2.4), all such methods make mistakes sometimes, and so you are never guaranteed a consistent unique tokenization. The other approach is to abandon word-based indexing and to do all indexing via just short subsequences of characters (character k-grams), regardless of whether particular sequences cross word boundaries or not. Three reasons why this approach is appealing are that an individual Chinese character is more like a syllable than a letter and usually has some semantic content, that most words are short (the commonest length is 2 characters), and that, given the lack of standardization of word breaking in the writing system, it is not always clear where word boundaries should be placed anyway. Even in English, some cases of where to put word boundaries are just orthographic conventions - think of notwithstanding vs. not to mention or into vs. on to - but people are educated to write the words with consistent use of spaces.\\n\\n2.2.2 Dropping common terms: stop words\\n\\nSometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words. The generalSTOP WORDS\\n\\nstrategy for determining a stop list is to sort the terms by collection frequencyCOLLECTION FREQUENCY (the total number of times each term appears in the document collection), and then to take the most frequent terms, often hand-filtered for their semantic content relative to the domain of the documents being indexed, as a stop list, the members of which are then discarded during indexing. AnSTOP LIST\\n\\nexample of a stop list is shown in Figure 2.5. Using a stop list significantly reduces the number of postings that a system has to store; we will present some statistics on this in Chapter 5 (see Table 5.1, page 87). And a lot of the time not indexing stop words does little harm: keyword searches with terms like the and by don't seem very useful. However, this is not true for phrase searches. The phrase query \\\"President of the United States\\\", which contains two stop words, is more precise than President AND \\\"United States\\\". The meaning of flights to London is likely to be lost if the word to is stopped out. A search for Vannevar Bush's article As we may think will be difficult if the first three words are stopped out, and the system searches simply for documents containing the word think. Some special query types are disproportionately affected. Some song titles and well known pieces of verse consist entirely of words that are commonly on stop lists (To be or not to be, Let It Be, I don't want to be, . . . ).\\n\\nThe general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists. Some of the design of modern IR systems has focused precisely on how we can exploit the statistics of language so as to be able to cope with common words in better ways. We will show in Section 5.3 (page 95) how good compression techniques greatly reduce the cost of storing the postings for common words. Section 6.2.1 (page 117) then discusses how standard term weighting leads to very common words having little impact on document rankings. Finally, Section 7.1.5 (page 140) shows how an IR system with impact-sorted indexes can terminate scanning a postings list early when weights get small, and hence common words do not cause a large additional processing cost for the average query, even though postings lists for stop words are very long. So for most modern IR systems, the additional cost of including stop words is not that big - neither in terms of index size nor in terms of query processing time.\\n\\nQuery term Terms in documents that should be matched\\n\\nWindows Windows windows Windows, windows, window window window, windows\\n\\n\\u25ee Figure 2.6 An example of how asymmetric expansion of query terms can usefully model users' expectations.\\n\\n2.2.3 Normalization (equivalence classing of terms)\\n\\nHaving broken up our documents (and also our query) into tokens, the easy case is if tokens in the query just match tokens in the token list of the document. However, there are many cases when two character sequences are not quite the same but you would like a match to occur. For instance, if you search for USA, you might hope to also match documents containing U.S.A. Token normalization is the process of canonicalizing tokens so that matchesTOKEN\\n\\nNORMALIZATION occur despite superficial differences in the character sequences of the tokens.4 The most standard way to normalize is to implicitly create equivalenceEQUIVALENCE CLASSES\\n\\nclasses, which are normally named after one member of the set. For instance, if the tokens anti-discriminatory and antidiscriminatory are both mapped onto the term antidiscriminatory, in both the document text and queries, then searches for one term will retrieve documents that contain either.\\n\\nThe advantage of just using mapping rules that remove characters like hyphens is that the equivalence classing to be done is implicit, rather than being fully calculated in advance: the terms that happen to become identical as the result of these rules are the equivalence classes. It is only easy to write rules of this sort that remove characters. Since the equivalence classes are implicit, it is not obvious when you might want to add characters. For instance, it would be hard to know to turn antidiscriminatory into anti-discriminatory.\\n\\nAn alternative to creating equivalence classes is to maintain relations between unnormalized tokens. This method can be extended to hand-constructed lists of synonyms such as car and automobile, a topic we discuss further in Chapter 9. These term relationships can be achieved in two ways. The usual way is to index unnormalized tokens and to maintain a query expansion list of multiple vocabulary entries to consider for a certain query term. A query term is then effectively a disjunction of several postings lists. The alternative is to perform the expansion during index construction. When the document contains automobile, we index it under car as well (and, usually, also vice-versa). Use of either of these methods is considerably less efficient than equivalence classing, as there are more postings to store and merge. The first\\n\\n4. It is also often referred to as term normalization, but we prefer to reserve the name term for the output of the normalization process.\\n\\nmethod adds a query expansion dictionary and requires more processing at query time, while the second method requires more space for storing postings. Traditionally, expanding the space required for the postings lists was seen as more disadvantageous, but with modern storage costs, the increased flexibility that comes from distinct postings lists is appealing.\\n\\nThese approaches are more flexible than equivalence classes because the expansion lists can overlap while not being identical. This means there can be an asymmetry in expansion. An example of how such an asymmetry can be exploited is shown in Figure 2.6: if the user enters windows, we wish to allow matches with the capitalized Windows operating system, but this is not plausible if the user enters window, even though it is plausible for this query to also match lowercase windows.\\n\\nThe best amount of equivalence classing or query expansion to do is a fairly open question. Doing some definitely seems a good idea. But doing a lot can easily have unexpected consequences of broadening queries in unintended ways. For instance, equivalence-classing U.S.A. and USA to the latter by deleting periods from tokens might at first seem very reasonable, given the prevalent pattern of optional use of periods in acronyms. However, if I put in as my query term C.A.T., I might be rather upset if it matches every appearance of the word cat in documents.5\\n\\nBelow we present some of the forms of normalization that are commonly employed and how they are implemented. In many cases they seem helpful, but they can also do harm. In fact, you can worry about many details of equivalence classing, but it often turns out that providing processing is done consistently to the query and to documents, the fine details may not have much aggregate effect on performance.\\n\\nAccents and diacritics. Diacritics on characters in English have a fairly marginal status, and we might well want clich\\u00e9 and cliche to match, or naive and na\\u00efve. This can be done by normalizing tokens to remove diacritics. In many other languages, diacritics are a regular part of the writing system and distinguish different sounds. Occasionally words are distinguished only by their accents. For instance, in Spanish, pe\\u00f1a is 'a cliff', while pena is 'sorrow'. Nevertheless, the important question is usually not prescriptive or linguistic but is a question of how users are likely to write queries for these words. In many cases, users will enter queries for words without diacritics, whether for reasons of speed, laziness, limited software, or habits born of the days when it was hard to use non-ASCII text on many computer systems. In these cases, it might be best to equate all words to a form without diacritics.\\n\\n5. At the time we wrote this chapter (Aug. 2005), this was actually the case on Google: the top result for the query C.A.T. was a site about cats, the Cat Fanciers Web Site http://www.fanciers.com/.\\n\\nCapitalization/case-folding. A common strategy is to do case-folding by re-CASE-FOLDING ducing all letters to lower case. Often this is a good idea: it will allow instances of Automobile at the beginning of a sentence to match with a query of automobile. It will also help on a web search engine when most of your users type in ferrari when they are interested in a Ferrari car. On the other hand, such case folding can equate words that might better be kept apart. Many proper nouns are derived from common nouns and so are distinguished only by case, including companies (General Motors, The Associated Press), government organizations (the Fed vs. fed) and person names (Bush, Black). We already mentioned an example of unintended query expansion with acronyms, which involved not only acronym normalization (C.A.T. \\u2192 CAT) but also case-folding (CAT\\u2192 cat).\\n\\nFor English, an alternative to making every token lowercase is to just make some tokens lowercase. The simplest heuristic is to convert to lowercase words at the beginning of a sentence and all words occurring in a title that is all uppercase or in which most or all words are capitalized. These words are usually ordinary words that have been capitalized. Mid-sentence capitalized words are left as capitalized (which is usually correct). This will mostly avoid case-folding in cases where distinctions should be kept apart. The same task can be done more accurately by a machine learning sequence model which uses more features to make the decision of when to case-fold. This is known as truecasing. However, trying to get capitalization right in this way probablyTRUECASING\\n\\ndoesn't help if your users usually use lowercase regardless of the correct case of words. Thus, lowercasing everything often remains the most practical solution.\\n\\nOther issues in English. Other possible normalizations are quite idiosyncratic and particular to English. For instance, you might wish to equate ne'er and never or the British spelling colour and the American spelling color. Dates, times and similar items come in multiple formats, presenting additional challenges. You might wish to collapse together 3/12/91 and Mar. 12, 1991. However, correct processing here is complicated by the fact that in the U.S., 3/12/91 is Mar. 12, 1991, whereas in Europe it is 3 Dec 1991.\\n\\nOther languages. English has maintained a dominant position on the WWW; approximately 60% of web pages are in English (Gerrand 2007). But that still leaves 40% of the web, and the non-English portion might be expected to grow over time, since less than one third of Internet users and less than 10% of the world's population primarily speak English. And there are signs of change: Sifry (2007) reports that only about one third of blog posts are in English.\\n\\nOther languages again present distinctive issues in equivalence classing.\\n\\n2.2 Determining the vocabulary of terms 31\\ufffd ! \\\" ! # $ % & ' ( ) * + , - . / 0 ) 1 2 3 4 5 6 7 & + 8 9 : ; : < = > ? @ A B C - D E6 8 9 : ; : < ) F G * H I * : J ) K + L M N ? O P Q RS T U V V W X Y & Z [ N ? ) + \\\\ ] ; ^ _ + ` 4 a + b; c d e * f V g h V - ? i N j k l m n : A o p N 5 +q V r s t u & v w x ) Q y z { h | & } ~ \\ufffd M ? @ A\\n\\n\\u25ee Figure 2.7 Japanese makes use of multiple intermingled writing systems and, like Chinese, does not segment words. The text is mainly Chinese characters with the hiragana syllabary for inflectional endings and function words. The part in latin letters is actually a Japanese expression, but has been taken up as the name of an environmental campaign by 2004 Nobel Peace Prize winner Wangari Maathai. His name is written using the katakana syllabary in the middle of the first line. The first four characters of the final line express a monetary amount that we would want to match with \\u00a5500,000 (500,000 Japanese yen).\\n\\nThe French word for the has distinctive forms based not only on the gender (masculine or feminine) and number of the following noun, but also depending on whether the following word begins with a vowel: le, la, l', les. We may well wish to equivalence class these various forms of the. German has a convention whereby vowels with an umlaut can be rendered instead as a two vowel digraph. We would want to treat Sch\\u00fctze and Schuetze as equivalent.\\n\\nJapanese is a well-known difficult writing system, as illustrated in Figure 2.7. Modern Japanese is standardly an intermingling of multiple alphabets, principally Chinese characters, two syllabaries (hiragana and katakana) and western characters (Latin letters, Arabic numerals, and various symbols). While there are strong conventions and standardization through the education system over the choice of writing system, in many cases the same word can be written with multiple writing systems. For example, a word may be written in katakana for emphasis (somewhat like italics). Or a word may sometimes be written in hiragana and sometimes in Chinese characters. Successful retrieval thus requires complex equivalence classing across the writing systems. In particular, an end user might commonly present a query entirely in hiragana, because it is easier to type, just as Western end users commonly use all lowercase.\\n\\nDocument collections being indexed can include documents from many different languages. Or a single document can easily contain text from multiple languages. For instance, a French email might quote clauses from a contract document written in English. Most commonly, the language is detected and language-particular tokenization and normalization rules are applied at a predetermined granularity, such as whole documents or individual paragraphs, but this still will not correctly deal with cases where language changes occur for brief quotations. When document collections contain mul- tiple languages, a single index may have to contain terms of several languages. One option is to run a language identification classifier on documents and then to tag terms in the vocabulary for their language. Or this tagging can simply be omitted, since it is relatively rare for the exact same character sequence to be a word in different languages.\\n\\nWhen dealing with foreign or complex words, particularly foreign names, the spelling may be unclear or there may be variant transliteration standards giving different spellings (for example, Chebyshev and Tchebycheff or Beijing and Peking). One way of dealing with this is to use heuristics to equivalence class or expand terms with phonetic equivalents. The traditional and best known such algorithm is the Soundex algorithm, which we cover in Section 3.4 (page 63).\\n\\n2.2.4 Stemming and lemmatization\\n\\nFor grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\\n\\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:\\n\\nam, are, is\\u21d2 be car, cars, car's, cars'\\u21d2 car\\n\\nThe result of this mapping of text will be something like:\\n\\nthe boy's cars are different colors\\u21d2 the boy car be differ color\\n\\nHowever, the two words differ in their flavor. Stemming usually refers toSTEMMING a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing thingsLEMMATIZATION\\n\\nproperly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma. If confrontedLEMMA\\n\\nwith the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatiza-tion commonly only collapses the different inflectional forms of a lemma.\\n\\nLinguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\\n\\nThe most common algorithm for stemming English, and one that has repeatedly been shown to be empirically very effective, is Porter's algorithmPORTER STEMMER\\n\\n(Porter 1980). The entire algorithm is too long and intricate to present here, but we will indicate its general nature. Porter's algorithm consists of 5 phases of word reductions, applied sequentially. Within each phase there are various conventions to select rules, such as selecting the rule from each rule group that applies to the longest suffix. In the first phase, this convention is used with the following rule group:\\n\\n(2.1) Rule Example\\n\\nSSES \\u2192 SS caresses \\u2192 caress\\n\\nIES \\u2192 I ponies \\u2192 poni\\n\\nSS \\u2192 SS caress \\u2192 caress\\n\\nS \\u2192 cats \\u2192 cat\\n\\nMany of the later rules use a concept of the measure of a word, which loosely checks the number of syllables to see whether a word is long enough that it is reasonable to regard the matching portion of a rule as a suffix rather than as part of the stem of a word. For example, the rule:\\n\\n(m > 1) EMENT \\u2192\\n\\nwould map replacement to replac, but not cement to c. The official site for the Porter Stemmer is:\\n\\nhttp://www.tartarus.org/\\u02dc martin/PorterStemmer/\\n\\nOther stemmers exist, including the older, one-pass Lovins stemmer (Lovins 1968), and newer entrants like the Paice/Husk stemmer (Paice 1990); see: http://www.cs.waikato.ac.nz/\\u02dc eibe/stemmers/ http://www.comp.lancs.ac.uk/computing/research/stemming/\\n\\nFigure 2.8 presents an informal comparison of the different behaviors of these stemmers. Stemmers use language-specific rules, but they require less knowledge than a lemmatizer, which needs a complete vocabulary and morphological analysis to correctly lemmatize words. Particular domains may also require special stemming rules. However, the exact stemmed form does not matter, only the equivalence classes it forms.\\n\\nRather than using a stemmer, you can use a lemmatizer, a tool from Nat-LEMMATIZER ural Language Processing which does full morphological analysis to accurately identify the lemma for each word. Doing full morphological analysis produces at most very modest benefits for retrieval. It is hard to say more,\\n\\nSample text: Such an analysis can reveal features that are not easily visible from the variations in the individual genes and can lead to a picture of expression that is more biologically transparent and accessible to interpretation\\n\\nLovins stemmer: such an analys can reve featur that ar not eas vis from th vari in th individu gen and can lead to a pictur of expres that is mor biolog transpar and acces to interpres\\n\\nPorter stemmer: such an analysi can reveal featur that ar not easili visibl from the variat in the individu gene and can lead to a pictur of express that is more biolog transpar and access to interpret\\n\\nPaice stemmer: such an analys can rev feat that are not easy vis from the vary in the individ gen and can lead to a pict of express that is mor biolog transp and access to interpret\\n\\n\\u25ee Figure 2.8 A comparison of three stemming algorithms on a sample text.\\n\\nbecause either form of normalization tends not to improve English information retrieval performance in aggregate - at least not by very much. While it helps a lot for some queries, it equally hurts performance a lot for others. Stemming increases recall while harming precision. As an example of what can go wrong, note that the Porter stemmer stems all of the following words:\\n\\noperate operating operates operation operative operatives operational\\n\\nto oper. However, since operate in its various forms is a common verb, we would expect to lose considerable precision on queries such as the following with Porter stemming:\\n\\noperational AND research operating AND system operative AND dentistry\\n\\nFor a case like this, moving to using a lemmatizer would not completely fix the problem because particular inflectional forms are used in particular collocations: a sentence with the words operate and system is not a good match for the query operating AND system. Getting better value from term normalization depends more on pragmatic issues of word use than on formal issues of linguistic morphology.\\n\\nThe situation is different for languages with much more morphology (such as Spanish, German, and Finnish). Results in the European CLEF evaluations have repeatedly shown quite large gains from the use of stemmers (and compound splitting for languages like German); see the references in Section 2.5.\\n\\n? Exercise 2.1 [\\u22c6]Are the following statements true or false?\\n\\na. In a Boolean retrieval system, stemming never lowers precision. b. In a Boolean retrieval system, stemming never lowers recall. c. Stemming increases the size of the vocabulary.\\n\\nd. Stemming should be invoked at indexing time but not while processing a query.\\n\\nExercise 2.2 [\\u22c6] Suggest what normalized form should be used for these words (including the word itself as a possibility):\\n\\na. 'Cos b. Shi'ite c. cont'd d. Hawai'i e. O'Rourke\\n\\nExercise 2.3 [\\u22c6] The following pairs of words are stemmed to the same form by the Porter stemmer. Which pairs would you argue shouldn't be conflated. Give your reasoning.\\n\\na. abandon/abandonment b. absorbency/absorbent c. marketing/markets d. university/universe e. volume/volumes\\n\\nExercise 2.4 [\\u22c6] For the Porter stemmer rule group shown in (2.1):\\n\\na. What is the purpose of including an identity rule such as SS\\u2192 SS?\\n\\nb. Applying just this rule group, what will the following words be stemmed to? circus canaries boss\\n\\nc. What rule should be added to correctly stem pony?\\n\\nd. The stemming for ponies and pony might seem strange. Does it have a deleterious effect on retrieval? Why or why not?\\n\\n\\u25ee Figure 2.9 Postings lists with skip pointers. The postings intersection can use a skip pointer when the end point is still less than the item on the other list.\\n\\n2.3 Faster postings list intersection via skip pointers\\n\\nIn the remainder of this chapter, we will discuss extensions to postings list data structures and ways to increase the efficiency of using postings lists. Recall the basic postings list intersection operation from Section 1.3 (page 10): we walk through the two postings lists simultaneously, in time linear in the total number of postings entries. If the list lengths are m and n, the intersection takes O(m + n) operations. Can we do better than this? That is, empirically, can we usually process postings list intersection in sublinear time? We can, if the index isn't changing too fast.\\n\\nOne way to do this is to use a skip list by augmenting postings lists withSKIP LIST skip pointers (at indexing time), as shown in Figure 2.9. Skip pointers are effectively shortcuts that allow us to avoid processing parts of the postings list that will not figure in the search results. The two questions are then where to place skip pointers and how to do efficient merging using skip pointers.\\n\\nConsider first efficient merging, with Figure 2.9 as an example. Suppose we've stepped through the lists in the figure until we have matched 8 on each list and moved it to the results list. We advance both pointers, giving us 16 on the upper list and 41 on the lower list. The smallest item is then the element 16 on the top list. Rather than simply advancing the upper pointer, we first check the skip list pointer and note that 28 is also less than 41. Hence we can follow the skip list pointer, and then we advance the upper pointer to 28 . We thus avoid stepping to 19 and 23 on the upper list. A number of variant versions of postings list intersection with skip pointers is possible depending on when exactly you check the skip pointer. One version is shown\\n\\nINTERSECTWITHSKIPS(p1, p2)\\n\\n1 answer \\u2190 \\u3008 \\u3009\\n\\n2 while p1 6= NIL and p2 6= NIL\\n\\n3 do if docID(p1) = docID(p2)\\n\\n4 then ADD(answer, docID(p1))\\n\\n5 p1 \\u2190 next(p1)\\n\\n6 p2 \\u2190 next(p2)\\n\\n7 else if docID(p1) < docID(p2)\\n\\n8 then if hasSkip(p1) and (docID(skip(p1)) \\u2264 docID(p2))\\n\\n9 then while hasSkip(p1) and (docID(skip(p1)) \\u2264 docID(p2))\\n\\n10 do p1 \\u2190 skip(p1)\\n\\n11 else p1 \\u2190 next(p1)\\n\\n12 else if hasSkip(p2) and (docID(skip(p2)) \\u2264 docID(p1))\\n\\n13 then while hasSkip(p2) and (docID(skip(p2)) \\u2264 docID(p1))\\n\\n14 do p2 \\u2190 skip(p2)\\n\\n15 else p2 \\u2190 next(p2) 16 return answer\\n\\n\\u25ee Figure 2.10 Postings lists intersection with skip pointers.\\n\\nin Figure 2.10. Skip pointers will only be available for the original postings lists. For an intermediate result in a complex query, the call hasSkip(p) will always return false. Finally, note that the presence of skip pointers only helps for AND queries, not for OR queries.\\n\\nWhere do we place skips? There is a tradeoff. More skips means shorter skip spans, and that we are more likely to skip. But it also means lots of comparisons to skip pointers, and lots of space storing skip pointers. Fewer skips means few pointer comparisons, but then long skip spans which means that there will be fewer opportunities to skip. A simple heuristic for placing skips, which has been found to work well in practice, is that for a postings list of length P, use\\n\\n\\u221a\\n\\nP evenly-spaced skip pointers. This heuristic can be improved upon; it ignores any details of the distribution of query terms.\\n\\nBuilding effective skip pointers is easy if an index is relatively static; it is harder if a postings list keeps changing because of updates. A malicious deletion strategy can render skip lists ineffective.\\n\\nChoosing the optimal encoding for an inverted index is an ever-changing game for the system builder, because it is strongly dependent on underlying computer technologies and their relative speeds and sizes. Traditionally, CPUs were slow, and so highly compressed techniques were not optimal. Now CPUs are fast and disk is slow, so reducing disk postings list size dominates. However, if you're running a search engine with everything in mem- ory then the equation changes again. We discuss the impact of hardware parameters on index construction time in Section 4.1 (page 68) and the impact of index size on system speed in Chapter 5.\\n\\n? Exercise 2.5 [\\u22c6]Why are skip pointers not useful for queries of the form x OR y?\\n\\nExercise 2.6 [\\u22c6]\\n\\nWe have a two-word query. For one term the postings list consists of the following 16 entries:\\n\\n[4,6,10,12,14,16,18,20,22,32,47,81,120,122,157,180]\\n\\nand for the other it is the one entry postings list:\\n\\n[47].\\n\\nWork out how many comparisons would be done to intersect the two postings lists with the following two strategies. Briefly justify your answers:\\n\\na. Using standard postings lists\\n\\nb. Using postings lists stored with skip pointers, with a skip length of\\n\\n\\u221a\\n\\nP, as sug-\\n\\ngested in Section 2.3.\\n\\nExercise 2.7 [\\u22c6]\\n\\nConsider a postings intersection between this postings list, with skip pointers:\\n\\n3 5 9 15 24 39 60 68 75 81 84 89 92 96 97 100 115\\n\\nand the following intermediate result postings list (which hence has no skip pointers):\\n\\n3 5 89 95 97 99 100 101\\n\\nTrace through the postings intersection algorithm in Figure 2.10 (page 37).\\n\\na. How often is a skip pointer followed (i.e., p1 is advanced to skip(p1))?\\n\\nb. How many postings comparisons will be made by this algorithm while intersecting the two lists?\\n\\nc. How many postings comparisons would be made if the postings lists are intersected without the use of skip pointers?\\n\\n2.4 Positional postings and phrase queries\\n\\nMany complex or technical concepts and many organization and product names are multiword compounds or phrases. We would like to be able to pose a query such as Stanford University by treating it as a phrase so that a sentence in a document like The inventor Stanford Ovshinsky never went to university. is not a match. Most recent search engines support a double quotes syntax (\\\"stanford university\\\") for phrase queries, which has proven to be veryPHRASE QUERIES\\n\\neasily understood and successfully used by users. As many as 10% of web queries are phrase queries, and many more are implicit phrase queries (such as person names), entered without use of double quotes. To be able to support such queries, it is no longer sufficient for postings lists to be simply lists of documents that contain individual terms. In this section we consider two approaches to supporting phrase queries and their combination. A search engine should not only support phrase queries, but implement them efficiently. A related but distinct concept is term proximity weighting, where a document is preferred to the extent that the query terms appear close to each other in the text. This technique is covered in Section 7.2.2 (page 144) in the context of ranked retrieval.\\n\\n2.4.1 Biword indexes\\n\\nOne approach to handling phrases is to consider every pair of consecutive terms in a document as a phrase. For example, the text Friends, Romans, Countrymen would generate the biwords:BIWORD INDEX\\n\\nfriends romans romans countrymen\\n\\nIn this model, we treat each of these biwords as a vocabulary term. Being able to process two-word phrase queries is immediate. Longer phrases can be processed by breaking them down. The query stanford university palo alto can be broken into the Boolean query on biwords:\\n\\n\\\"stanford university\\\" AND \\\"university palo\\\" AND \\\"palo alto\\\"\\n\\nThis query could be expected to work fairly well in practice, but there can and will be occasional false positives. Without examining the documents, we cannot verify that the documents matching the above Boolean query do actually contain the original 4 word phrase.\\n\\nAmong possible queries, nouns and noun phrases have a special status in describing the concepts people are interested in searching for. But related nouns can often be divided from each other by various function words, in phrases such as the abolition of slavery or renegotiation of the constitution. These needs can be incorporated into the biword indexing model in the following way. First, we tokenize the text and perform part-of-speech-tagging.6 We can then group terms into nouns, including proper nouns, (N) and function words, including articles and prepositions, (X), among other classes. Now deem any string of terms of the form NX*N to be an extended biword. Each such extended biword is made a term in the vocabulary. For example:\\n\\nrenegotiation of the constitution\\n\\nN X X N\\n\\nTo process a query using such an extended biword index, we need to also parse it into N's and X's, and then segment the query into extended biwords, which can be looked up in the index.\\n\\nThis algorithm does not always work in an intuitively optimal manner when parsing longer queries into Boolean queries. Using the above algorithm, the query\\n\\ncost overruns on a power plant\\n\\nis parsed into\\n\\n\\\"cost overruns\\\" AND \\\"overruns power\\\" AND \\\"power plant\\\"\\n\\nwhereas it might seem a better query to omit the middle biword. Better results can be obtained by using more precise part-of-speech patterns that define which extended biwords should be indexed.\\n\\nThe concept of a biword index can be extended to longer sequences of words, and if the index includes variable length word sequences, it is generally referred to as a phrase index. Indeed, searches for a single term arePHRASE INDEX\\n\\nnot naturally handled in a biword index (you would need to scan the dictionary for all biwords containing the term), and so we also need to have an index of single-word terms. While there is always a chance of false positive matches, the chance of a false positive match on indexed phrases of length 3 or more becomes very small indeed. But on the other hand, storing longer phrases has the potential to greatly expand the vocabulary size. Maintaining exhaustive phrase indexes for phrases of length greater than two is a daunting prospect, and even use of an exhaustive biword dictionary greatly expands the size of the vocabulary. However, towards the end of this section we discuss the utility of the strategy of using a partial phrase index in a compound indexing scheme.\\n\\n6. Part of speech taggers classify words as nouns, verbs, etc. - or, in practice, often as finer-grained classes like \\\"plural proper noun\\\". Many fairly accurate (c. 96% per-tag accuracy) part-of-speech taggers now exist, usually trained by machine learning methods on hand-tagged text. See, for instance, Manning and Sch\\u00fctze (1999, ch. 10).\\n\\nto, 993427:\\n\\n\\u3008 1, 6: \\u30087, 18, 33, 72, 86, 231\\u3009; 2, 5: \\u30081, 17, 74, 222, 255\\u3009; 4, 5: \\u30088, 16, 190, 429, 433\\u3009; 5, 2: \\u3008363, 367\\u3009;\\n\\n7, 3: \\u300813, 23, 191\\u3009; . . . \\u3009\\n\\nbe, 178239:\\n\\n\\u3008 1, 2: \\u300817, 25\\u3009;\\n\\n4, 5: \\u300817, 191, 291, 430, 434\\u3009; 5, 3: \\u300814, 19, 101\\u3009; . . . \\u3009\\n\\n\\u25ee Figure 2.11 Positional index example. The word to has a document frequency 993,477, and occurs 6 times in document 1 at positions 7, 18, 33, etc.\\n\\n2.4.2 Positional indexes\\n\\nFor the reasons given, a biword index is not the standard solution. Rather, a positional index is most commonly employed. Here, for each term in thePOSITIONAL INDEX\\n\\nvocabulary, we store postings of the form docID: \\u3008position1, position2, . . . \\u3009, as shown in Figure 2.11, where each position is a token index in the document. Each posting will also usually record the term frequency, for reasons discussed in Chapter 6.\\n\\nTo process a phrase query, you still need to access the inverted index entries for each distinct term. As before, you would start with the least frequent term and then work to further restrict the list of possible candidates. In the merge operation, the same general technique is used as before, but rather than simply checking that both terms are in a document, you also need to check that their positions of appearance in the document are compatible with the phrase query being evaluated. This requires working out offsets between the words.\\n\\n\\u270e Example 2.1: Satisfying phrase queries. Suppose the postings lists for to andbe are as in Figure 2.11, and the query is \\\"to be or not to be\\\". The postings lists to access are: to, be, or, not. We will examine intersecting the postings lists for to and be. We first look for documents that contain both terms. Then, we look for places in the lists where there is an occurrence of be with a token index one higher than a position of to, and then we look for another occurrence of each word with token index 4 higher than the first occurrence. In the above lists, the pattern of occurrences that is a possible match is:\\n\\nto: \\u3008. . . ; 4:\\u3008. . . ,429,433\\u3009; . . . \\u3009 be: \\u3008. . . ; 4:\\u3008. . . ,430,434\\u3009; . . . \\u3009\\n\\nPOSITIONALINTERSECT(p1, p2, k)\\n\\n1 answer \\u2190 \\u3008 \\u3009\\n\\n2 while p1 6= NIL and p2 6= NIL\\n\\n3 do if docID(p1) = docID(p2)\\n\\n4 then l \\u2190 \\u3008 \\u3009\\n\\n5 pp1 \\u2190 positions(p1)\\n\\n6 pp2 \\u2190 positions(p2)\\n\\n7 while pp1 6= NIL\\n\\n8 do while pp2 6= NIL\\n\\n9 do if |pos(pp1)\\u2212 pos(pp2)| \\u2264 k\\n\\n10 then ADD(l, pos(pp2))\\n\\n11 else if pos(pp2) > pos(pp1)\\n\\n12 then break\\n\\n13 pp2 \\u2190 next(pp2)\\n\\n14 while l 6= \\u3008 \\u3009 and |l[0]\\u2212 pos(pp1)| > k\\n\\n15 do DELETE(l[0])\\n\\n16 for each ps \\u2208 l\\n\\n17 do ADD(answer, \\u3008docID(p1), pos(pp1), ps\\u3009)\\n\\n18 pp1 \\u2190 next(pp1)\\n\\n19 p1 \\u2190 next(p1)\\n\\n20 p2 \\u2190 next(p2)\\n\\n21 else if docID(p1) < docID(p2)\\n\\n22 then p1 \\u2190 next(p1)\\n\\n23 else p2 \\u2190 next(p2) 24 return answer\\n\\n\\u25ee Figure 2.12 An algorithm for proximity intersection of postings lists p1 and p2. The algorithm finds places where the two terms appear within k words of each other\\n\\nand returns a list of triples giving docID and the term position in p1 and p2.\\n\\nThe same general method is applied for within k word proximity searches,\\n\\nof the sort we saw in Example 1.1 (page 15):\\n\\nemployment /3 place\\n\\nHere, /k means \\\"within k words of (on either side)\\\". Clearly, positional in-\\n\\ndexes can be used for such queries; biword indexes cannot. We show in Figure 2.12 an algorithm for satisfying within k word proximity searches; it\\n\\nis further discussed in Exercise 2.12.\\n\\nPositional index size. Adopting a positional index expands required post-\\n\\nings storage significantly, even if we compress position values/offsets as we will discuss in Section 5.3 (page 95). Indeed, moving to a positional index also changes the asymptotic complexity of a postings intersection operation, because the number of items to check is now bounded not by the number of documents but by the total number of tokens in the document collection T. That is, the complexity of a Boolean query is \\u0398(T) rather than \\u0398(N). However, most applications have little choice but to accept this, since most users now expect to have the functionality of phrase and proximity searches.\\n\\nLet's examine the space implications of having a positional index. A posting now needs an entry for each occurrence of a term. The index size thus depends on the average document size. The average web page has less than 1000 terms, but documents like SEC stock filings, books, and even some epic poems easily reach 100,000 terms. Consider a term with frequency 1 in 1000 terms on average. The result is that large documents cause an increase of two orders of magnitude in the space required to store the postings list:\\n\\nExpected Expected entries Document size postings in positional posting\\n\\n1000 1 1\\n\\n100,000 1 100\\n\\nWhile the exact numbers depend on the type of documents and the language being indexed, some rough rules of thumb are to expect a positional index to be 2 to 4 times as large as a non-positional index, and to expect a compressed positional index to be about one third to one half the size of the raw text (after removal of markup, etc.) of the original uncompressed documents. Specific numbers for an example collection are given in Table 5.1 (page 87) and Table 5.6 (page 103).\\n\\n2.4.3 Combination schemes\\n\\nThe strategies of biword indexes and positional indexes can be fruitfully combined. If users commonly query on particular phrases, such as Michael Jackson, it is quite inefficient to keep merging positional postings lists. A combination strategy uses a phrase index, or just a biword index, for certain queries and uses a positional index for other phrase queries. Good queries to include in the phrase index are ones known to be common based on recent querying behavior. But this is not the only criterion: the most expensive phrase queries to evaluate are ones where the individual words are common but the desired phrase is comparatively rare. Adding Britney Spears as a phrase index entry may only give a speedup factor to that query of about 3, since most documents that mention either word are valid results, whereas adding The Who as a phrase index entry may speed up that query by a factor of 1000. Hence, having the latter is more desirable, even if it is a relatively less common query.\\n\\nWilliams et al. (2004) evaluate an even more sophisticated scheme which employs indexes of both these sorts and additionally a partial next word index as a halfway house between the first two strategies. For each term, a next word index records terms that follow it in a document. They concludeNEXT WORD INDEX\\n\\nthat such a strategy allows a typical mixture of web phrase queries to be completed in one quarter of the time taken by use of a positional index alone, while taking up 26% more space than use of a positional index alone.\\n\\n? Exercise 2.8 [\\u22c6]Assume a biword index. Give an example of a document which will be returned for a query of New York University but is actually a false positive which should not be returned.\\n\\nExercise 2.9 [\\u22c6]\\n\\nShown below is a portion of a positional index in the format: term: doc1: \\u3008position1, position2, . . . \\u3009; doc2: \\u3008position1, position2, . . . \\u3009; etc.\\n\\nangels: 2: \\u300836,174,252,651\\u3009; 4: \\u300812,22,102,432\\u3009; 7: \\u300817\\u3009; fools: 2: \\u30081,17,74,222\\u3009; 4: \\u30088,78,108,458\\u3009; 7: \\u30083,13,23,193\\u3009; fear: 2: \\u300887,704,722,901\\u3009; 4: \\u300813,43,113,433\\u3009; 7: \\u300818,328,528\\u3009; in: 2: \\u30083,37,76,444,851\\u3009; 4: \\u300810,20,110,470,500\\u3009; 7: \\u30085,15,25,195\\u3009; rush: 2: \\u30082,66,194,321,702\\u3009; 4: \\u30089,69,149,429,569\\u3009; 7: \\u30084,14,404\\u3009; to: 2: \\u300847,86,234,999\\u3009; 4: \\u300814,24,774,944\\u3009; 7: \\u3008199,319,599,709\\u3009; tread: 2: \\u300857,94,333\\u3009; 4: \\u300815,35,155\\u3009; 7: \\u300820,320\\u3009; where: 2: \\u300867,124,393,1001\\u3009; 4: \\u300811,41,101,421,431\\u3009; 7: \\u300816,36,736\\u3009;\\n\\nWhich document(s) if any match each of the following queries, where each expression within quotes is a phrase query?\\n\\na. \\\"fools rush in\\\" b. \\\"fools rush in\\\" AND \\\"angels fear to tread\\\"\\n\\nExercise 2.10 [\\u22c6]\\n\\nConsider the following fragment of a positional index with the format:\\n\\nword: document: \\u3008position, position, . . .\\u3009; document: \\u3008position, . . .\\u3009 . . .\\n\\nGates: 1: \\u30083\\u3009; 2: \\u30086\\u3009; 3: \\u30082,17\\u3009; 4: \\u30081\\u3009; IBM: 4: \\u30083\\u3009; 7: \\u300814\\u3009;\\n\\nMicrosoft: 1: \\u30081\\u3009; 2: \\u30081,21\\u3009; 3: \\u30083\\u3009; 5: \\u300816,22,51\\u3009;\\n\\nThe /k operator, word1 /k word2 finds occurrences of word1 within k words of word2 (on either side), where k is a positive integer argument. Thus k = 1 demands that word1 be adjacent to word2.\\n\\na. Describe the set of documents that satisfy the query Gates /2 Microsoft.\\n\\nb. Describe each set of values for k for which the query Gates /k Microsoft returns a different set of documents as the answer.\\n\\nExercise 2.11 [\\u22c6\\u22c6]\\n\\nConsider the general procedure for merging two positional postings lists for a given document, to determine the document positions where a document satisfies a /k clause (in general there can be multiple positions at which each term occurs in a single document). We begin with a pointer to the position of occurrence of each term and move each pointer along the list of occurrences in the document, checking as we do so whether we have a hit for /k. Each move of either pointer counts as a step. Let L denote the total number of occurrences of the two terms in the document. What is the big-O complexity of the merge procedure, if we wish to have postings including positions in the result?\\n\\nExercise 2.12 [\\u22c6\\u22c6]\\n\\nConsider the adaptation of the basic algorithm for intersection of two postings lists (Figure 1.6, page 11) to the one in Figure 2.12 (page 42), which handles proximity\\n\\nqueries. A naive algorithm for this operation could be O(PLmax2), where P is the sum of the lengths of the postings lists (i.e., the sum of document frequencies) and Lmax is the maximum length of a document (in tokens).\\n\\na. Go through this algorithm carefully and explain how it works.\\n\\nb. What is the complexity of this algorithm? Justify your answer carefully.\\n\\nc. For certain queries and data distributions, would another algorithm be more efficient? What complexity does it have?\\n\\nExercise 2.13 [\\u22c6\\u22c6]\\n\\nSuppose we wish to use a postings intersection procedure to determine simply the list of documents that satisfy a /k clause, rather than returning the list of positions, as in Figure 2.12 (page 42). For simplicity, assume k \\u2265 2. Let L denote the total number of occurrences of the two terms in the document collection (i.e., the sum of their collection frequencies). Which of the following is true? Justify your answer.\\n\\na. The merge can be accomplished in a number of steps linear in L and independent of k, and we can ensure that each pointer moves only to the right.\\n\\nb. The merge can be accomplished in a number of steps linear in L and independent of k, but a pointer may be forced to move non-monotonically (i.e., to sometimes back up)\\n\\nc. The merge can require kL steps in some cases.\\n\\nExercise 2.14 [\\u22c6\\u22c6]\\n\\nHow could an IR system combine use of a positional index and use of stop words? What is the potential problem, and how could it be handled?\\n\\n2.5 References and further reading\\n\\nExhaustive discussion of the character-level processing of East Asian lan-EAST ASIAN LANGUAGES guages can be found in Lunde (1998). Character bigram indexes are perhaps the most standard approach to indexing Chinese, although some systems use word segmentation. Due to differences in the language and writing system, word segmentation is most usual for Japanese (Luk and Kwok 2002, Kishida et al. 2005). The structure of a character k-gram index over unsegmented text differs from that in Section 3.2.2 (page 54): there the k-gram dictionary points to postings lists of entries in the regular dictionary, whereas here it points directly to document postings lists. For further discussion of Chinese word segmentation, see Sproat et al. (1996), Sproat and Emerson (2003), Tseng et al. (2005), and Gao et al. (2005).\\n\\nLita et al. (2003) present a method for truecasing. Natural language processing work on computational morphology is presented in (Sproat 1992, Beesley and Karttunen 2003).\\n\\nLanguage identification was perhaps first explored in cryptography; for example, Konheim (1981) presents a character-level k-gram language identification algorithm. While other methods such as looking for particular distinctive function words and letter combinations have been used, with the advent of widespread digital text, many people have explored the character n-gram technique, and found it to be highly successful (Beesley 1998, Dunning 1994, Cavnar and Trenkle 1994). Written language identification is regarded as a fairly easy problem, while spoken language identification remains more difficult; see Hughes et al. (2006) for a recent survey.\\n\\nExperiments on and discussion of the positive and negative impact of stemming in English can be found in the following works: Salton (1989), Har-man (1991), Krovetz (1995), Hull (1996). Hollink et al. (2004) provide detailed results for the effectiveness of language-specific methods on 8 European languages. In terms of percent change in mean average precision (see page 159) over a baseline system, diacritic removal gains up to 23% (being especially helpful for Finnish, French, and Swedish). Stemming helped markedly for Finnish (30% improvement) and Spanish (10% improvement), but for most languages, including English, the gain from stemming was in the range 0- 5%, and results from a lemmatizer were poorer still. Compound splitting gained 25% for Swedish and 15% for German, but only 4% for Dutch. Rather than language-particular methods, indexing character k-grams (as we suggested for Chinese) could often give as good or better results: using within-word character 4-grams rather than words gave gains of 37% in Finnish, 27% in Swedish, and 20% in German, while even being slightly positive for other languages, such as Dutch, Spanish, and English. Tomlinson (2003) presents broadly similar results. Bar-Ilan and Gutman (2005) suggest that, at the time of their study (2003), the major commercial web search engines suffered from lacking decent language-particular processing; for example, a query on www.google.fr for l'\\u00e9lectricit\\u00e9 did not separate off the article l' but only matched pages with precisely this string of article+noun.\\n\\nThe classic presentation of skip pointers for IR can be found in Moffat andSKIP LIST Zobel (1996). Extended techniques are discussed in Boldi and Vigna (2005). The main paper in the algorithms literature is Pugh (1990), which uses multilevel skip pointers to give expected O(log P) list access (the same expected efficiency as using a tree data structure) with less implementational complexity. In practice, the effectiveness of using skip pointers depends on various system parameters. Moffat and Zobel (1996) report conjunctive queries running about five times faster with the use of skip pointers, but Bahle et al. (2002, p. 217) report that, with modern CPUs, using skip lists instead slows down search because it expands the size of the postings list (i.e., disk I/O dominates performance). In contrast, Strohman and Croft (2007) again show good performance gains from skipping, in a system architecture designed to optimize for the large memory spaces and multiple cores of recent CPUs.\\n\\nJohnson et al. (2006) report that 11.7% of all queries in two 2002 web query logs contained phrase queries, though Kammenhuber et al. (2006) report only 3% phrase queries for a different data set. Silverstein et al. (1999) note that many queries without explicit phrase operators are actually implicit phrase searches.\",\n",
      "      \"enriched_text\": {\n",
      "        \"entities\": [\n",
      "          {\n",
      "            \"count\": 6,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"United States\",\n",
      "            \"relevance\": 0.640611,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Region\",\n",
      "                \"AdministrativeDivision\",\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"FilmEditor\",\n",
      "                \"Country\"\n",
      "              ],\n",
      "              \"name\": \"United States\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/United_States\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 4,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Porter\",\n",
      "            \"relevance\": 0.562595,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 5,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.705635,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"neill\",\n",
      "            \"relevance\": 0.56041,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.371998,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Tokenization\",\n",
      "            \"relevance\": 0.52564,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 3,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Porter Stemmer\",\n",
      "            \"relevance\": 0.503697,\n",
      "            \"type\": \"GeographicFeature\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.463724,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Vannevar Bush\",\n",
      "            \"relevance\": 0.491967,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Academic\",\n",
      "                \"AwardWinner\",\n",
      "                \"CompanyFounder\",\n",
      "                \"ProjectParticipant\",\n",
      "                \"Scientist\"\n",
      "              ],\n",
      "              \"name\": \"Vannevar Bush\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Vannevar_Bush\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 3,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.568154,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Mr. O'Neill\",\n",
      "            \"relevance\": 0.481096,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 4,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.468691,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"hasSkip\",\n",
      "            \"relevance\": 0.46716,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 3,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"lemmatization\",\n",
      "            \"relevance\": 0.454941,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Algeria\",\n",
      "            \"relevance\": 0.446383,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"CityTown\",\n",
      "                \"Country\"\n",
      "              ],\n",
      "              \"name\": \"Algeria\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Algeria\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Stanford University\",\n",
      "            \"relevance\": 0.444987,\n",
      "            \"type\": \"Organization\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Location\",\n",
      "                \"Company\",\n",
      "                \"SportsAssociation\",\n",
      "                \"AcademicInstitution\",\n",
      "                \"ChivalricOrderMember\",\n",
      "                \"CollegeUniversity\",\n",
      "                \"University\"\n",
      "              ],\n",
      "              \"name\": \"Stanford University\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Stanford_University\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Romans\",\n",
      "            \"relevance\": 0.443322,\n",
      "            \"type\": \"Organization\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"MusicalGroup\",\n",
      "                \"MusicalArtist\"\n",
      "              ],\n",
      "              \"name\": \"Romans (group)\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Romans_(group)\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Hewlett-Packard\",\n",
      "            \"relevance\": 0.435258,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProcessorManufacturer\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"AcademicInstitution\"\n",
      "              ],\n",
      "              \"name\": \"Hewlett-Packard\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Hewlett-Packard\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"China\",\n",
      "            \"relevance\": 0.428722,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Region\",\n",
      "                \"AdministrativeDivision\",\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"Kingdom\",\n",
      "                \"FilmScreeningVenue\",\n",
      "                \"Country\"\n",
      "              ],\n",
      "              \"name\": \"China\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/China\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"York University\",\n",
      "            \"relevance\": 0.426925,\n",
      "            \"type\": \"Organization\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Location\",\n",
      "                \"AcademicInstitution\",\n",
      "                \"CollegeUniversity\",\n",
      "                \"University\"\n",
      "              ],\n",
      "              \"name\": \"York University\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/York_University\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.477247,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"M*A*S*H\",\n",
      "            \"relevance\": 0.42,\n",
      "            \"type\": \"Movie\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Hawai'i\",\n",
      "            \"relevance\": 0.416799,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"PoliticalDistrict\",\n",
      "                \"AdministrativeDivision\",\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"MilitaryPost\",\n",
      "                \"USState\",\n",
      "                \"StateOrCounty\"\n",
      "              ],\n",
      "              \"name\": \"Hawaii\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Hawaii\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Wangari Maathai\",\n",
      "            \"relevance\": 0.41636,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Politician\",\n",
      "                \"AwardWinner\",\n",
      "                \"FilmActor\"\n",
      "              ],\n",
      "              \"name\": \"Wangari Maathai\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Wangari_Maathai\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.498757,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"Chile\",\n",
      "            \"relevance\": 0.415613,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"Country\"\n",
      "              ],\n",
      "              \"name\": \"Chile\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Chile\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 3,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.353581,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Lovins\",\n",
      "            \"relevance\": 0.414398,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Porter stemmer\",\n",
      "            \"relevance\": 0.414356,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.420384,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"writer\",\n",
      "            \"relevance\": 0.411906,\n",
      "            \"type\": \"JobTitle\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"New York University\",\n",
      "            \"relevance\": 0.411757,\n",
      "            \"type\": \"Organization\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Location\",\n",
      "                \"Facility\",\n",
      "                \"CollegeUniversity\",\n",
      "                \"University\"\n",
      "              ],\n",
      "              \"name\": \"New York University\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/New_York_University\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Google\",\n",
      "            \"relevance\": 0.41001,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"AcademicInstitution\",\n",
      "                \"AwardPresentingOrganization\",\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProgrammingLanguageDeveloper\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"VentureFundedCompany\"\n",
      "              ],\n",
      "              \"name\": \"Google\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Google\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"China\",\n",
      "            \"relevance\": 0.409666,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Country\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Europe\",\n",
      "            \"relevance\": 0.406498,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"MusicalGroup\",\n",
      "                \"BroadcastArtist\",\n",
      "                \"FilmMusicContributor\",\n",
      "                \"Lyricist\",\n",
      "                \"MusicalArtist\",\n",
      "                \"RecordProducer\",\n",
      "                \"Continent\"\n",
      "              ],\n",
      "              \"name\": \"Europe\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Europe\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Stanford Ovshinsky\",\n",
      "            \"relevance\": 0.401939,\n",
      "            \"type\": \"Person\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"BoardMember\",\n",
      "                \"Inventor\"\n",
      "              ],\n",
      "              \"name\": \"Stanford R. Ovshinsky\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Stanford_R._Ovshinsky\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"San Francisco-Los Angeles\",\n",
      "            \"relevance\": 0.401525,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.505171,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"President\",\n",
      "            \"relevance\": 0.400282,\n",
      "            \"type\": \"JobTitle\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Paice\",\n",
      "            \"relevance\": 0.395214,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"guistics\",\n",
      "            \"relevance\": 0.392185,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.754733,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"London\",\n",
      "            \"relevance\": 0.391644,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"ural Language Processing\",\n",
      "            \"relevance\": 0.383794,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Fed\",\n",
      "            \"relevance\": 0.381123,\n",
      "            \"type\": \"Organization\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [],\n",
      "              \"name\": \"Federal Reserve System\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Federal_Reserve_System\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"General Motors\",\n",
      "            \"relevance\": 0.379753,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"AircraftManufacturer\",\n",
      "                \"AutomobileCompany\",\n",
      "                \"AwardWinner\"\n",
      "              ],\n",
      "              \"name\": \"General Motors\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/General_Motors\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.611057,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"pe\\u00f1a\",\n",
      "            \"relevance\": 0.379228,\n",
      "            \"type\": \"Person\"\n",
      "          }\n",
      "        ],\n",
      "        \"sentiment\": {\n",
      "          \"document\": {\n",
      "            \"score\": -0.0633304,\n",
      "            \"label\": \"negative\"\n",
      "          }\n",
      "        },\n",
      "        \"concepts\": [\n",
      "          {\n",
      "            \"text\": \"Natural language processing\",\n",
      "            \"relevance\": 0.952081,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Natural_language_processing\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Information retrieval\",\n",
      "            \"relevance\": 0.908409,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Information_retrieval\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Word\",\n",
      "            \"relevance\": 0.757336,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Word\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Stemming\",\n",
      "            \"relevance\": 0.703912,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Stemming\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Microsoft Word\",\n",
      "            \"relevance\": 0.633622,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Microsoft_Word\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Query expansion\",\n",
      "            \"relevance\": 0.512487,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Query_expansion\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Equivalence relation\",\n",
      "            \"relevance\": 0.446061,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Equivalence_relation\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Stop words\",\n",
      "            \"relevance\": 0.441953,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Stop_words\"\n",
      "          }\n",
      "        ],\n",
      "        \"categories\": [\n",
      "          {\n",
      "            \"score\": 0.521894,\n",
      "            \"label\": \"/technology and computing/software/databases\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.365124,\n",
      "            \"label\": \"/technology and computing/internet technology/web search\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.308276,\n",
      "            \"label\": \"/technology and computing/programming languages/c and c++\"\n",
      "          }\n",
      "        ],\n",
      "        \"relations\": []\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"e6a0187156022de42cec7015e1a998b3\",\n",
      "      \"result_metadata\": {\n",
      "        \"score\": 1\n",
      "      },\n",
      "      \"extracted_metadata\": {\n",
      "        \"publicationdate\": \"2009-04-01\",\n",
      "        \"sha1\": \"baf9f5ffab45580e698d981174390d08f62e0816\",\n",
      "        \"author\": \"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\",\n",
      "        \"filename\": \"03dict.pdf\",\n",
      "        \"file_type\": \"pdf\",\n",
      "        \"title\": \"Introduction to Information Retrieval\"\n",
      "      },\n",
      "      \"html\": \"<?xml version='1.0' encoding='UTF-8' standalone='yes'?><html>\\n<head>\\n    <meta content=\\\"text/html; charset=UTF-8\\\" http-equiv=\\\"Content-Type\\\"/><meta content=\\\"Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze\\\" name=\\\"author\\\"/><meta content=\\\"2009-04-01\\\" name=\\\"publicationdate\\\"/><meta content=\\\"18\\\" name=\\\"numPages\\\"/><title>Introduction to Information Retrieval</title></head>\\n<body><h1><p>3 </p></h1><h3><p>Dictionaries and tolerantretrieval\\n</p></h3><p>In Chapters 1 and 2 we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a <i>wildcard query</i>: a query such as *a*e*i*o*u*, which seeks doc-WILDCARD QUERY\\n</p><p>uments containing any term that includes all the five vowels in sequence. The * symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated.\\n</p><p>We then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3. Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection.\\n</p><p>Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase <i>standard inverted index </i>to mean the inverted index developed in Chapters 1 and 2, in which each vocabulary term has a postings list with the documents in the collection.\\n</p><p><b>3.1 Search structures for dictionaries\\n</b></p><p>Given an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the\\ncorresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as <i>keys</i>. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot - and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain.1 At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2. Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years' time.\\n</p><p>Search trees overcome many of these issues - for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the <i>binary tree</i>, in which each internal node has two children.BINARY TREE\\n</p><p>The search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is <i>O</i>(log <i>M</i>)) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained.\\n</p><p>To mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the <i>B-tree </i>- a search tree in which every internal nodeB-TREE\\n</p><p>has a number of children in the interval [<i>a</i>, <i>b</i>], where <i>a </i>and <i>b </i>are appropriate positive integers; Figure 3.2 shows an example with <i>a </i>= 2 and <i>b </i>= 4. Each branch under an internal node again represents a test for a range of char-\\n</p><p>1. So-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.\\n</p><p>\\u25ee <b>Figure 3.1 </b>A binary search tree. In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest.\\n</p><p>acter sequences, as in the binary tree example of Figure 3.1. A B-tree may be viewed as \\\"collapsing\\\" multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers <i>a </i>and <i>b </i>are determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees.\\n</p><p>It should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets.\\n</p><p><b>3.2 Wildcard queries\\n</b></p><p>Wildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which\\n</p><p>\\u25ee <b>Figure 3.2 </b>A B-tree. In this example every internal node has between 2 and 4 children.\\n</p><p>leads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart).\\n</p><p>A query such as mon* is known as a <i>trailing wildcard query</i>, because the *WILDCARD QUERY symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set <i>W </i>of terms in the dictionary with the prefix mon. Finally, we use |<i>W</i>| lookups on the standard inverted index to retrieve all documents containing any term in <i>W</i>.\\n</p><p>But what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider\\n<i>leading wildcard queries</i>, or queries of the form *mon. Consider a <i>reverse B-tree\\n</i>on the dictionary - one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written <i>backwards: </i>thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms <i>R </i>in the vocabulary with a given prefix.\\n</p><p>In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set\\n<i>W </i>of dictionary terms beginning with the prefix se, then the reverse B-tree to\\nenumerate the set <i>R </i>of terms ending with the suffix mon. Next, we take the intersection <i>W </i>\\u2229 <i>R </i>of these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single * symbol using two B-trees, the normal B-tree and a reverse B-tree.\\n</p><p><b>3.2.1 General wildcard queries\\n</b></p><p>We now study two techniques for handling general wildcard queries. Both techniques share a common strategy: express the given wildcard query <i>q</i><i>w </i>as a Boolean query <i>Q </i>on a specially constructed index, such that the answer to\\n<i>Q </i>is a superset of the set of vocabulary terms matching <i>q</i><i>w</i>. Then, we check each term in the answer to <i>Q </i>against <i>q</i><i>w</i>, discarding those vocabulary terms that do not match <i>q</i><i>w</i>. At this point we have the vocabulary terms matching\\n<i>q</i><i>w </i>and can resort to the standard inverted index.\\n</p><p><b>Permuterm indexes\\n</b></p><p>Our first special index for general wildcard queries is the <i>permuterm index</i>,PERMUTERM INDEX a form of inverted index. First, we introduce a special symbol $ into our character set, to mark the end of a term. Thus, the term hello is shown here as the augmented term hello$. Next, we construct a permuterm index, in which the various rotations of each term (augmented with $) all link to the original vocabulary term. Figure 3.3 gives an example of such a permuterm index entry for the term hello.\\n</p><p>We refer to the set of rotated terms in the permuterm index as the <i>per-muterm vocabulary</i>.\\n</p><p>How does this index help us with wildcard queries? Consider the wildcard query m*n. The key is to <i>rotate </i>such a wildcard query so that the * symbol appears at the end of the string - thus the rotated wildcard query becomes n$m*. Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron.\\n</p><p>Now that the permuterm index enables us to identify the original vocabulary terms matching a wildcard query, we look up these terms in the standard inverted index to retrieve matching documents. We can thus handle any wildcard query with a single * symbol. But what about a query such as fi*mo*er? In this case we first enumerate the terms in the dictionary that are in the permuterm index of er$fi*. Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo. In this example, the term fishmonger would survive this filtering but filibuster would not. We then\\n</p><p>\\u25ee <b>Figure 3.3 </b>A portion of a permuterm index.\\n</p><p>run the surviving terms through the standard inverted index for document retrieval. One disadvantage of the permuterm index is that its dictionary becomes quite large, including as it does all rotations of each term.\\n</p><p>Notice the close interplay between the B-tree and the permuterm index above. Indeed, it suggests that the structure should perhaps be viewed as a permuterm B-tree. However, we follow traditional terminology here in describing the permuterm index as distinct from the B-tree that allows us to select the rotations with a given prefix.\\n</p><p><b>3.2.2 </b><b><i>k</i></b><b>-gram indexes for wildcard queries\\n</b></p><p>Whereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase. We now present a second technique, known as the <i>k</i>-gram index, for processing wildcard queries. We will also use <i>k</i>-gram indexes in Section 3.3.4. A <i>k</i>-<i>gram </i>is a sequence of <i>k\\n</i>characters. Thus cas, ast and stl are all 3-grams occurring in the term castle. We use a special character $ to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$.\\n</p><p>In a <i>k-gram index</i>, the dictionary contains all <i>k</i>-grams that occur in any term<i>k</i>-GRAM INDEX in the vocabulary. Each postings list points from a <i>k</i>-gram to all vocabulary\\netr beetroot metric petrify retrieval- - - -\\n</p><p>\\u25ee <b>Figure 3.4 </b>Example of a postings list in a 3-gram index. Here the 3-gram etr is illustrated. Matching vocabulary terms are lexicographically ordered in the postings.\\n</p><p>terms containing that <i>k</i>-gram. For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval. An example is given in Figure 3.4. How does such an index help us with wildcard queries? Consider the wildcard query re*ve. We are seeking documents containing any term that begins with re and ends with ve. Accordingly, we run the Boolean query $re AND ve$. This is looked up in the 3-gram index and yields a list of matching terms such as relive, remove and retrieve. Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query.\\n</p><p>There is however a difficulty with the use of <i>k</i>-gram indexes, that demands one further step of processing. Consider using the 3-gram index described above for the query red*. Following the process described above, we first issue the Boolean query $re AND red to the 3-gram index. This leads to a match on terms such as retired, which contain the conjunction of the two 3-grams $re and red, yet do not match the original wildcard query red*.\\n</p><p>To cope with this, we introduce a <i>post-filtering </i>step, in which the terms enumerated by the Boolean query on the 3-gram index are checked individually against the original query red*. This is a simple string-matching operation and weeds out terms such as retired that do not match the original query. Terms that survive are then searched in the standard inverted index as usual. We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index. Search engines do allow the combination of wildcard queries using Boolean operators, for example, re*d AND fe*ri. What is the appropriate semantics for such a query? Since each wildcard query turns into a disjunction of single-term queries, the appropriate interpretation of this example is that we have a conjunction of disjunctions: we seek all documents that contain any term matching re*d <i>and </i>any term matching fe*ri.\\n</p><p>Even without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index. A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an \\\"Advanced Query\\\" interface) that most\\nusers never use. Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine.\\n</p><p><i>? </i><b>Exercise 3.1</b>In the permuterm index, each permuterm vocabulary term points to the original vocabulary term(s) from which it was derived. How many original vocabulary terms can there be in the postings list of a permuterm vocabulary term?\\n</p><p><b>Exercise 3.2\\n</b></p><p>Write down the entries in the permuterm index dictionary that are generated by the term mama.\\n</p><p><b>Exercise 3.3\\n</b></p><p>If you wanted to search for s*ng in a permuterm wildcard index, what key(s) would one do the lookup on?\\n</p><p><b>Exercise 3.4\\n</b></p><p>Refer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the postings are lexicographically ordered. Why is this ordering useful?\\n</p><p><b>Exercise 3.5\\n</b></p><p>Consider again the query fi*mo*er from Section 3.2.1. What Boolean query on a bigram index would be generated for this query? Can you think of a term that matches the permuterm query in Section 3.2.1, but does not satisfy this Boolean query?\\n</p><p><b>Exercise 3.6\\n</b></p><p>Give an example of a sentence that falsely matches the wildcard query mon*h if the search were to simply use a conjunction of bigrams.\\n</p><p><b>3.3 Spelling correction\\n</b></p><p>We next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears:\\n</p><p>britian spears, britney's spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on <i>edit distance </i>and the second based on <i>k-gram overlap</i>. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.\\n</p><p><b>3.3.1 Implementing spelling correction\\n</b></p><p>There are two basic principles underlying most spelling correction algorithms.\\n</p><p>1. Of various alternative correct spellings for a mis-spelled query, choose the \\\"nearest\\\" one. This demands that we have a notion of nearness or proximity between a pair of queries. We will develop these proximity measures in Section 3.3.3.\\n</p><p>2. When two correctly spelled queries are tied (or nearly tied), select the one that is more common. For instance, grunt and grant both seem equally plausible as corrections for grnt. Then, the algorithm should choose the more common of grunt and grant as the correction. The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction. A different notion of more common is employed in many search engines, especially on the web. The idea is to use the correction that is most common among queries typed in by other users. The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt.\\n</p><p>Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation. Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways:\\n</p><p>1. On the query carot always retrieve documents containing carot as well as any \\\"spell-corrected\\\" version of carot, including carrot and tarot.\\n</p><p>2. As in (1) above, but only when the query term carot is not in the dictionary.\\n</p><p>3. As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents).\\n</p><p>4. When the original query returns fewer than a preset number of documents, the search interface presents a <i>spelling suggestion </i>to the end user: this suggestion consists of the spell-corrected query term(s). Thus, the search engine might respond to the user: \\\"Did you mean carrot?\\\"\\n</p><p><b>3.3.2 Forms of spelling correction\\n</b></p><p>We focus on two specific forms of spelling correction that we refer to as\\n<i>isolated-term </i>correction and <i>context-sensitive </i>correction. In isolated-term correction, we attempt to correct a single query term at a time - even when we\\nhave a multiple-term query. The carot example demonstrates this type of correction. Such isolated-term correction would fail to detect, for instance, that the query flew form Heathrow contains a mis-spelling of the term from - because each term in the query is correctly spelled in isolation.\\n</p><p>We begin by examining two techniques for addressing isolated-term correction: edit distance, and <i>k</i>-gram overlap. We then proceed to context-sensitive correction.\\n</p><p><b>3.3.3 Edit distance\\n</b></p><p>Given two character strings <i>s</i>1 and <i>s</i>2, the <i>edit distance </i>between them is theEDIT DISTANCE minimum number of <i>edit operations </i>required to transform <i>s</i>1 into <i>s</i>2. Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string and (iii) replace a character of a string by another character; for these operations, edit distance is sometimes known as <i>Levenshtein distance</i>. For example, the edit distance be-LEVENSHTEIN\\n</p><p>DISTANCE tween cat and dog is 3. In fact, the notion of edit distance can be generalized to allowing different weights for different kinds of edit operations, for instance a higher weight may be placed on replacing the character s by the character p, than on replacing it by the character a (the latter being closer to s on the keyboard). Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity). However, the remainder of our treatment here will focus on the case in which all edit operations have the same weight.\\n</p><p>It is well-known how to compute the (weighted) edit distance between two strings in time <i>O</i>(|<i>s</i>1| \\u00d7 |<i>s</i>2|), where |<i>s</i><i>i</i>| denotes the length of a string <i>s</i><i>i</i>. The idea is to use the dynamic programming algorithm in Figure 3.5, where the characters in <i>s</i>1 and <i>s</i>2 are given in array form. The algorithm fills the (integer) entries in a matrix <i>m </i>whose two dimensions equal the lengths of the two strings whose edit distances is being computed; the (<i>i</i>, <i>j</i>) entry of the matrix will hold (after the algorithm is executed) the edit distance between the strings consisting of the first <i>i </i>characters of <i>s</i>1 and the first <i>j </i>characters of <i>s</i>2. The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5, where the three quantities whose minimum is taken correspond to substituting a character in <i>s</i>1, inserting a character in <i>s</i>1 and inserting a character in <i>s</i>2.\\n</p><p>Figure 3.6 shows an example Levenshtein distance computation of Figure 3.5. The typical cell [<i>i</i>, <i>j</i>] has four entries formatted as a 2\\u00d7 2 cell. The lower right entry in each cell is the min of the other three, corresponding to the main dynamic programming step in Figure 3.5. The other three entries are the three entries <i>m</i>[<i>i </i>\\u2212 1, <i>j </i>\\u2212 1] + 0 or 1 depending on whether <i>s</i>1[<i>i</i>] =\\n</p><p>EDITDISTANCE(<i>s</i>1, <i>s</i>2)\\n</p><p>1 <i>int m</i>[<i>i</i>, <i>j</i>] = 0\\n</p><p>2 <b>for </b><i>i</i>\\u2190 1 <b>to </b>|<i>s</i>1| 3 <b>do </b><i>m</i>[<i>i</i>, 0] = <i>i\\n</i></p><p>4 <b>for </b><i>j</i>\\u2190 1 <b>to </b>|<i>s</i>2| 5 <b>do </b><i>m</i>[0, <i>j</i>] = <i>j\\n</i></p><p>6 <b>for </b><i>i</i>\\u2190 1 <b>to </b>|<i>s</i>1|\\n</p><p>7 <b>do for </b><i>j</i>\\u2190 1 <b>to </b>|<i>s</i>2|\\n</p><p>8 <b>do </b><i>m</i>[<i>i</i>, <i>j</i>] = min{<i>m</i>[<i>i</i>\\u2212 1, <i>j</i>\\u2212 1] + if (<i>s</i>1[<i>i</i>] = <i>s</i>2[<i>j</i>]) then 0 else 1fi,\\n</p><p>9 <i>m</i>[<i>i</i>\\u2212 1, <i>j</i>] + 1,\\n</p><p>10 <i>m</i>[<i>i</i>, <i>j</i>\\u2212 1] + 1}\\n</p><p>11 <b>return </b><i>m</i>[|<i>s</i>1|, |<i>s</i>2|]\\n</p><p>\\u25ee <b>Figure 3.5 </b>Dynamic programming algorithm for computing the edit distance between strings <i>s</i>1 and <i>s</i>2.\\n</p><p>f a s t\\n</p><p><b>0 1 1 2 2 3 3 4 4\\n</b></p><p>c\\n</p><p><b>1 1\\n</b></p><p>1 2 2 1\\n</p><p><b>2 </b>3\\n<b>2 2\\n</b></p><p><b>3 </b>4\\n<b>3 3\\n</b></p><p><b>4 </b>5\\n<b>4 4\\n</b></p><p>a\\n</p><p><b>2 2\\n</b></p><p><b>2 2\\n</b>3 <b>2\\n</b></p><p>1 3 3 1\\n</p><p>3 4 2 2\\n</p><p>4 5\\n</p><p><b>3 3\\n</b></p><p>t\\n</p><p><b>3 3\\n</b></p><p><b>3 3\\n</b>4 <b>3\\n</b></p><p>3 <b>2\\n</b>4 <b>2\\n</b></p><p>2 3 3 2\\n</p><p>2 4 3 2\\n</p><p>s\\n</p><p><b>4 4\\n</b></p><p><b>4 4\\n</b>5 <b>4\\n</b></p><p>4 <b>3\\n</b>5 <b>3\\n</b></p><p><b>2 </b>3 4 <b>2\\n</b></p><p>3 3 3 3\\n</p><p>\\u25ee <b>Figure 3.6 </b>Example Levenshtein distance computation. The 2\\u00d7 2 cell in the [<i>i</i>, <i>j</i>] entry of the table shows the three numbers whose minimum yields the fourth. The cells in italics determine the edit distance in this example.\\n</p><p><i>s</i>2[<i>j</i>], <i>m</i>[<i>i</i>\\u2212 1, <i>j</i>] + 1 and <i>m</i>[<i>i</i>, <i>j</i>\\u2212 1] + 1. The cells with numbers in italics depict the path by which we determine the Levenshtein distance.\\n</p><p>The spelling correction problem however demands more than computing edit distance: given a set S of strings (corresponding to terms in the vocabulary) and a query string <i>q</i>, we seek the string(s) in <i>V </i>of least edit distance from <i>q</i>. We may view this as a decoding problem, in which the codewords (the strings in <i>V</i>) are prescribed in advance. The obvious way of doing this is to compute the edit distance from <i>q </i>to each string in <i>V</i>, before selecting the\\nstring(s) of minimum edit distance. This exhaustive search is inordinately expensive. Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s).\\n</p><p>The simplest such heuristic is to restrict the search to dictionary terms beginning with the same letter as the query string; the hope would be that spelling errors do not occur in the first character of the query. A more sophisticated variant of this heuristic is to use a version of the permuterm index, in which we omit the end-of-word symbol $. Consider the set of all rotations of the query string <i>q</i>. For each rotation <i>r </i>from this set, we traverse the B-tree into the permuterm index, thereby retrieving all dictionary terms that have a rotation beginning with <i>r</i>. For instance, if <i>q </i>is mase and we consider the rotation <i>r </i>= sema, we would retrieve dictionary terms such as semantic and semaphore that do not have a small edit distance to <i>q</i>. Unfortunately, we would miss more pertinent dictionary terms such as mare and mane. To address this, we refine this rotation scheme: for each rotation, we omit a suffix of \\u2113 characters before performing the B-tree traversal. This ensures that each term in the set <i>R </i>of terms retrieved from the dictionary includes a \\\"long\\\" substring in common with <i>q</i>. The value of \\u2113 could depend on the length of <i>q</i>. Alternatively, we may set it to a fixed constant such as 2.\\n</p><p><b>3.3.4 </b><b><i>k</i></b><b>-gram indexes for spelling correction\\n</b></p><p>To further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the <i>k</i>-gram index of Section 3.2.2 (page 54) to assist with retrieving vocabulary terms with low edit distance to the query <i>q</i>. Once we retrieve such terms, we can then find the ones of least edit distance from <i>q</i>.\\n</p><p>In fact, we will use the <i>k</i>-gram index to retrieve vocabulary terms that have many <i>k</i>-grams in common with the query. We will argue that for reasonable definitions of \\\"many <i>k</i>-grams in common,\\\" the retrieval process is essentially that of a single scan through the postings for the <i>k</i>-grams in the query string <i>q</i>.\\n</p><p>The 2-gram (or <i>bigram</i>) index in Figure 3.7 shows (a portion of) the postings for the three bigrams in the query bord. Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams. A single scan of the postings (much as in Chapter 1) would let us enumerate all such terms; in the example of Figure 3.7 we would enumerate aboard, boardroom and border.\\n</p><p>This straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of <i>k</i>-grams from the query <i>q</i>: terms like boardroom, an implausible \\\"correction\\\" of bord, get enumerated. Conse-\\nrd aboard ardent boardroom border\\n</p><p>or border lord morbid sordid\\n</p><p>bo aboard about boardroom border\\n</p><p>- - - -\\n</p><p>- - - -\\n</p><p>- - - -\\n</p><p>\\u25ee <b>Figure 3.7 </b>Matching at least two of the three 2-grams in the query bord.\\n</p><p>quently, we require more nuanced measures of the overlap in <i>k</i>-grams between a vocabulary term and <i>q</i>. The linear scan intersection can be adapted when the measure of overlap is the <i>Jaccard coefficient </i>for measuring the over-JACCARD COEFFICIENT\\n</p><p>lap between two sets <i>A </i>and <i>B</i>, defined to be |<i>A</i>\\u2229 <i>B</i>|/|<i>A</i>\\u222a <i>B</i>|. The two sets we consider are the set of <i>k</i>-grams in the query <i>q</i>, and the set of <i>k</i>-grams in a vocabulary term. As the scan proceeds, we proceed from one vocabulary term\\n<i>t </i>to the next, computing on the fly the Jaccard coefficient between <i>q </i>and <i>t</i>. If the coefficient exceeds a preset threshold, we add <i>t </i>to the output; if not, we move on to the next term in the postings. To compute the Jaccard coefficient, we need the set of <i>k</i>-grams in <i>q </i>and <i>t</i>.\\n</p><p>Since we are scanning the postings for all <i>k</i>-grams in <i>q</i>, we immediately have these <i>k</i>-grams on hand. What about the <i>k</i>-grams of <i>t</i>? In principle, we could enumerate these on the fly from <i>t</i>; in practice this is not only slow but potentially infeasible since, in all likelihood, the postings entries themselves do not contain the complete string <i>t </i>but rather some encoding of <i>t</i>. The crucial observation is that to compute the Jaccard coefficient, we only need the length of the string <i>t</i>. To see this, recall the example of Figure 3.7 and consider the point when the postings scan for query <i>q </i>= bord reaches term\\n<i>t </i>= boardroom. We know that two bigrams match. If the postings stored the (pre-computed) number of bigrams in boardroom (namely, 8), we have all the information we require to compute the Jaccard coefficient to be 2/(8 + 3\\u2212 2); the numerator is obtained from the number of postings hits (2, from bo and rd) while the denominator is the sum of the number of bigrams in bord and boardroom, less the number of postings hits.\\n</p><p>We could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans. How do we use these\\nfor spelling correction? One method that has some empirical support is to first use the <i>k</i>-gram index to enumerate a set of candidate vocabulary terms that are potential corrections of <i>q</i>. We then compute the edit distance from <i>q\\n</i>to each term in this set, selecting terms from the set with small edit distance to <i>q</i>.\\n</p><p><b>3.3.5 Context sensitive spelling correction\\n</b></p><p>Isolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled. When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow. The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4) even though each query term is correctly spelled, then try substitutions of each correction in the phrase. For the example flew form Heathrow, we enumerate such phrases as fled form Heathrow and flew fore Heathrow. For each such substitute phrase, the search engine runs the query and determines the number of matching results.\\n</p><p>This enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives. Several heuristics are used to trim this space. In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users. For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form. In this example, the biword fled fore is likely to be rare compared to the biword flew from. Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow. As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors.\\n</p><p><i>? </i><b>Exercise 3.7</b>If |<i>s</i><i>i</i>| denotes the length of string <i>s</i><i>i</i>, show that the edit distance between <i>s</i>1 and <i>s</i>2 is never more than max{|<i>s</i>1|, |<i>s</i>2|}.\\n</p><p><b>Exercise 3.8\\n</b></p><p>Compute the edit distance between paris and alice. Write down the 5 \\u00d7 5 array of distances between all prefixes as computed by the algorithm in Figure 3.5.\\n</p><p><b>Exercise 3.9\\n</b></p><p>Write pseudocode showing the details of computing on the fly the Jaccard coefficient while scanning the postings of the <i>k</i>-gram index, as mentioned on page 61.\\n</p><p><b>Exercise 3.10\\n</b></p><p>Compute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or.\\n</p><p><b>Exercise 3.11\\n</b></p><p>Consider the four-term query catched in the rye and suppose that each of the query terms has five alternative terms suggested by isolated-term correction. How many possible corrected phrases must we consider if we do not trim the space of corrected phrases, but instead try all six variants for each of the terms?\\n</p><p><b>Exercise 3.12\\n</b></p><p>For each of the prefixes of the query - catched, catched in and catched in the - we have a number of substitute prefixes arising from each term and its alternatives. Suppose that we were to retain only the top 10 of these substitute prefixes, as measured by its number of occurrences in the collection. We eliminate the rest from consideration for extension to longer prefixes: thus, if batched in is not one of the 10 most common 2-term queries in the collection, we do not consider any extension of batched in as possibly leading to a correction of catched in the rye. How many of the possible substitute prefixes are we eliminating at each phase?\\n</p><p><b>Exercise 3.13\\n</b></p><p>Are we guaranteed that retaining and extending only the 10 commonest substitute prefixes of catched in will lead to one of the 10 commonest substitute prefixes of catched in the?\\n</p><p><b>3.4 Phonetic correction\\n</b></p><p>Our final technique for tolerant retrieval has to do with <i>phonetic </i>correction: misspellings that arise because the user types a query that sounds like the target term. Such algorithms are especially applicable to searches on the names of people. The main idea here is to generate, for each term, a \\\"phonetic hash\\\" so that similar-sounding terms hash to the same value. The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries. It is mainly used to correct phonetic misspellings in proper nouns.\\n</p><p>Algorithms for such phonetic hashing are commonly collectively known as\\n<i>soundex </i>algorithms. However, there is an original soundex algorithm, withSOUNDEX\\n</p><p>various variants, built on the following scheme:\\n</p><p>1. Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index.\\n</p><p>2. Do the same with query terms.\\n</p><p>3. When the query calls for a soundex match, search this soundex index.\\n</p><p>The variations in different soundex algorithms have to do with the conversion of terms to 4-character forms. A commonly used conversion results in a 4-character code, with the first character being a letter of the alphabet and the other three being digits between 0 and 9.\\n</p><p>1. Retain the first letter of the term.\\n</p><p>2. Change all occurrences of the following letters to '0' (zero): 'A', E', 'I', 'O', 'U', 'H', 'W', 'Y'.\\n</p><p>3. Change letters to digits as follows:\\n</p><p>B, F, P, V to 1.\\n</p><p>C, G, J, K, Q, S, X, Z to 2.\\n</p><p>D,T to 3.\\n</p><p>L to 4.\\n</p><p>M, N to 5.\\n</p><p>R to 6.\\n</p><p>4. Repeatedly remove one out of each pair of consecutive identical digits.\\n</p><p>5. Remove all zeros from the resulting string. Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits.\\n</p><p>For an example of a soundex map, Hermann maps to H655. Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index.\\n</p><p>This algorithm rests on a few observations: (1) vowels are viewed as interchangeable, in transcribing names; (2) consonants with similar sounds (e.g., D and T) are put in equivalence classes. This leads to related names often having the same soundex codes. While these rules work for many cases, especially European languages, such rules tend to be writing system dependent. For example, Chinese names can be written in Wade-Giles or Pinyin transcription. While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently.\\n</p><p><i>? </i><b>Exercise 3.14</b>Find two differently spelled proper nouns whose soundex codes are the same.\\n</p><p><b>Exercise 3.15\\n</b></p><p>Find two phonetically similar proper nouns whose soundex codes are different.\\n</p><p><b>3.5 References and further reading\\n</b></p><p>Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries.\\n</p><p>Garfield (1976) gives one of the first complete descriptions of the permuterm index. Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes.\\n</p><p>One of the earliest formal treatments of spelling correction was due to Damerau (1964). The notion of edit distance that we have used is due to Lev-enshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974). Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that <i>k</i>-gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings. Gusfield (1997) is a standard reference on string algorithms such as edit distance.\\n</p><p>Probabilistic models (\\\"noisy channel\\\" models) for spelling correction were pioneered by Kernighan et al. (1990) and further developed by Brill and Moore (2000) and Toutanova and Moore (2002). In these models, the misspelled query is viewed as a probabilistic corruption of a correct query. They have a similar mathematical basis to the language model methods presented in Chapter 12, and also provide ways of incorporating phonetic similarity, closeness on the keyboard, and data from the actual spelling mistakes of users. Many would regard them as the state-of-the-art approach. Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs.\\n</p><p>The soundex algorithm is attributed to Margaret K. Odell and Robert C. Russelli (from U.S. patents granted in 1918 and 1922); the version described here draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate various phonetic matching algorithms, finding that a variant of the soundex algorithm performs poorly for general spelling correction, but that other algorithms based on the phonetic similarity of term pronunciations perform well.\\n</p></body></html>\",\n",
      "      \"text\": \"Introduction to Information Retrieval\\n\\n3\\n\\nDictionaries and tolerantretrieval\\n\\nIn Chapters 1 and 2 we developed the ideas underlying inverted indexes for handling Boolean and proximity queries. Here, we develop techniques that are robust to typographical errors in the query, as well as alternative spellings. In Section 3.1 we develop data structures that help the search for terms in the vocabulary in an inverted index. In Section 3.2 we study the idea of a wildcard query: a query such as *a*e*i*o*u*, which seeks doc-WILDCARD QUERY\\n\\numents containing any term that includes all the five vowels in sequence. The * symbol indicates any (possibly empty) string of characters. Users pose such queries to a search engine when they are uncertain about how to spell a query term, or seek documents containing variants of a query term; for instance, the query automat* would seek documents containing any of the terms automatic, automation and automated.\\n\\nWe then turn to other forms of imprecisely posed queries, focusing on spelling errors in Section 3.3. Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection. We detail a number of techniques for correcting spelling errors in queries, one term at a time as well as for an entire string of query terms. Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s). This can be especially useful in cases like the Herman example, where the user may not know how a proper name is spelled in documents in the collection.\\n\\nBecause we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1 and 2, in which each vocabulary term has a postings list with the documents in the collection.\\n\\n3.1 Search structures for dictionaries\\n\\nGiven an inverted index and a query, our first task is to determine whether each query term exists in the vocabulary and if so, identify the pointer to the corresponding postings. This vocabulary lookup operation uses a classical data structure called the dictionary and has two broad classes of solutions: hashing, and search trees. In the literature of data structures, the entries in the vocabulary (in our case, terms) are often referred to as keys. The choice of solution (hashing, or search trees) is governed by a number of questions: (1) How many keys are we likely to have? (2) Is the number likely to remain static, or change a lot - and in the case of changes, are we likely to only have new keys inserted, or to also have some keys in the dictionary be deleted? (3) What are the relative frequencies with which various keys will be accessed? Hashing has been used for dictionary lookup in some search engines. Each vocabulary term (key) is hashed into an integer over a large enough space that hash collisions are unlikely; collisions if any are resolved by auxiliary structures that can demand care to maintain.1 At query time, we hash each query term separately and following a pointer to the corresponding postings, taking into account any logic for resolving hash collisions. There is no easy way to find minor variants of a query term (such as the accented and non-accented versions of a word like resume), since these could be hashed to very different integers. In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2. Finally, in a setting (such as the Web) where the size of the vocabulary keeps growing, a hash function designed for current needs may not suffice in a few years' time.\\n\\nSearch trees overcome many of these issues - for instance, they permit us to enumerate all vocabulary terms beginning with automat. The best-known search tree is the binary tree, in which each internal node has two children.BINARY TREE\\n\\nThe search for a term begins at the root of the tree. Each internal node (including the root) represents a binary test, based on whose outcome the search proceeds to one of the two sub-trees below that node. Figure 3.1 gives an example of a binary search tree used for a dictionary. Efficient search (with a number of comparisons that is O(log M)) hinges on the tree being balanced: the numbers of terms under the two sub-trees of any node are either equal or differ by one. The principal issue here is that of rebalancing: as terms are inserted into or deleted from the binary search tree, it needs to be rebalanced so that the balance property is maintained.\\n\\nTo mitigate rebalancing, one approach is to allow the number of sub-trees under an internal node to vary in a fixed interval. A search tree commonly used for a dictionary is the B-tree - a search tree in which every internal nodeB-TREE\\n\\nhas a number of children in the interval [a, b], where a and b are appropriate positive integers; Figure 3.2 shows an example with a = 2 and b = 4. Each branch under an internal node again represents a test for a range of char-\\n\\n1. So-called perfect hash functions are designed to preclude collisions, but are rather more complicated both to implement and to compute.\\n\\n\\u25ee Figure 3.1 A binary search tree. In this example the branch at the root partitions vocabulary terms into two subtrees, those whose first letter is between a and m, and the rest.\\n\\nacter sequences, as in the binary tree example of Figure 3.1. A B-tree may be viewed as \\\"collapsing\\\" multiple levels of the binary tree into one; this is especially advantageous when some of the dictionary is disk-resident, in which case this collapsing serves the function of pre-fetching imminent binary tests. In such cases, the integers a and b are determined by the sizes of disk blocks. Section 3.5 contains pointers to further background on search trees and B-trees.\\n\\nIt should be noted that unlike hashing, search trees demand that the characters used in the document collection have a prescribed ordering; for instance, the 26 letters of the English alphabet are always listed in the specific order A through Z. Some Asian languages such as Chinese do not always have a unique ordering, although by now all languages (including Chinese and Japanese) have adopted a standard ordering system for their character sets.\\n\\n3.2 Wildcard queries\\n\\nWildcard queries are used in any of the following situations: (1) the user is uncertain of the spelling of a query term (e.g., Sydney vs. Sidney, which\\n\\n\\u25ee Figure 3.2 A B-tree. In this example every internal node has between 2 and 4 children.\\n\\nleads to the wildcard query S*dney); (2) the user is aware of multiple variants of spelling a term and (consciously) seeks documents containing any of the variants (e.g., color vs. colour); (3) the user seeks documents containing variants of a term that would be caught by stemming, but is unsure whether the search engine performs stemming (e.g., judicial vs. judiciary, leading to the wildcard query judicia*); (4) the user is uncertain of the correct rendition of a foreign word or phrase (e.g., the query Universit* Stuttgart).\\n\\nA query such as mon* is known as a trailing wildcard query, because the *WILDCARD QUERY symbol occurs only once, at the end of the search string. A search tree on the dictionary is a convenient way of handling trailing wildcard queries: we walk down the tree following the symbols m, o and n in turn, at which point we can enumerate the set W of terms in the dictionary with the prefix mon. Finally, we use |W| lookups on the standard inverted index to retrieve all documents containing any term in W.\\n\\nBut what about wildcard queries in which the * symbol is not constrained to be at the end of the search string? Before handling this general case, we mention a slight generalization of trailing wildcard queries. First, consider leading wildcard queries, or queries of the form *mon. Consider a reverse B-tree on the dictionary - one in which each root-to-leaf path of the B-tree corresponds to a term in the dictionary written backwards: thus, the term lemon would, in the B-tree, be represented by the path root-n-o-m-e-l. A walk down the reverse B-tree then enumerates all terms R in the vocabulary with a given prefix.\\n\\nIn fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon. To do this, we use the regular B-tree to enumerate the set W of dictionary terms beginning with the prefix se, then the reverse B-tree to enumerate the set R of terms ending with the suffix mon. Next, we take the intersection W \\u2229 R of these two sets, to arrive at the set of terms that begin with the prefix se and end with the suffix mon. Finally, we use the standard inverted index to retrieve all documents containing any terms in this intersection. We can thus handle wildcard queries that contain a single * symbol using two B-trees, the normal B-tree and a reverse B-tree.\\n\\n3.2.1 General wildcard queries\\n\\nWe now study two techniques for handling general wildcard queries. Both techniques share a common strategy: express the given wildcard query qw as a Boolean query Q on a specially constructed index, such that the answer to Q is a superset of the set of vocabulary terms matching qw. Then, we check each term in the answer to Q against qw, discarding those vocabulary terms that do not match qw. At this point we have the vocabulary terms matching qw and can resort to the standard inverted index.\\n\\nPermuterm indexes\\n\\nOur first special index for general wildcard queries is the permuterm index,PERMUTERM INDEX a form of inverted index. First, we introduce a special symbol $ into our character set, to mark the end of a term. Thus, the term hello is shown here as the augmented term hello$. Next, we construct a permuterm index, in which the various rotations of each term (augmented with $) all link to the original vocabulary term. Figure 3.3 gives an example of such a permuterm index entry for the term hello.\\n\\nWe refer to the set of rotated terms in the permuterm index as the per-muterm vocabulary.\\n\\nHow does this index help us with wildcard queries? Consider the wildcard query m*n. The key is to rotate such a wildcard query so that the * symbol appears at the end of the string - thus the rotated wildcard query becomes n$m*. Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron.\\n\\nNow that the permuterm index enables us to identify the original vocabulary terms matching a wildcard query, we look up these terms in the standard inverted index to retrieve matching documents. We can thus handle any wildcard query with a single * symbol. But what about a query such as fi*mo*er? In this case we first enumerate the terms in the dictionary that are in the permuterm index of er$fi*. Not all such dictionary terms will have the string mo in the middle - we filter these out by exhaustive enumeration, checking each candidate to see if it contains mo. In this example, the term fishmonger would survive this filtering but filibuster would not. We then\\n\\n\\u25ee Figure 3.3 A portion of a permuterm index.\\n\\nrun the surviving terms through the standard inverted index for document retrieval. One disadvantage of the permuterm index is that its dictionary becomes quite large, including as it does all rotations of each term.\\n\\nNotice the close interplay between the B-tree and the permuterm index above. Indeed, it suggests that the structure should perhaps be viewed as a permuterm B-tree. However, we follow traditional terminology here in describing the permuterm index as distinct from the B-tree that allows us to select the rotations with a given prefix.\\n\\n3.2.2 k-gram indexes for wildcard queries\\n\\nWhereas the permuterm index is simple, it can lead to a considerable blowup from the number of rotations per term; for a dictionary of English terms, this can represent an almost ten-fold space increase. We now present a second technique, known as the k-gram index, for processing wildcard queries. We will also use k-gram indexes in Section 3.3.4. A k-gram is a sequence of k characters. Thus cas, ast and stl are all 3-grams occurring in the term castle. We use a special character $ to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$.\\n\\nIn a k-gram index, the dictionary contains all k-grams that occur in any termk-GRAM INDEX in the vocabulary. Each postings list points from a k-gram to all vocabulary etr beetroot metric petrify retrieval- - - -\\n\\n\\u25ee Figure 3.4 Example of a postings list in a 3-gram index. Here the 3-gram etr is illustrated. Matching vocabulary terms are lexicographically ordered in the postings.\\n\\nterms containing that k-gram. For instance, the 3-gram etr would point to vocabulary terms such as metric and retrieval. An example is given in Figure 3.4. How does such an index help us with wildcard queries? Consider the wildcard query re*ve. We are seeking documents containing any term that begins with re and ends with ve. Accordingly, we run the Boolean query $re AND ve$. This is looked up in the 3-gram index and yields a list of matching terms such as relive, remove and retrieve. Each of these matching terms is then looked up in the standard inverted index to yield documents matching the query.\\n\\nThere is however a difficulty with the use of k-gram indexes, that demands one further step of processing. Consider using the 3-gram index described above for the query red*. Following the process described above, we first issue the Boolean query $re AND red to the 3-gram index. This leads to a match on terms such as retired, which contain the conjunction of the two 3-grams $re and red, yet do not match the original wildcard query red*.\\n\\nTo cope with this, we introduce a post-filtering step, in which the terms enumerated by the Boolean query on the 3-gram index are checked individually against the original query red*. This is a simple string-matching operation and weeds out terms such as retired that do not match the original query. Terms that survive are then searched in the standard inverted index as usual. We have seen that a wildcard query can result in multiple terms being enumerated, each of which becomes a single-term query on the standard inverted index. Search engines do allow the combination of wildcard queries using Boolean operators, for example, re*d AND fe*ri. What is the appropriate semantics for such a query? Since each wildcard query turns into a disjunction of single-term queries, the appropriate interpretation of this example is that we have a conjunction of disjunctions: we seek all documents that contain any term matching re*d and any term matching fe*ri.\\n\\nEven without Boolean combinations of wildcard queries, the processing of a wildcard query can be quite expensive, because of the added lookup in the special index, filtering and finally the standard inverted index. A search engine may support such rich functionality, but most commonly, the capability is hidden behind an interface (say an \\\"Advanced Query\\\" interface) that most users never use. Exposing such functionality in the search interface often encourages users to invoke it even when they do not require it (say, by typing a prefix of their query followed by a *), increasing the processing load on the search engine.\\n\\n? Exercise 3.1In the permuterm index, each permuterm vocabulary term points to the original vocabulary term(s) from which it was derived. How many original vocabulary terms can there be in the postings list of a permuterm vocabulary term?\\n\\nExercise 3.2\\n\\nWrite down the entries in the permuterm index dictionary that are generated by the term mama.\\n\\nExercise 3.3\\n\\nIf you wanted to search for s*ng in a permuterm wildcard index, what key(s) would one do the lookup on?\\n\\nExercise 3.4\\n\\nRefer to Figure 3.4; it is pointed out in the caption that the vocabulary terms in the postings are lexicographically ordered. Why is this ordering useful?\\n\\nExercise 3.5\\n\\nConsider again the query fi*mo*er from Section 3.2.1. What Boolean query on a bigram index would be generated for this query? Can you think of a term that matches the permuterm query in Section 3.2.1, but does not satisfy this Boolean query?\\n\\nExercise 3.6\\n\\nGive an example of a sentence that falsely matches the wildcard query mon*h if the search were to simply use a conjunction of bigrams.\\n\\n3.3 Spelling correction\\n\\nWe next look at the problem of correcting spelling errors in queries. For instance, we may wish to retrieve documents containing the term carrot when the user types the query carot. Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears:\\n\\nbritian spears, britney's spears, brandy spears and prittany spears. We look at two steps to solving this problem: the first based on edit distance and the second based on k-gram overlap. Before getting into the algorithmic details of these methods, we first review how search engines provide spell-correction as part of a user experience.\\n\\n3.3.1 Implementing spelling correction\\n\\nThere are two basic principles underlying most spelling correction algorithms.\\n\\n1. Of various alternative correct spellings for a mis-spelled query, choose the \\\"nearest\\\" one. This demands that we have a notion of nearness or proximity between a pair of queries. We will develop these proximity measures in Section 3.3.3.\\n\\n2. When two correctly spelled queries are tied (or nearly tied), select the one that is more common. For instance, grunt and grant both seem equally plausible as corrections for grnt. Then, the algorithm should choose the more common of grunt and grant as the correction. The simplest notion of more common is to consider the number of occurrences of the term in the collection; thus if grunt occurs more often than grant, it would be the chosen correction. A different notion of more common is employed in many search engines, especially on the web. The idea is to use the correction that is most common among queries typed in by other users. The idea here is that if grunt is typed as a query more often than grant, then it is more likely that the user who typed grnt intended to type the query grunt.\\n\\nBeginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation. Spelling correction algorithms build on these computations of proximity; their functionality is then exposed to users in one of several ways:\\n\\n1. On the query carot always retrieve documents containing carot as well as any \\\"spell-corrected\\\" version of carot, including carrot and tarot.\\n\\n2. As in (1) above, but only when the query term carot is not in the dictionary.\\n\\n3. As in (1) above, but only when the original query returned fewer than a preset number of documents (say fewer than five documents).\\n\\n4. When the original query returns fewer than a preset number of documents, the search interface presents a spelling suggestion to the end user: this suggestion consists of the spell-corrected query term(s). Thus, the search engine might respond to the user: \\\"Did you mean carrot?\\\"\\n\\n3.3.2 Forms of spelling correction\\n\\nWe focus on two specific forms of spelling correction that we refer to as isolated-term correction and context-sensitive correction. In isolated-term correction, we attempt to correct a single query term at a time - even when we have a multiple-term query. The carot example demonstrates this type of correction. Such isolated-term correction would fail to detect, for instance, that the query flew form Heathrow contains a mis-spelling of the term from - because each term in the query is correctly spelled in isolation.\\n\\nWe begin by examining two techniques for addressing isolated-term correction: edit distance, and k-gram overlap. We then proceed to context-sensitive correction.\\n\\n3.3.3 Edit distance\\n\\nGiven two character strings s1 and s2, the edit distance between them is theEDIT DISTANCE minimum number of edit operations required to transform s1 into s2. Most commonly, the edit operations allowed for this purpose are: (i) insert a character into a string; (ii) delete a character from a string and (iii) replace a character of a string by another character; for these operations, edit distance is sometimes known as Levenshtein distance. For example, the edit distance be-LEVENSHTEIN\\n\\nDISTANCE tween cat and dog is 3. In fact, the notion of edit distance can be generalized to allowing different weights for different kinds of edit operations, for instance a higher weight may be placed on replacing the character s by the character p, than on replacing it by the character a (the latter being closer to s on the keyboard). Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity). However, the remainder of our treatment here will focus on the case in which all edit operations have the same weight.\\n\\nIt is well-known how to compute the (weighted) edit distance between two strings in time O(|s1| \\u00d7 |s2|), where |si| denotes the length of a string si. The idea is to use the dynamic programming algorithm in Figure 3.5, where the characters in s1 and s2 are given in array form. The algorithm fills the (integer) entries in a matrix m whose two dimensions equal the lengths of the two strings whose edit distances is being computed; the (i, j) entry of the matrix will hold (after the algorithm is executed) the edit distance between the strings consisting of the first i characters of s1 and the first j characters of s2. The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5, where the three quantities whose minimum is taken correspond to substituting a character in s1, inserting a character in s1 and inserting a character in s2.\\n\\nFigure 3.6 shows an example Levenshtein distance computation of Figure 3.5. The typical cell [i, j] has four entries formatted as a 2\\u00d7 2 cell. The lower right entry in each cell is the min of the other three, corresponding to the main dynamic programming step in Figure 3.5. The other three entries are the three entries m[i \\u2212 1, j \\u2212 1] + 0 or 1 depending on whether s1[i] =\\n\\nEDITDISTANCE(s1, s2)\\n\\n1 int m[i, j] = 0\\n\\n2 for i\\u2190 1 to |s1| 3 do m[i, 0] = i\\n\\n4 for j\\u2190 1 to |s2| 5 do m[0, j] = j\\n\\n6 for i\\u2190 1 to |s1|\\n\\n7 do for j\\u2190 1 to |s2|\\n\\n8 do m[i, j] = min{m[i\\u2212 1, j\\u2212 1] + if (s1[i] = s2[j]) then 0 else 1fi,\\n\\n9 m[i\\u2212 1, j] + 1,\\n\\n10 m[i, j\\u2212 1] + 1}\\n\\n11 return m[|s1|, |s2|]\\n\\n\\u25ee Figure 3.5 Dynamic programming algorithm for computing the edit distance between strings s1 and s2.\\n\\nf a s t\\n\\n0 1 1 2 2 3 3 4 4\\n\\nc\\n\\n1 1\\n\\n1 2 2 1\\n\\n2 3 2 2\\n\\n3 4 3 3\\n\\n4 5 4 4\\n\\na\\n\\n2 2\\n\\n2 2 3 2\\n\\n1 3 3 1\\n\\n3 4 2 2\\n\\n4 5\\n\\n3 3\\n\\nt\\n\\n3 3\\n\\n3 3 4 3\\n\\n3 2 4 2\\n\\n2 3 3 2\\n\\n2 4 3 2\\n\\ns\\n\\n4 4\\n\\n4 4 5 4\\n\\n4 3 5 3\\n\\n2 3 4 2\\n\\n3 3 3 3\\n\\n\\u25ee Figure 3.6 Example Levenshtein distance computation. The 2\\u00d7 2 cell in the [i, j] entry of the table shows the three numbers whose minimum yields the fourth. The cells in italics determine the edit distance in this example.\\n\\ns2[j], m[i\\u2212 1, j] + 1 and m[i, j\\u2212 1] + 1. The cells with numbers in italics depict the path by which we determine the Levenshtein distance.\\n\\nThe spelling correction problem however demands more than computing edit distance: given a set S of strings (corresponding to terms in the vocabulary) and a query string q, we seek the string(s) in V of least edit distance from q. We may view this as a decoding problem, in which the codewords (the strings in V) are prescribed in advance. The obvious way of doing this is to compute the edit distance from q to each string in V, before selecting the string(s) of minimum edit distance. This exhaustive search is inordinately expensive. Accordingly, a number of heuristics are used in practice to efficiently retrieve vocabulary terms likely to have low edit distance to the query term(s).\\n\\nThe simplest such heuristic is to restrict the search to dictionary terms beginning with the same letter as the query string; the hope would be that spelling errors do not occur in the first character of the query. A more sophisticated variant of this heuristic is to use a version of the permuterm index, in which we omit the end-of-word symbol $. Consider the set of all rotations of the query string q. For each rotation r from this set, we traverse the B-tree into the permuterm index, thereby retrieving all dictionary terms that have a rotation beginning with r. For instance, if q is mase and we consider the rotation r = sema, we would retrieve dictionary terms such as semantic and semaphore that do not have a small edit distance to q. Unfortunately, we would miss more pertinent dictionary terms such as mare and mane. To address this, we refine this rotation scheme: for each rotation, we omit a suffix of \\u2113 characters before performing the B-tree traversal. This ensures that each term in the set R of terms retrieved from the dictionary includes a \\\"long\\\" substring in common with q. The value of \\u2113 could depend on the length of q. Alternatively, we may set it to a fixed constant such as 2.\\n\\n3.3.4 k-gram indexes for spelling correction\\n\\nTo further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the k-gram index of Section 3.2.2 (page 54) to assist with retrieving vocabulary terms with low edit distance to the query q. Once we retrieve such terms, we can then find the ones of least edit distance from q.\\n\\nIn fact, we will use the k-gram index to retrieve vocabulary terms that have many k-grams in common with the query. We will argue that for reasonable definitions of \\\"many k-grams in common,\\\" the retrieval process is essentially that of a single scan through the postings for the k-grams in the query string q.\\n\\nThe 2-gram (or bigram) index in Figure 3.7 shows (a portion of) the postings for the three bigrams in the query bord. Suppose we wanted to retrieve vocabulary terms that contained at least two of these three bigrams. A single scan of the postings (much as in Chapter 1) would let us enumerate all such terms; in the example of Figure 3.7 we would enumerate aboard, boardroom and border.\\n\\nThis straightforward application of the linear scan intersection of postings immediately reveals the shortcoming of simply requiring matched vocabulary terms to contain a fixed number of k-grams from the query q: terms like boardroom, an implausible \\\"correction\\\" of bord, get enumerated. Conse- rd aboard ardent boardroom border\\n\\nor border lord morbid sordid\\n\\nbo aboard about boardroom border\\n\\n- - - -\\n\\n- - - -\\n\\n- - - -\\n\\n\\u25ee Figure 3.7 Matching at least two of the three 2-grams in the query bord.\\n\\nquently, we require more nuanced measures of the overlap in k-grams between a vocabulary term and q. The linear scan intersection can be adapted when the measure of overlap is the Jaccard coefficient for measuring the over-JACCARD COEFFICIENT\\n\\nlap between two sets A and B, defined to be |A\\u2229 B|/|A\\u222a B|. The two sets we consider are the set of k-grams in the query q, and the set of k-grams in a vocabulary term. As the scan proceeds, we proceed from one vocabulary term t to the next, computing on the fly the Jaccard coefficient between q and t. If the coefficient exceeds a preset threshold, we add t to the output; if not, we move on to the next term in the postings. To compute the Jaccard coefficient, we need the set of k-grams in q and t.\\n\\nSince we are scanning the postings for all k-grams in q, we immediately have these k-grams on hand. What about the k-grams of t? In principle, we could enumerate these on the fly from t; in practice this is not only slow but potentially infeasible since, in all likelihood, the postings entries themselves do not contain the complete string t but rather some encoding of t. The crucial observation is that to compute the Jaccard coefficient, we only need the length of the string t. To see this, recall the example of Figure 3.7 and consider the point when the postings scan for query q = bord reaches term t = boardroom. We know that two bigrams match. If the postings stored the (pre-computed) number of bigrams in boardroom (namely, 8), we have all the information we require to compute the Jaccard coefficient to be 2/(8 + 3\\u2212 2); the numerator is obtained from the number of postings hits (2, from bo and rd) while the denominator is the sum of the number of bigrams in bord and boardroom, less the number of postings hits.\\n\\nWe could replace the Jaccard coefficient by other measures that allow efficient on the fly computation during postings scans. How do we use these for spelling correction? One method that has some empirical support is to first use the k-gram index to enumerate a set of candidate vocabulary terms that are potential corrections of q. We then compute the edit distance from q to each term in this set, selecting terms from the set with small edit distance to q.\\n\\n3.3.5 Context sensitive spelling correction\\n\\nIsolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled. When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow. The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4) even though each query term is correctly spelled, then try substitutions of each correction in the phrase. For the example flew form Heathrow, we enumerate such phrases as fled form Heathrow and flew fore Heathrow. For each such substitute phrase, the search engine runs the query and determines the number of matching results.\\n\\nThis enumeration can be expensive if we find many corrections of the individual terms, since we could encounter a large number of combinations of alternatives. Several heuristics are used to trim this space. In the example above, as we expand the alternatives for flew and form, we retain only the most frequent combinations in the collection or in the query logs, which contain previous queries by users. For instance, we would retain flew from as an alternative to try and extend to a three-term corrected query, but perhaps not fled fore or flea form. In this example, the biword fled fore is likely to be rare compared to the biword flew from. Then, we only attempt to extend the list of top biwords (such as flew from), to corrections of Heathrow. As an alternative to using the biword statistics in the collection, we may use the logs of queries issued by users; these could of course include queries with spelling errors.\\n\\n? Exercise 3.7If |si| denotes the length of string si, show that the edit distance between s1 and s2 is never more than max{|s1|, |s2|}.\\n\\nExercise 3.8\\n\\nCompute the edit distance between paris and alice. Write down the 5 \\u00d7 5 array of distances between all prefixes as computed by the algorithm in Figure 3.5.\\n\\nExercise 3.9\\n\\nWrite pseudocode showing the details of computing on the fly the Jaccard coefficient while scanning the postings of the k-gram index, as mentioned on page 61.\\n\\nExercise 3.10\\n\\nCompute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or.\\n\\nExercise 3.11\\n\\nConsider the four-term query catched in the rye and suppose that each of the query terms has five alternative terms suggested by isolated-term correction. How many possible corrected phrases must we consider if we do not trim the space of corrected phrases, but instead try all six variants for each of the terms?\\n\\nExercise 3.12\\n\\nFor each of the prefixes of the query - catched, catched in and catched in the - we have a number of substitute prefixes arising from each term and its alternatives. Suppose that we were to retain only the top 10 of these substitute prefixes, as measured by its number of occurrences in the collection. We eliminate the rest from consideration for extension to longer prefixes: thus, if batched in is not one of the 10 most common 2-term queries in the collection, we do not consider any extension of batched in as possibly leading to a correction of catched in the rye. How many of the possible substitute prefixes are we eliminating at each phase?\\n\\nExercise 3.13\\n\\nAre we guaranteed that retaining and extending only the 10 commonest substitute prefixes of catched in will lead to one of the 10 commonest substitute prefixes of catched in the?\\n\\n3.4 Phonetic correction\\n\\nOur final technique for tolerant retrieval has to do with phonetic correction: misspellings that arise because the user types a query that sounds like the target term. Such algorithms are especially applicable to searches on the names of people. The main idea here is to generate, for each term, a \\\"phonetic hash\\\" so that similar-sounding terms hash to the same value. The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries. It is mainly used to correct phonetic misspellings in proper nouns.\\n\\nAlgorithms for such phonetic hashing are commonly collectively known as soundex algorithms. However, there is an original soundex algorithm, withSOUNDEX\\n\\nvarious variants, built on the following scheme:\\n\\n1. Turn every term to be indexed into a 4-character reduced form. Build an inverted index from these reduced forms to the original terms; call this the soundex index.\\n\\n2. Do the same with query terms.\\n\\n3. When the query calls for a soundex match, search this soundex index.\\n\\nThe variations in different soundex algorithms have to do with the conversion of terms to 4-character forms. A commonly used conversion results in a 4-character code, with the first character being a letter of the alphabet and the other three being digits between 0 and 9.\\n\\n1. Retain the first letter of the term.\\n\\n2. Change all occurrences of the following letters to '0' (zero): 'A', E', 'I', 'O', 'U', 'H', 'W', 'Y'.\\n\\n3. Change letters to digits as follows:\\n\\nB, F, P, V to 1.\\n\\nC, G, J, K, Q, S, X, Z to 2.\\n\\nD,T to 3.\\n\\nL to 4.\\n\\nM, N to 5.\\n\\nR to 6.\\n\\n4. Repeatedly remove one out of each pair of consecutive identical digits.\\n\\n5. Remove all zeros from the resulting string. Pad the resulting string with trailing zeros and return the first four positions, which will consist of a letter followed by three digits.\\n\\nFor an example of a soundex map, Hermann maps to H655. Given a query (say herman), we compute its soundex code and then retrieve all vocabulary terms matching this soundex code from the soundex index, before running the resulting query on the standard inverted index.\\n\\nThis algorithm rests on a few observations: (1) vowels are viewed as interchangeable, in transcribing names; (2) consonants with similar sounds (e.g., D and T) are put in equivalence classes. This leads to related names often having the same soundex codes. While these rules work for many cases, especially European languages, such rules tend to be writing system dependent. For example, Chinese names can be written in Wade-Giles or Pinyin transcription. While soundex works for some of the differences in the two transcriptions, for instance mapping both Wade-Giles hs and Pinyin x to 2, it fails in other cases, for example Wade-Giles j and Pinyin r are mapped differently.\\n\\n? Exercise 3.14Find two differently spelled proper nouns whose soundex codes are the same.\\n\\nExercise 3.15\\n\\nFind two phonetically similar proper nouns whose soundex codes are different.\\n\\n3.5 References and further reading\\n\\nKnuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries.\\n\\nGarfield (1976) gives one of the first complete descriptions of the permuterm index. Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes.\\n\\nOne of the earliest formal treatments of spelling correction was due to Damerau (1964). The notion of edit distance that we have used is due to Lev-enshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974). Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that k-gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings. Gusfield (1997) is a standard reference on string algorithms such as edit distance.\\n\\nProbabilistic models (\\\"noisy channel\\\" models) for spelling correction were pioneered by Kernighan et al. (1990) and further developed by Brill and Moore (2000) and Toutanova and Moore (2002). In these models, the misspelled query is viewed as a probabilistic corruption of a correct query. They have a similar mathematical basis to the language model methods presented in Chapter 12, and also provide ways of incorporating phonetic similarity, closeness on the keyboard, and data from the actual spelling mistakes of users. Many would regard them as the state-of-the-art approach. Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs.\\n\\nThe soundex algorithm is attributed to Margaret K. Odell and Robert C. Russelli (from U.S. patents granted in 1918 and 1922); the version described here draws on Bourne and Ford (1961). Zobel and Dart (1996) evaluate various phonetic matching algorithms, finding that a variant of the soundex algorithm performs poorly for general spelling correction, but that other algorithms based on the phonetic similarity of term pronunciations perform well.\",\n",
      "      \"enriched_text\": {\n",
      "        \"entities\": [\n",
      "          {\n",
      "            \"count\": 4,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"bigrams\",\n",
      "            \"relevance\": 0.430345,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"principal\",\n",
      "            \"relevance\": 0.337553,\n",
      "            \"type\": \"JobTitle\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Herman\",\n",
      "            \"relevance\": 0.332404,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Google\",\n",
      "            \"relevance\": 0.309222,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"AcademicInstitution\",\n",
      "                \"AwardPresentingOrganization\",\n",
      "                \"OperatingSystemDeveloper\",\n",
      "                \"ProgrammingLanguageDeveloper\",\n",
      "                \"SoftwareDeveloper\",\n",
      "                \"VentureFundedCompany\"\n",
      "              ],\n",
      "              \"name\": \"Google\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Google\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Sidney\",\n",
      "            \"relevance\": 0.298036,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.423062,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Sydney\",\n",
      "            \"relevance\": 0.297361,\n",
      "            \"type\": \"Organization\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Dart\",\n",
      "            \"relevance\": 0.289143,\n",
      "            \"type\": \"Company\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [],\n",
      "              \"name\": \"Dart Container\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/Dart_Container\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Levenshtein\",\n",
      "            \"relevance\": 0.28764,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"H655\",\n",
      "            \"relevance\": 0.285472,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Zobel\",\n",
      "            \"relevance\": 0.28512,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Damerau\",\n",
      "            \"relevance\": 0.283586,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"bord\",\n",
      "            \"relevance\": 0.283571,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Knuth\",\n",
      "            \"relevance\": 0.283206,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Gusfield\",\n",
      "            \"relevance\": 0.283025,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.54623,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Garfield\",\n",
      "            \"relevance\": 0.282795,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Kernighan\",\n",
      "            \"relevance\": 0.282336,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Ferragina\",\n",
      "            \"relevance\": 0.278717,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"U.S.\",\n",
      "            \"relevance\": 0.277328,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Region\",\n",
      "                \"AdministrativeDivision\",\n",
      "                \"GovernmentalJurisdiction\",\n",
      "                \"FilmEditor\",\n",
      "                \"Country\"\n",
      "              ],\n",
      "              \"name\": \"United States\",\n",
      "              \"dbpedia_resource\": \"http://dbpedia.org/resource/United_States\"\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"britian\",\n",
      "            \"relevance\": 0.275655,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"Country\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": -0.747865,\n",
      "              \"label\": \"negative\"\n",
      "            },\n",
      "            \"text\": \"bo\",\n",
      "            \"relevance\": 0.271462,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"quently\",\n",
      "            \"relevance\": 0.270626,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Peterson\",\n",
      "            \"relevance\": 0.266993,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Venturini\",\n",
      "            \"relevance\": 0.266113,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Wagner\",\n",
      "            \"relevance\": 0.264506,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"paris\",\n",
      "            \"relevance\": 0.263916,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.497083,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Brill\",\n",
      "            \"relevance\": 0.263194,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.497083,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Cucerzan\",\n",
      "            \"relevance\": 0.259526,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Kukich\",\n",
      "            \"relevance\": 0.255594,\n",
      "            \"type\": \"Location\",\n",
      "            \"disambiguation\": {\n",
      "              \"subtype\": [\n",
      "                \"City\"\n",
      "              ]\n",
      "            }\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Dart\",\n",
      "            \"relevance\": 0.255285,\n",
      "            \"type\": \"Organization\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Margaret K. Odell\",\n",
      "            \"relevance\": 0.254305,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Ford\",\n",
      "            \"relevance\": 0.252659,\n",
      "            \"type\": \"Company\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.302033,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"britney\",\n",
      "            \"relevance\": 0.250515,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.303135,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Moore\",\n",
      "            \"relevance\": 0.246248,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"brandy spears\",\n",
      "            \"relevance\": 0.242803,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0.303135,\n",
      "              \"label\": \"positive\"\n",
      "            },\n",
      "            \"text\": \"Toutanova\",\n",
      "            \"relevance\": 0.242414,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Fischer\",\n",
      "            \"relevance\": 0.239547,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"Robert C. Russelli\",\n",
      "            \"relevance\": 0.229156,\n",
      "            \"type\": \"Person\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 7,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"3-gram\",\n",
      "            \"relevance\": 0.229156,\n",
      "            \"type\": \"Quantity\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 2,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"3-grams\",\n",
      "            \"relevance\": 0.229156,\n",
      "            \"type\": \"Quantity\"\n",
      "          },\n",
      "          {\n",
      "            \"count\": 1,\n",
      "            \"sentiment\": {\n",
      "              \"score\": 0,\n",
      "              \"label\": \"neutral\"\n",
      "            },\n",
      "            \"text\": \"three 2-grams\",\n",
      "            \"relevance\": 0.229156,\n",
      "            \"type\": \"Quantity\"\n",
      "          }\n",
      "        ],\n",
      "        \"concepts\": [\n",
      "          {\n",
      "            \"text\": \"Binary search tree\",\n",
      "            \"relevance\": 0.966757,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Binary_search_tree\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Spell checker\",\n",
      "            \"relevance\": 0.662819,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Spell_checker\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Information retrieval\",\n",
      "            \"relevance\": 0.646866,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Information_retrieval\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Soundex\",\n",
      "            \"relevance\": 0.617843,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Soundex\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Levenshtein distance\",\n",
      "            \"relevance\": 0.601336,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Levenshtein_distance\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Edit distance\",\n",
      "            \"relevance\": 0.584284,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Edit_distance\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Character encoding\",\n",
      "            \"relevance\": 0.53203,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Character_encoding\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Algorithms on strings\",\n",
      "            \"relevance\": 0.516731,\n",
      "            \"dbpedia_resource\": \"http://dbpedia.org/resource/Algorithms_on_strings\"\n",
      "          }\n",
      "        ],\n",
      "        \"categories\": [\n",
      "          {\n",
      "            \"score\": 0.66341,\n",
      "            \"label\": \"/technology and computing/software/databases\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.306831,\n",
      "            \"label\": \"/health and fitness/exercise\"\n",
      "          },\n",
      "          {\n",
      "            \"score\": 0.228719,\n",
      "            \"label\": \"/technology and computing/internet technology/web search\"\n",
      "          }\n",
      "        ],\n",
      "        \"relations\": [\n",
      "          {\n",
      "            \"type\": \"affectedBy\",\n",
      "            \"sentence\": \"Users make spelling errors either by accident, or because the term they are searching for (e.g., Herman) has no unambiguous spelling in the collection.\",\n",
      "            \"score\": 0.505292,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Users\",\n",
      "                \"location\": [\n",
      "                  1054,\n",
      "                  1059\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Users\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"accident\",\n",
      "                \"location\": [\n",
      "                  1091,\n",
      "                  1099\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"NaturalDisaster\",\n",
      "                    \"text\": \"accident\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"We will also use k-gram indexes in Section 3.3.4.\",\n",
      "            \"score\": 0.633993,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.3.4\",\n",
      "                \"location\": [\n",
      "                  12204,\n",
      "                  12209\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.3.4\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  12196,\n",
      "                  12203\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"hasAttribute\",\n",
      "            \"sentence\": \"We use a special character $ to denote the beginning or end of a term, so the full set of 3-grams generated for castle is: $ca, cas, ast, stl, tle, le$.\",\n",
      "            \"score\": 0.425393,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"character\",\n",
      "                \"location\": [\n",
      "                  12336,\n",
      "                  12345\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"character\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"$\",\n",
      "                \"location\": [\n",
      "                  12346,\n",
      "                  12347\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Money\",\n",
      "                    \"text\": \"$\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Consider again the query fi*mo*er from Section 3.2.1.\",\n",
      "            \"score\": 0.666713,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.2.1\",\n",
      "                \"location\": [\n",
      "                  16191,\n",
      "                  16196\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.2.1\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  16183,\n",
      "                  16190\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Can you think of a term that matches the permuterm query in Section 3.2.1, but does not satisfy this Boolean query?\",\n",
      "            \"score\": 0.588508,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.2.1\",\n",
      "                \"location\": [\n",
      "                  16338,\n",
      "                  16343\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.2.1\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  16330,\n",
      "                  16337\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"hasAttribute\",\n",
      "            \"sentence\": \"Google reports (http://www.google.com/jobs/britney.html) that the following are all treated as misspellings of the query britney spears:\",\n",
      "            \"score\": 0.462326,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Google\",\n",
      "                \"location\": [\n",
      "                  16744,\n",
      "                  16750\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"Google\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"http://www.google.com/jobs/britney.html\",\n",
      "                \"location\": [\n",
      "                  16760,\n",
      "                  16799\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Web\",\n",
      "                    \"text\": \"http://www.google.com/jobs/britney.html\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"We will develop these proximity measures in Section 3.3.3.\",\n",
      "            \"score\": 0.633993,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.3.3\",\n",
      "                \"location\": [\n",
      "                  17577,\n",
      "                  17582\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.3.3\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  17569,\n",
      "                  17576\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Beginning in Section 3.3.3 we describe notions of proximity between queries, as well as their efficient computation.\",\n",
      "            \"score\": 0.691582,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.3.3\",\n",
      "                \"location\": [\n",
      "                  18411,\n",
      "                  18416\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.3.3\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  18403,\n",
      "                  18410\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Setting weights in this way depending on the likelihood of letters substituting for each other is very effective in practice (see Section 3.4 for the separate issue of phonetic similarity).\",\n",
      "            \"score\": 0.67153,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.4\",\n",
      "                \"location\": [\n",
      "                  21006,\n",
      "                  21009\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.4\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  20998,\n",
      "                  21005\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"locatedAt\",\n",
      "            \"sentence\": \"The idea is to use the dynamic programming algorithm in Figure 3.5, where the characters in s1 and s2 are given in array form.\",\n",
      "            \"score\": 0.402634,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Figure\",\n",
      "                \"location\": [\n",
      "                  21385,\n",
      "                  21391\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Figure\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"where\",\n",
      "                \"location\": [\n",
      "                  21397,\n",
      "                  21402\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Facility\",\n",
      "                    \"text\": \"where\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"residesIn\",\n",
      "            \"sentence\": \"The idea is to use the dynamic programming algorithm in Figure 3.5, where the characters in s1 and s2 are given in array form.\",\n",
      "            \"score\": 0.256864,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"characters\",\n",
      "                \"location\": [\n",
      "                  21407,\n",
      "                  21417\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"characters\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"where\",\n",
      "                \"location\": [\n",
      "                  21397,\n",
      "                  21402\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Facility\",\n",
      "                    \"text\": \"where\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Finally, in Section 3.4 we study a method for seeking vocabulary terms that are phonetically close to the query term(s).\",\n",
      "            \"score\": 0.691582,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.4\",\n",
      "                \"location\": [\n",
      "                  1369,\n",
      "                  1372\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.4\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  1361,\n",
      "                  1368\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"locatedAt\",\n",
      "            \"sentence\": \"The central dynamic programming step is depicted in Lines 8-10 of Figure 3.5, where the three quantities whose minimum is taken correspond to substituting a character in s1, inserting a character in s1 and inserting a character in s2.\",\n",
      "            \"score\": 0.362956,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Figure\",\n",
      "                \"location\": [\n",
      "                  21866,\n",
      "                  21872\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Figure\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"where\",\n",
      "                \"location\": [\n",
      "                  21878,\n",
      "                  21883\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Facility\",\n",
      "                    \"text\": \"where\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"colleague\",\n",
      "            \"sentence\": \"8 do m[i, j] = min{m[i- 1, j- 1] + if (s1[i] = s2[j]) then 0 else 1fi,\",\n",
      "            \"score\": 0.24952,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"min\",\n",
      "                \"location\": [\n",
      "                  22585,\n",
      "                  22588\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"min\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"{m\",\n",
      "                \"location\": [\n",
      "                  22588,\n",
      "                  22590\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"{m\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"To further limit the set of vocabulary terms for which we compute edit distances to the query term, we now show how to invoke the k-gram index of Section 3.2.2 (page 54) to assist with retrieving vocabulary terms with low edit distance to the query q.\",\n",
      "            \"score\": 0.633959,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.2.2\",\n",
      "                \"location\": [\n",
      "                  25478,\n",
      "                  25483\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.2.2\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  25470,\n",
      "                  25477\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"agentOf\",\n",
      "            \"sentence\": \"We will argue that for reasonable definitions of \\\"many k-grams in common,\\\" the retrieval process is essentially that of a single scan through the postings for the k-grams in the query string q.\",\n",
      "            \"score\": 0.949698,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"We\",\n",
      "                \"location\": [\n",
      "                  25779,\n",
      "                  25781\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"We\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"argue\",\n",
      "                \"location\": [\n",
      "                  25787,\n",
      "                  25792\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"EventCommunication\",\n",
      "                    \"text\": \"argue\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Isolated-term correction would fail to correct typographical errors such as flew form Heathrow, where all three query terms are correctly spelled.\",\n",
      "            \"score\": 0.417122,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"three\",\n",
      "                \"location\": [\n",
      "                  29247,\n",
      "                  29252\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"three\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"where\",\n",
      "                \"location\": [\n",
      "                  29237,\n",
      "                  29242\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"where\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"locatedAt\",\n",
      "            \"sentence\": \"When a phrase such as this retrieves few documents, a search engine may like to offer the corrected query flew from Heathrow.\",\n",
      "            \"score\": 0.451603,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"search engine\",\n",
      "                \"location\": [\n",
      "                  29342,\n",
      "                  29355\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"search engine\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Heathrow\",\n",
      "                \"location\": [\n",
      "                  29404,\n",
      "                  29412\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Heathrow\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"The simplest way to do this is to enumerate corrections of each of the three query terms (using the methods leading up to Section 3.3.4) even though each query term is correctly spelled, then try substitutions of each correction in the phrase.\",\n",
      "            \"score\": 0.653102,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.3.4\",\n",
      "                \"location\": [\n",
      "                  29544,\n",
      "                  29549\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.3.4\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  29536,\n",
      "                  29543\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"agentOf\",\n",
      "            \"sentence\": \"Write pseudocode showing the details of computing on the fly the Jaccard coefficient while scanning the postings of the k-gram index, as mentioned on page 61.\",\n",
      "            \"score\": 0.644775,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Jaccard coefficient\",\n",
      "                \"location\": [\n",
      "                  31198,\n",
      "                  31217\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"Jaccard coefficient\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"mentioned\",\n",
      "                \"location\": [\n",
      "                  31270,\n",
      "                  31279\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"EventCommunication\",\n",
      "                    \"text\": \"mentioned\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"employedBy\",\n",
      "            \"sentence\": \"Compute the Jaccard coefficients between the query bord and each of the terms in Figure 3.7 that contain the bigram or.\",\n",
      "            \"score\": 0.404627,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"coefficients\",\n",
      "                \"location\": [\n",
      "                  31328,\n",
      "                  31340\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"coefficients\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Jaccard\",\n",
      "                \"location\": [\n",
      "                  31320,\n",
      "                  31327\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"Jaccard\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"participantIn\",\n",
      "            \"sentence\": \"The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries.\",\n",
      "            \"score\": 0.458065,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"police departments\",\n",
      "                \"location\": [\n",
      "                  33065,\n",
      "                  33083\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"police departments\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"match\",\n",
      "                \"location\": [\n",
      "                  33124,\n",
      "                  33129\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"SportingEvent\",\n",
      "                    \"text\": \"match\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"Because we will develop many variants of inverted indexes in this chapter, we will use sometimes the phrase standard inverted index to mean the inverted index developed in Chapters 1 and 2, in which each vocabulary term has a postings list with the documents in the collection.\",\n",
      "            \"score\": 0.526683,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"2\",\n",
      "                \"location\": [\n",
      "                  1811,\n",
      "                  1812\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"2\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Chapters\",\n",
      "                \"location\": [\n",
      "                  1796,\n",
      "                  1804\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Chapters\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"timeOf\",\n",
      "            \"sentence\": \"The idea owes its origins to work in international police departments from the early 20th century, seeking to match names for wanted criminals despite the names being spelled differently in different countries.\",\n",
      "            \"score\": 0.770073,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"early 20th century\",\n",
      "                \"location\": [\n",
      "                  33093,\n",
      "                  33111\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"early 20th century\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"match\",\n",
      "                \"location\": [\n",
      "                  33124,\n",
      "                  33129\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"SportingEvent\",\n",
      "                    \"text\": \"match\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries.\",\n",
      "            \"score\": 0.499534,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Knuth\",\n",
      "                \"location\": [\n",
      "                  35756,\n",
      "                  35761\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Knuth\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1997\",\n",
      "                \"location\": [\n",
      "                  35763,\n",
      "                  35767\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1997\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"partOfMany\",\n",
      "            \"sentence\": \"Knuth (1997) is a comprehensive source for information on search trees, including B-trees and their use in searching through dictionaries.\",\n",
      "            \"score\": 0.742264,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Knuth\",\n",
      "                \"location\": [\n",
      "                  35756,\n",
      "                  35761\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Knuth\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"their\",\n",
      "                \"location\": [\n",
      "                  35850,\n",
      "                  35855\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"their\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"Ferragina and Venturini (2007) give an approach to addressing the space blowup in permuterm indexes.\",\n",
      "            \"score\": 0.378517,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Venturini\",\n",
      "                \"location\": [\n",
      "                  35995,\n",
      "                  36004\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Venturini\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"2007\",\n",
      "                \"location\": [\n",
      "                  36006,\n",
      "                  36010\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"2007\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"One of the earliest formal treatments of spelling correction was due to Damerau (1964).\",\n",
      "            \"score\": 0.495056,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Damerau\",\n",
      "                \"location\": [\n",
      "                  36155,\n",
      "                  36162\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Damerau\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1964\",\n",
      "                \"location\": [\n",
      "                  36164,\n",
      "                  36168\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1964\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"The notion of edit distance that we have used is due to Lev-enshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974).\",\n",
      "            \"score\": 0.588329,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Lev-enshtein\",\n",
      "                \"location\": [\n",
      "                  36227,\n",
      "                  36239\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Lev-enshtein\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1965\",\n",
      "                \"location\": [\n",
      "                  36241,\n",
      "                  36245\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1965\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"The notion of edit distance that we have used is due to Lev-enshtein (1965) and the algorithm in Figure 3.5 is due to Wagner and Fischer (1974).\",\n",
      "            \"score\": 0.583164,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Fischer\",\n",
      "                \"location\": [\n",
      "                  36300,\n",
      "                  36307\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Fischer\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1974\",\n",
      "                \"location\": [\n",
      "                  36309,\n",
      "                  36313\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1974\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"partOfMany\",\n",
      "            \"sentence\": \"Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that k-gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings.\",\n",
      "            \"score\": 0.497517,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Dart\",\n",
      "                \"location\": [\n",
      "                  36479,\n",
      "                  36483\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Dart\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"variants\",\n",
      "                \"location\": [\n",
      "                  36360,\n",
      "                  36368\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"variants\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"Peterson (1980) and Kukich (1992) developed variants of methods based on edit distances, culminating in a detailed empirical study of several methods by Zobel and Dart (1995), which shows that k-gram indexing is very effective for finding candidate mismatches, but should be combined with a more fine-grained technique such as edit distance to determine the most likely misspellings.\",\n",
      "            \"score\": 0.419892,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Dart\",\n",
      "                \"location\": [\n",
      "                  36479,\n",
      "                  36483\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Dart\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1995\",\n",
      "                \"location\": [\n",
      "                  36485,\n",
      "                  36489\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1995\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"(1990) and further developed by Brill and Moore (2000) and Toutanova and Moore (2002).\",\n",
      "            \"score\": 0.416281,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Moore\",\n",
      "                \"location\": [\n",
      "                  36963,\n",
      "                  36968\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Moore\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"2002\",\n",
      "                \"location\": [\n",
      "                  36970,\n",
      "                  36974\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"2002\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"ownerOf\",\n",
      "            \"sentence\": \"3.1 Search structures for dictionaries\",\n",
      "            \"score\": 0.86321,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Search\",\n",
      "                \"location\": [\n",
      "                  1907,\n",
      "                  1913\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"Search\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"structures\",\n",
      "                \"location\": [\n",
      "                  1914,\n",
      "                  1924\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Facility\",\n",
      "                    \"text\": \"structures\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"Cucerzan and Brill (2004) show how this work can be extended to learning spelling correction models based on query reformulations in search engine logs.\",\n",
      "            \"score\": 0.377004,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Brill\",\n",
      "                \"location\": [\n",
      "                  37379,\n",
      "                  37384\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Brill\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"2004\",\n",
      "                \"location\": [\n",
      "                  37386,\n",
      "                  37390\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"2004\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"bornOn\",\n",
      "            \"sentence\": \"Zobel and Dart (1996) evaluate various phonetic matching algorithms, finding that a variant of the soundex algorithm performs poorly for general spelling correction, but that other algorithms based on the phonetic similarity of term pronunciations perform well.\",\n",
      "            \"score\": 0.368777,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"Dart\",\n",
      "                \"location\": [\n",
      "                  37716,\n",
      "                  37720\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"Dart\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"1996\",\n",
      "                \"location\": [\n",
      "                  37722,\n",
      "                  37726\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Date\",\n",
      "                    \"text\": \"1996\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"populationOf\",\n",
      "            \"sentence\": \"In particular, we cannot seek (for instance) all terms beginning with the prefix automat, an operation that we will require below in Section 3.2.\",\n",
      "            \"score\": 0.633993,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"3.2\",\n",
      "                \"location\": [\n",
      "                  3564,\n",
      "                  3567\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Cardinal\",\n",
      "                    \"text\": \"3.2\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"Section\",\n",
      "                \"location\": [\n",
      "                  3556,\n",
      "                  3563\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"Section\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"locatedAt\",\n",
      "            \"sentence\": \"In fact, using a regular B-tree together with a reverse B-tree, we can handle an even more general case: wildcard queries in which there is a single * symbol, such as se*mon.\",\n",
      "            \"score\": 0.342958,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"we\",\n",
      "                \"location\": [\n",
      "                  8339,\n",
      "                  8341\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"we\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"B-tree\",\n",
      "                \"location\": [\n",
      "                  8331,\n",
      "                  8337\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"GeopoliticalEntity\",\n",
      "                    \"text\": \"B-tree\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"employedBy\",\n",
      "            \"sentence\": \"First, we introduce a special symbol $ into our character set, to mark the end of a term.\",\n",
      "            \"score\": 0.69988,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"character\",\n",
      "                \"location\": [\n",
      "                  9745,\n",
      "                  9754\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"character\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"our\",\n",
      "                \"location\": [\n",
      "                  9741,\n",
      "                  9744\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Organization\",\n",
      "                    \"text\": \"our\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"partOfMany\",\n",
      "            \"sentence\": \"Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron.\",\n",
      "            \"score\": 0.482714,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"man\",\n",
      "                \"location\": [\n",
      "                  10535,\n",
      "                  10538\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"man\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"others\",\n",
      "                \"location\": [\n",
      "                  10517,\n",
      "                  10523\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"others\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          },\n",
      "          {\n",
      "            \"type\": \"partOfMany\",\n",
      "            \"sentence\": \"Next, we look up this string in the permuterm index, where seeking n$m* (via a search tree) leads to rotations of (among others) the terms man and moron.\",\n",
      "            \"score\": 0.571453,\n",
      "            \"arguments\": [\n",
      "              {\n",
      "                \"text\": \"moron\",\n",
      "                \"location\": [\n",
      "                  10543,\n",
      "                  10548\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"moron\"\n",
      "                  }\n",
      "                ]\n",
      "              },\n",
      "              {\n",
      "                \"text\": \"others\",\n",
      "                \"location\": [\n",
      "                  10517,\n",
      "                  10523\n",
      "                ],\n",
      "                \"entities\": [\n",
      "                  {\n",
      "                    \"type\": \"Person\",\n",
      "                    \"text\": \"others\"\n",
      "                  }\n",
      "                ]\n",
      "              }\n",
      "            ]\n",
      "          }\n",
      "        ],\n",
      "        \"keywords\": [\n",
      "          {\n",
      "            \"text\": \"query\",\n",
      "            \"relevance\": 0.969333\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"edit distance\",\n",
      "            \"relevance\": 0.858885\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"standard inverted index\",\n",
      "            \"relevance\": 0.797442\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"vocabulary terms\",\n",
      "            \"relevance\": 0.741078\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query\",\n",
      "            \"relevance\": 0.738934\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query term\",\n",
      "            \"relevance\": 0.697236\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard queries\",\n",
      "            \"relevance\": 0.660645\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"permuterm index\",\n",
      "            \"relevance\": 0.653825\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"search\",\n",
      "            \"relevance\": 0.609521\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"search engine\",\n",
      "            \"relevance\": 0.573412\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query terms\",\n",
      "            \"relevance\": 0.56876\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Boolean query\",\n",
      "            \"relevance\": 0.56521\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"vocabulary term\",\n",
      "            \"relevance\": 0.561758\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query qw\",\n",
      "            \"relevance\": 0.557893\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query S*dney\",\n",
      "            \"relevance\": 0.557145\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query judicia*\",\n",
      "            \"relevance\": 0.556967\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"trailing wildcard query\",\n",
      "            \"relevance\": 0.556097\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query m*n\",\n",
      "            \"relevance\": 0.554325\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original wildcard query\",\n",
      "            \"relevance\": 0.553789\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"rotated wildcard query\",\n",
      "            \"relevance\": 0.553347\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query re*ve\",\n",
      "            \"relevance\": 0.551683\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"wildcard query mon*h\",\n",
      "            \"relevance\": 0.548821\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"example\",\n",
      "            \"relevance\": 0.544768\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"search tree\",\n",
      "            \"relevance\": 0.542314\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original query\",\n",
      "            \"relevance\": 0.540226\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query term carot\",\n",
      "            \"relevance\": 0.537902\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"single query term\",\n",
      "            \"relevance\": 0.537393\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"spell-corrected query term\",\n",
      "            \"relevance\": 0.536669\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"correction\",\n",
      "            \"relevance\": 0.536229\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query bord\",\n",
      "            \"relevance\": 0.535821\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"*WILDCARD QUERY symbol\",\n",
      "            \"relevance\": 0.523812\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"Figure\",\n",
      "            \"relevance\": 0.522937\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query Universit* Stuttgart\",\n",
      "            \"relevance\": 0.522418\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query red*\",\n",
      "            \"relevance\": 0.521408\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original query red*\",\n",
      "            \"relevance\": 0.520873\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query carot\",\n",
      "            \"relevance\": 0.520413\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query string\",\n",
      "            \"relevance\": 0.520323\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"search trees\",\n",
      "            \"relevance\": 0.51824\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original query returns\",\n",
      "            \"relevance\": 0.516991\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"doc-WILDCARD QUERY\",\n",
      "            \"relevance\": 0.515237\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query automat*\",\n",
      "            \"relevance\": 0.515039\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"binary search tree\",\n",
      "            \"relevance\": 0.514395\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"query time\",\n",
      "            \"relevance\": 0.514138\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"k-gram index\",\n",
      "            \"relevance\": 0.512454\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original vocabulary terms\",\n",
      "            \"relevance\": 0.511578\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"isolated-term correction\",\n",
      "            \"relevance\": 0.509548\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"original vocabulary term\",\n",
      "            \"relevance\": 0.509461\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"permuterm query\",\n",
      "            \"relevance\": 0.509176\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"dictionary terms\",\n",
      "            \"relevance\": 0.50701\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"three-term corrected query\",\n",
      "            \"relevance\": 0.506273\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('9. Query ')\n",
    "my_query = discovery.query(environment_id=my_environment_id, collection_id=my_collection_id)\n",
    "print(json.dumps(my_query, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
